{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the documentation for our machine learning project! \u2693\ufe0e Implementations \u2693\ufe0e Neural Network Classifier \u2693\ufe0e A neural network for use in classification problems. DenseLayer - A fully connected layer with user defined size and activation function Loss Functions - The loss function to use in the neural network (currently only categorical cross entropy is supported). Activation Functions - The activation functions to use in the fully connected layers (currently stable_softmax and leaky_relu is supported). Decision Tree Classifier \u2693\ufe0e A decision tree for use in classification problems. Node - A tree node used by the decision tree classifier, is either leaf or not. Impurtiy Criterion - The impurity function to use when decision whether to split nodes in the decision tree (currently only gini_impurity and entropy are supported).","title":"Home"},{"location":"#welcome-to-the-documentation-for-our-machine-learning-project","text":"","title":"Welcome to the documentation for our machine learning project!"},{"location":"#implementations","text":"","title":"Implementations"},{"location":"#neural-network-classifier","text":"A neural network for use in classification problems. DenseLayer - A fully connected layer with user defined size and activation function Loss Functions - The loss function to use in the neural network (currently only categorical cross entropy is supported). Activation Functions - The activation functions to use in the fully connected layers (currently stable_softmax and leaky_relu is supported).","title":"Neural Network Classifier"},{"location":"#decision-tree-classifier","text":"A decision tree for use in classification problems. Node - A tree node used by the decision tree classifier, is either leaf or not. Impurtiy Criterion - The impurity function to use when decision whether to split nodes in the decision tree (currently only gini_impurity and entropy are supported).","title":"Decision Tree Classifier"},{"location":"installation/","text":"Installation \u2693\ufe0e Normal installation \u2693\ufe0e To start using the library simply run the following command from the root of the github repository directory pip install . Development installation \u2693\ufe0e If you want to edit or add anything to the library you can install the library in editable mode , so you don't have to reinstall the library after each change: pip install -e .","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#normal-installation","text":"To start using the library simply run the following command from the root of the github repository directory pip install .","title":"Normal installation"},{"location":"installation/#development-installation","text":"If you want to edit or add anything to the library you can install the library in editable mode , so you don't have to reinstall the library after each change: pip install -e .","title":"Development installation"},{"location":"reference/SUMMARY/","text":"mlproject helpers _data_loader _metrics neural_net _activations _loss _dense_layer _neural_net decision_tree _decision_tree _node _impurity","title":"SUMMARY"},{"location":"reference/mlproject/decision_tree/","text":"DecisionTreeClassifier \u2693\ufe0e Decision Tree Classifier Simple decision tree classifier with user specific impurity, max depth and minimum number of samples in leaf nodes. Parameters: Name Type Description Default criterion str , optional The impurity criterion to use when splitting nodes, by default 'gini' 'gini' max_depth int , optional The maximum depth of the decision tree, by default 100 100 min_samples_in_leaf int , optional The minimum number of samples that need to be in a leaf, by default 2 2 Source code in mlproject/decision_tree/_decision_tree.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 class DecisionTreeClassifier : \"\"\"Decision Tree Classifier Simple decision tree classifier with user specific impurity, max depth and minimum number of samples in leaf nodes. Parameters ---------- criterion : str, optional The impurity criterion to use when splitting nodes, by default 'gini' max_depth : int, optional The maximum depth of the decision tree, by default 100 min_samples_in_leaf : int, optional The minimum number of samples that need to be in a leaf, by default 2 \"\"\" def __init__ ( self , criterion = \"gini\" , max_depth = 100 , min_samples_in_leaf = 2 ): self . max_depth = max_depth self . min_samples_in_leaf = min_samples_in_leaf self . root = None if criterion . lower () == \"gini\" : self . criterion = gini_impurity elif criterion . lower () == \"entropy\" : self . criterion = entropy_impurity def fit ( self , X , y ): \"\"\"Fit the decision tree to the given data Parameters ---------- X : 2d ndarray The data to be used for fitting the decision tree y : 2d ndarray An array of the true labels for the data points \"\"\" with progress as pb : t1 = pb . add_task ( \"[blue]Training\" , total = 1 ) self . root = self . _grow ( X , y ) pb . update ( t1 , advance = 1 ) if progress . finished : pb . update ( t1 , description = \"[bright_green]Training complete!\" ) def predict ( self , X ): \"\"\"Predict class labels for the given data. For all data points in the dataset traverse the decision tree until it reaches a leaf node and return the majority class of that leaf node. Parameters ---------- X : 2d ndarray The data that we want to use to make predictions. Returns ------- 1d ndarray All predicted class labels with size n, where n is the number of data points. \"\"\" return np . array ([ self . _traverse ( datapoint , self . root ) for datapoint in X ]) def predict_proba ( self , X ): \"\"\"Predict class probabilities for the given data For all data points in the dataset traverse the decision tree until it reaches a leaf node and return the class probabilities of that leaf node. Parameters ---------- X : 2d ndarray The data that we want to use to make predictions Returns ------- 2d ndarray All probabilites with size n x k, where n is the number of data points and k is the number classes \"\"\" return np . array ( [ self . _traverse ( datapoint , self . root , prob = True ) for datapoint in X ] ) def _grow ( self , X , y , cur_depth = 0 ): \"\"\"Grows a decision tree from the given data. This is the part that is doing the actual fitting of the decision tree. Parameters ---------- X : 2d ndarray The data to use when growing the decision tree y : 2d ndarray array of the true class labels cur_depth : int, optional The current depth of the decision tree, by default 0 Returns ------- Node A new node of class Node with new left and right children. \"\"\" self . n , self . p = X . shape node_unique_classes = np . unique ( y ) self . node_k = len ( node_unique_classes ) if ( cur_depth >= self . max_depth or self . n < self . min_samples_in_leaf or self . node_k == 1 ): most_common = self . _most_common_label ( y , prob = False ) class_probs = self . _most_common_label ( y , prob = True ) return Node ( majority_class = most_common , class_probs = class_probs ) cur_depth += 1 best_feature , best_threshold = self . _best_split ( X , y ) left_idxs = np . argwhere ( X [:, best_feature ] <= best_threshold ) . flatten () right_idxs = np . argwhere ( X [:, best_feature ] > best_threshold ) . flatten () left = self . _grow ( X [ left_idxs , :], y [ left_idxs ], cur_depth ) right = self . _grow ( X [ right_idxs , :], y [ right_idxs ], cur_depth ) return Node ( left , right , best_feature , best_threshold ) def _best_split ( self , X , y ): \"\"\"Calculates the best split of a node with the given data points Parameters ---------- X : 2d ndarray The data points to consider for splitting this node y : 2d ndarray The true labels to consider for splitting this node Returns ------- tuple A tuple containing the best index and threshold for the split \"\"\" best_gain = - np . inf for feat_idx in range ( X . shape [ 1 ]): feature_col = X [:, feat_idx ] possible_splits = np . unique ( feature_col ) for split in possible_splits : cur_gain = self . _information_gain ( y , feature_col , split ) if cur_gain > best_gain : best_gain = cur_gain split_idx = feat_idx split_thresh = split return split_idx , split_thresh def _information_gain ( self , y , feature_col , split_thresh ): \"\"\"Calculates the information gain of a node with the given data labels Parameters ---------- y : 2d ndarray array of true labels for this node feature_col : 2d ndarray Column of dataset containing the data points of the best feature for this split split_thresh : float or int the threshold for the best split of the data Returns ------- float The information gain from this node compared to it's parent \"\"\" parent_impurity = self . criterion ( y ) left_idxs = np . argwhere ( feature_col <= split_thresh ) . flatten () right_idxs = np . argwhere ( feature_col > split_thresh ) . flatten () if len ( left_idxs ) == 0 or len ( right_idxs ) == 0 : return 0 n = len ( y ) left_prob = len ( left_idxs ) / n right_prob = len ( right_idxs ) / n left_impurity = self . criterion ( y [ left_idxs ]) right_impurity = self . criterion ( y [ right_idxs ]) weighted_impurity = left_prob * left_impurity + right_prob * right_impurity information_gain = parent_impurity - weighted_impurity return information_gain def _traverse ( self , X , node , prob = False ): \"\"\"Traverses the tree until it reaches a leaf node and returns either the majority class of that node or the class probabilities if prob is True. Parameters ---------- X : 2d ndarray The data points to use for traversing the tree node : Node The node to start the traversal from. prob : bool, optional used to specify whether or not to return class probabilities, by default False Returns ------- int or bool Depending on `prob` it either returns the majority class or the class probabilities \"\"\" if node . is_leaf (): if prob : return np . argmax ( node . class_probs ) return node . majority_class if X [ node . feature ] <= node . threshold : return self . _traverse ( X , node . left ) elif X [ node . feature ] > node . threshold : return self . _traverse ( X , node . right ) def _most_common_label ( self , y , prob = False ): \"\"\"Calculates the most common label of a leaf node or the class probabilities Parameters ---------- y : 2d ndarray Array of true labels for this particular node prob : bool, optional used to specify whether or not to return class probabilities, by default False Returns ------- int or bool Depending on `prob` it either returns the majority class or the class probabilities \"\"\" uniques , counts = np . unique ( y , return_counts = True ) label_counts = dict ( zip ( uniques , counts )) sorted_keys = list ( sorted ( label_counts . items (), key = lambda x : x [ 1 ], reverse = True ) ) if prob : n = np . sum ( counts ) return np . array ([ label_counts [ i ] / n for i in uniques ]) return sorted_keys [ 0 ][ 0 ] fit ( X , y ) \u2693\ufe0e Fit the decision tree to the given data Parameters: Name Type Description Default X 2d ndarray The data to be used for fitting the decision tree required y 2d ndarray An array of the true labels for the data points required Source code in mlproject/decision_tree/_decision_tree.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def fit ( self , X , y ): \"\"\"Fit the decision tree to the given data Parameters ---------- X : 2d ndarray The data to be used for fitting the decision tree y : 2d ndarray An array of the true labels for the data points \"\"\" with progress as pb : t1 = pb . add_task ( \"[blue]Training\" , total = 1 ) self . root = self . _grow ( X , y ) pb . update ( t1 , advance = 1 ) if progress . finished : pb . update ( t1 , description = \"[bright_green]Training complete!\" ) predict ( X ) \u2693\ufe0e Predict class labels for the given data. For all data points in the dataset traverse the decision tree until it reaches a leaf node and return the majority class of that leaf node. Parameters: Name Type Description Default X 2d ndarray The data that we want to use to make predictions. required Returns: Type Description 1d ndarray All predicted class labels with size n, where n is the number of data points. Source code in mlproject/decision_tree/_decision_tree.py 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def predict ( self , X ): \"\"\"Predict class labels for the given data. For all data points in the dataset traverse the decision tree until it reaches a leaf node and return the majority class of that leaf node. Parameters ---------- X : 2d ndarray The data that we want to use to make predictions. Returns ------- 1d ndarray All predicted class labels with size n, where n is the number of data points. \"\"\" return np . array ([ self . _traverse ( datapoint , self . root ) for datapoint in X ]) predict_proba ( X ) \u2693\ufe0e Predict class probabilities for the given data For all data points in the dataset traverse the decision tree until it reaches a leaf node and return the class probabilities of that leaf node. Parameters: Name Type Description Default X 2d ndarray The data that we want to use to make predictions required Returns: Type Description 2d ndarray All probabilites with size n x k, where n is the number of data points and k is the number classes Source code in mlproject/decision_tree/_decision_tree.py 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def predict_proba ( self , X ): \"\"\"Predict class probabilities for the given data For all data points in the dataset traverse the decision tree until it reaches a leaf node and return the class probabilities of that leaf node. Parameters ---------- X : 2d ndarray The data that we want to use to make predictions Returns ------- 2d ndarray All probabilites with size n x k, where n is the number of data points and k is the number classes \"\"\" return np . array ( [ self . _traverse ( datapoint , self . root , prob = True ) for datapoint in X ] ) Node \u2693\ufe0e Node object for building a decision tree. Parameters: Name Type Description Default feature int index, optional index of the best feature for splitting this Node, by default None None threshold float , optional the threshold for the best split of the data, by default None None left Node , optional the left child of this Node also of class Node, by default None None right Node , optional the right child of this Node also of class Node, by default None None majority_class int , optional The majority class in this node, only if this Node is a leaf, by default None None class_probs 1d ndarray, optional An array of class probabilities for this node, only if this Node is a leaf, by default None None Source code in mlproject/decision_tree/_node.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 class Node : \"\"\"Node object for building a decision tree. Parameters ---------- feature : int index, optional index of the best feature for splitting this Node, by default None threshold : float, optional the threshold for the best split of the data, by default None left : Node, optional the left child of this Node also of class Node, by default None right : Node, optional the right child of this Node also of class Node, by default None majority_class : int, optional The majority class in this node, only if this Node is a leaf, by default None class_probs : 1d ndarray, optional An array of class probabilities for this node, only if this Node is a leaf, by default None \"\"\" def __init__ ( self , left = None , right = None , feature = None , threshold = None , * , majority_class = None , class_probs = None ): self . feature = feature self . threshold = threshold self . left , self . right = left , right self . majority_class = majority_class self . class_probs = class_probs def is_leaf ( self ): \"\"\"Returns True if this Node is a leaf node, otherwise False Returns ------- bool True if this Node is a leaf node, otherwise False \"\"\" return self . majority_class is not None is_leaf () \u2693\ufe0e Returns True if this Node is a leaf node, otherwise False Returns: Type Description bool True if this Node is a leaf node, otherwise False Source code in mlproject/decision_tree/_node.py 30 31 32 33 34 35 36 37 38 39 def is_leaf ( self ): \"\"\"Returns True if this Node is a leaf node, otherwise False Returns ------- bool True if this Node is a leaf node, otherwise False \"\"\" return self . majority_class is not None","title":"decision_tree"},{"location":"reference/mlproject/decision_tree/#mlproject.decision_tree.DecisionTreeClassifier","text":"Decision Tree Classifier Simple decision tree classifier with user specific impurity, max depth and minimum number of samples in leaf nodes. Parameters: Name Type Description Default criterion str , optional The impurity criterion to use when splitting nodes, by default 'gini' 'gini' max_depth int , optional The maximum depth of the decision tree, by default 100 100 min_samples_in_leaf int , optional The minimum number of samples that need to be in a leaf, by default 2 2 Source code in mlproject/decision_tree/_decision_tree.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 class DecisionTreeClassifier : \"\"\"Decision Tree Classifier Simple decision tree classifier with user specific impurity, max depth and minimum number of samples in leaf nodes. Parameters ---------- criterion : str, optional The impurity criterion to use when splitting nodes, by default 'gini' max_depth : int, optional The maximum depth of the decision tree, by default 100 min_samples_in_leaf : int, optional The minimum number of samples that need to be in a leaf, by default 2 \"\"\" def __init__ ( self , criterion = \"gini\" , max_depth = 100 , min_samples_in_leaf = 2 ): self . max_depth = max_depth self . min_samples_in_leaf = min_samples_in_leaf self . root = None if criterion . lower () == \"gini\" : self . criterion = gini_impurity elif criterion . lower () == \"entropy\" : self . criterion = entropy_impurity def fit ( self , X , y ): \"\"\"Fit the decision tree to the given data Parameters ---------- X : 2d ndarray The data to be used for fitting the decision tree y : 2d ndarray An array of the true labels for the data points \"\"\" with progress as pb : t1 = pb . add_task ( \"[blue]Training\" , total = 1 ) self . root = self . _grow ( X , y ) pb . update ( t1 , advance = 1 ) if progress . finished : pb . update ( t1 , description = \"[bright_green]Training complete!\" ) def predict ( self , X ): \"\"\"Predict class labels for the given data. For all data points in the dataset traverse the decision tree until it reaches a leaf node and return the majority class of that leaf node. Parameters ---------- X : 2d ndarray The data that we want to use to make predictions. Returns ------- 1d ndarray All predicted class labels with size n, where n is the number of data points. \"\"\" return np . array ([ self . _traverse ( datapoint , self . root ) for datapoint in X ]) def predict_proba ( self , X ): \"\"\"Predict class probabilities for the given data For all data points in the dataset traverse the decision tree until it reaches a leaf node and return the class probabilities of that leaf node. Parameters ---------- X : 2d ndarray The data that we want to use to make predictions Returns ------- 2d ndarray All probabilites with size n x k, where n is the number of data points and k is the number classes \"\"\" return np . array ( [ self . _traverse ( datapoint , self . root , prob = True ) for datapoint in X ] ) def _grow ( self , X , y , cur_depth = 0 ): \"\"\"Grows a decision tree from the given data. This is the part that is doing the actual fitting of the decision tree. Parameters ---------- X : 2d ndarray The data to use when growing the decision tree y : 2d ndarray array of the true class labels cur_depth : int, optional The current depth of the decision tree, by default 0 Returns ------- Node A new node of class Node with new left and right children. \"\"\" self . n , self . p = X . shape node_unique_classes = np . unique ( y ) self . node_k = len ( node_unique_classes ) if ( cur_depth >= self . max_depth or self . n < self . min_samples_in_leaf or self . node_k == 1 ): most_common = self . _most_common_label ( y , prob = False ) class_probs = self . _most_common_label ( y , prob = True ) return Node ( majority_class = most_common , class_probs = class_probs ) cur_depth += 1 best_feature , best_threshold = self . _best_split ( X , y ) left_idxs = np . argwhere ( X [:, best_feature ] <= best_threshold ) . flatten () right_idxs = np . argwhere ( X [:, best_feature ] > best_threshold ) . flatten () left = self . _grow ( X [ left_idxs , :], y [ left_idxs ], cur_depth ) right = self . _grow ( X [ right_idxs , :], y [ right_idxs ], cur_depth ) return Node ( left , right , best_feature , best_threshold ) def _best_split ( self , X , y ): \"\"\"Calculates the best split of a node with the given data points Parameters ---------- X : 2d ndarray The data points to consider for splitting this node y : 2d ndarray The true labels to consider for splitting this node Returns ------- tuple A tuple containing the best index and threshold for the split \"\"\" best_gain = - np . inf for feat_idx in range ( X . shape [ 1 ]): feature_col = X [:, feat_idx ] possible_splits = np . unique ( feature_col ) for split in possible_splits : cur_gain = self . _information_gain ( y , feature_col , split ) if cur_gain > best_gain : best_gain = cur_gain split_idx = feat_idx split_thresh = split return split_idx , split_thresh def _information_gain ( self , y , feature_col , split_thresh ): \"\"\"Calculates the information gain of a node with the given data labels Parameters ---------- y : 2d ndarray array of true labels for this node feature_col : 2d ndarray Column of dataset containing the data points of the best feature for this split split_thresh : float or int the threshold for the best split of the data Returns ------- float The information gain from this node compared to it's parent \"\"\" parent_impurity = self . criterion ( y ) left_idxs = np . argwhere ( feature_col <= split_thresh ) . flatten () right_idxs = np . argwhere ( feature_col > split_thresh ) . flatten () if len ( left_idxs ) == 0 or len ( right_idxs ) == 0 : return 0 n = len ( y ) left_prob = len ( left_idxs ) / n right_prob = len ( right_idxs ) / n left_impurity = self . criterion ( y [ left_idxs ]) right_impurity = self . criterion ( y [ right_idxs ]) weighted_impurity = left_prob * left_impurity + right_prob * right_impurity information_gain = parent_impurity - weighted_impurity return information_gain def _traverse ( self , X , node , prob = False ): \"\"\"Traverses the tree until it reaches a leaf node and returns either the majority class of that node or the class probabilities if prob is True. Parameters ---------- X : 2d ndarray The data points to use for traversing the tree node : Node The node to start the traversal from. prob : bool, optional used to specify whether or not to return class probabilities, by default False Returns ------- int or bool Depending on `prob` it either returns the majority class or the class probabilities \"\"\" if node . is_leaf (): if prob : return np . argmax ( node . class_probs ) return node . majority_class if X [ node . feature ] <= node . threshold : return self . _traverse ( X , node . left ) elif X [ node . feature ] > node . threshold : return self . _traverse ( X , node . right ) def _most_common_label ( self , y , prob = False ): \"\"\"Calculates the most common label of a leaf node or the class probabilities Parameters ---------- y : 2d ndarray Array of true labels for this particular node prob : bool, optional used to specify whether or not to return class probabilities, by default False Returns ------- int or bool Depending on `prob` it either returns the majority class or the class probabilities \"\"\" uniques , counts = np . unique ( y , return_counts = True ) label_counts = dict ( zip ( uniques , counts )) sorted_keys = list ( sorted ( label_counts . items (), key = lambda x : x [ 1 ], reverse = True ) ) if prob : n = np . sum ( counts ) return np . array ([ label_counts [ i ] / n for i in uniques ]) return sorted_keys [ 0 ][ 0 ]","title":"DecisionTreeClassifier"},{"location":"reference/mlproject/decision_tree/#mlproject.decision_tree._decision_tree.DecisionTreeClassifier.fit","text":"Fit the decision tree to the given data Parameters: Name Type Description Default X 2d ndarray The data to be used for fitting the decision tree required y 2d ndarray An array of the true labels for the data points required Source code in mlproject/decision_tree/_decision_tree.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def fit ( self , X , y ): \"\"\"Fit the decision tree to the given data Parameters ---------- X : 2d ndarray The data to be used for fitting the decision tree y : 2d ndarray An array of the true labels for the data points \"\"\" with progress as pb : t1 = pb . add_task ( \"[blue]Training\" , total = 1 ) self . root = self . _grow ( X , y ) pb . update ( t1 , advance = 1 ) if progress . finished : pb . update ( t1 , description = \"[bright_green]Training complete!\" )","title":"fit()"},{"location":"reference/mlproject/decision_tree/#mlproject.decision_tree._decision_tree.DecisionTreeClassifier.predict","text":"Predict class labels for the given data. For all data points in the dataset traverse the decision tree until it reaches a leaf node and return the majority class of that leaf node. Parameters: Name Type Description Default X 2d ndarray The data that we want to use to make predictions. required Returns: Type Description 1d ndarray All predicted class labels with size n, where n is the number of data points. Source code in mlproject/decision_tree/_decision_tree.py 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def predict ( self , X ): \"\"\"Predict class labels for the given data. For all data points in the dataset traverse the decision tree until it reaches a leaf node and return the majority class of that leaf node. Parameters ---------- X : 2d ndarray The data that we want to use to make predictions. Returns ------- 1d ndarray All predicted class labels with size n, where n is the number of data points. \"\"\" return np . array ([ self . _traverse ( datapoint , self . root ) for datapoint in X ])","title":"predict()"},{"location":"reference/mlproject/decision_tree/#mlproject.decision_tree._decision_tree.DecisionTreeClassifier.predict_proba","text":"Predict class probabilities for the given data For all data points in the dataset traverse the decision tree until it reaches a leaf node and return the class probabilities of that leaf node. Parameters: Name Type Description Default X 2d ndarray The data that we want to use to make predictions required Returns: Type Description 2d ndarray All probabilites with size n x k, where n is the number of data points and k is the number classes Source code in mlproject/decision_tree/_decision_tree.py 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def predict_proba ( self , X ): \"\"\"Predict class probabilities for the given data For all data points in the dataset traverse the decision tree until it reaches a leaf node and return the class probabilities of that leaf node. Parameters ---------- X : 2d ndarray The data that we want to use to make predictions Returns ------- 2d ndarray All probabilites with size n x k, where n is the number of data points and k is the number classes \"\"\" return np . array ( [ self . _traverse ( datapoint , self . root , prob = True ) for datapoint in X ] )","title":"predict_proba()"},{"location":"reference/mlproject/decision_tree/#mlproject.decision_tree.Node","text":"Node object for building a decision tree. Parameters: Name Type Description Default feature int index, optional index of the best feature for splitting this Node, by default None None threshold float , optional the threshold for the best split of the data, by default None None left Node , optional the left child of this Node also of class Node, by default None None right Node , optional the right child of this Node also of class Node, by default None None majority_class int , optional The majority class in this node, only if this Node is a leaf, by default None None class_probs 1d ndarray, optional An array of class probabilities for this node, only if this Node is a leaf, by default None None Source code in mlproject/decision_tree/_node.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 class Node : \"\"\"Node object for building a decision tree. Parameters ---------- feature : int index, optional index of the best feature for splitting this Node, by default None threshold : float, optional the threshold for the best split of the data, by default None left : Node, optional the left child of this Node also of class Node, by default None right : Node, optional the right child of this Node also of class Node, by default None majority_class : int, optional The majority class in this node, only if this Node is a leaf, by default None class_probs : 1d ndarray, optional An array of class probabilities for this node, only if this Node is a leaf, by default None \"\"\" def __init__ ( self , left = None , right = None , feature = None , threshold = None , * , majority_class = None , class_probs = None ): self . feature = feature self . threshold = threshold self . left , self . right = left , right self . majority_class = majority_class self . class_probs = class_probs def is_leaf ( self ): \"\"\"Returns True if this Node is a leaf node, otherwise False Returns ------- bool True if this Node is a leaf node, otherwise False \"\"\" return self . majority_class is not None","title":"Node"},{"location":"reference/mlproject/decision_tree/#mlproject.decision_tree._node.Node.is_leaf","text":"Returns True if this Node is a leaf node, otherwise False Returns: Type Description bool True if this Node is a leaf node, otherwise False Source code in mlproject/decision_tree/_node.py 30 31 32 33 34 35 36 37 38 39 def is_leaf ( self ): \"\"\"Returns True if this Node is a leaf node, otherwise False Returns ------- bool True if this Node is a leaf node, otherwise False \"\"\" return self . majority_class is not None","title":"is_leaf()"},{"location":"reference/mlproject/decision_tree/_decision_tree/","text":"DecisionTreeClassifier \u2693\ufe0e Decision Tree Classifier Simple decision tree classifier with user specific impurity, max depth and minimum number of samples in leaf nodes. Parameters: Name Type Description Default criterion str , optional The impurity criterion to use when splitting nodes, by default 'gini' 'gini' max_depth int , optional The maximum depth of the decision tree, by default 100 100 min_samples_in_leaf int , optional The minimum number of samples that need to be in a leaf, by default 2 2 Source code in mlproject/decision_tree/_decision_tree.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 class DecisionTreeClassifier : \"\"\"Decision Tree Classifier Simple decision tree classifier with user specific impurity, max depth and minimum number of samples in leaf nodes. Parameters ---------- criterion : str, optional The impurity criterion to use when splitting nodes, by default 'gini' max_depth : int, optional The maximum depth of the decision tree, by default 100 min_samples_in_leaf : int, optional The minimum number of samples that need to be in a leaf, by default 2 \"\"\" def __init__ ( self , criterion = \"gini\" , max_depth = 100 , min_samples_in_leaf = 2 ): self . max_depth = max_depth self . min_samples_in_leaf = min_samples_in_leaf self . root = None if criterion . lower () == \"gini\" : self . criterion = gini_impurity elif criterion . lower () == \"entropy\" : self . criterion = entropy_impurity def fit ( self , X , y ): \"\"\"Fit the decision tree to the given data Parameters ---------- X : 2d ndarray The data to be used for fitting the decision tree y : 2d ndarray An array of the true labels for the data points \"\"\" with progress as pb : t1 = pb . add_task ( \"[blue]Training\" , total = 1 ) self . root = self . _grow ( X , y ) pb . update ( t1 , advance = 1 ) if progress . finished : pb . update ( t1 , description = \"[bright_green]Training complete!\" ) def predict ( self , X ): \"\"\"Predict class labels for the given data. For all data points in the dataset traverse the decision tree until it reaches a leaf node and return the majority class of that leaf node. Parameters ---------- X : 2d ndarray The data that we want to use to make predictions. Returns ------- 1d ndarray All predicted class labels with size n, where n is the number of data points. \"\"\" return np . array ([ self . _traverse ( datapoint , self . root ) for datapoint in X ]) def predict_proba ( self , X ): \"\"\"Predict class probabilities for the given data For all data points in the dataset traverse the decision tree until it reaches a leaf node and return the class probabilities of that leaf node. Parameters ---------- X : 2d ndarray The data that we want to use to make predictions Returns ------- 2d ndarray All probabilites with size n x k, where n is the number of data points and k is the number classes \"\"\" return np . array ( [ self . _traverse ( datapoint , self . root , prob = True ) for datapoint in X ] ) def _grow ( self , X , y , cur_depth = 0 ): \"\"\"Grows a decision tree from the given data. This is the part that is doing the actual fitting of the decision tree. Parameters ---------- X : 2d ndarray The data to use when growing the decision tree y : 2d ndarray array of the true class labels cur_depth : int, optional The current depth of the decision tree, by default 0 Returns ------- Node A new node of class Node with new left and right children. \"\"\" self . n , self . p = X . shape node_unique_classes = np . unique ( y ) self . node_k = len ( node_unique_classes ) if ( cur_depth >= self . max_depth or self . n < self . min_samples_in_leaf or self . node_k == 1 ): most_common = self . _most_common_label ( y , prob = False ) class_probs = self . _most_common_label ( y , prob = True ) return Node ( majority_class = most_common , class_probs = class_probs ) cur_depth += 1 best_feature , best_threshold = self . _best_split ( X , y ) left_idxs = np . argwhere ( X [:, best_feature ] <= best_threshold ) . flatten () right_idxs = np . argwhere ( X [:, best_feature ] > best_threshold ) . flatten () left = self . _grow ( X [ left_idxs , :], y [ left_idxs ], cur_depth ) right = self . _grow ( X [ right_idxs , :], y [ right_idxs ], cur_depth ) return Node ( left , right , best_feature , best_threshold ) def _best_split ( self , X , y ): \"\"\"Calculates the best split of a node with the given data points Parameters ---------- X : 2d ndarray The data points to consider for splitting this node y : 2d ndarray The true labels to consider for splitting this node Returns ------- tuple A tuple containing the best index and threshold for the split \"\"\" best_gain = - np . inf for feat_idx in range ( X . shape [ 1 ]): feature_col = X [:, feat_idx ] possible_splits = np . unique ( feature_col ) for split in possible_splits : cur_gain = self . _information_gain ( y , feature_col , split ) if cur_gain > best_gain : best_gain = cur_gain split_idx = feat_idx split_thresh = split return split_idx , split_thresh def _information_gain ( self , y , feature_col , split_thresh ): \"\"\"Calculates the information gain of a node with the given data labels Parameters ---------- y : 2d ndarray array of true labels for this node feature_col : 2d ndarray Column of dataset containing the data points of the best feature for this split split_thresh : float or int the threshold for the best split of the data Returns ------- float The information gain from this node compared to it's parent \"\"\" parent_impurity = self . criterion ( y ) left_idxs = np . argwhere ( feature_col <= split_thresh ) . flatten () right_idxs = np . argwhere ( feature_col > split_thresh ) . flatten () if len ( left_idxs ) == 0 or len ( right_idxs ) == 0 : return 0 n = len ( y ) left_prob = len ( left_idxs ) / n right_prob = len ( right_idxs ) / n left_impurity = self . criterion ( y [ left_idxs ]) right_impurity = self . criterion ( y [ right_idxs ]) weighted_impurity = left_prob * left_impurity + right_prob * right_impurity information_gain = parent_impurity - weighted_impurity return information_gain def _traverse ( self , X , node , prob = False ): \"\"\"Traverses the tree until it reaches a leaf node and returns either the majority class of that node or the class probabilities if prob is True. Parameters ---------- X : 2d ndarray The data points to use for traversing the tree node : Node The node to start the traversal from. prob : bool, optional used to specify whether or not to return class probabilities, by default False Returns ------- int or bool Depending on `prob` it either returns the majority class or the class probabilities \"\"\" if node . is_leaf (): if prob : return np . argmax ( node . class_probs ) return node . majority_class if X [ node . feature ] <= node . threshold : return self . _traverse ( X , node . left ) elif X [ node . feature ] > node . threshold : return self . _traverse ( X , node . right ) def _most_common_label ( self , y , prob = False ): \"\"\"Calculates the most common label of a leaf node or the class probabilities Parameters ---------- y : 2d ndarray Array of true labels for this particular node prob : bool, optional used to specify whether or not to return class probabilities, by default False Returns ------- int or bool Depending on `prob` it either returns the majority class or the class probabilities \"\"\" uniques , counts = np . unique ( y , return_counts = True ) label_counts = dict ( zip ( uniques , counts )) sorted_keys = list ( sorted ( label_counts . items (), key = lambda x : x [ 1 ], reverse = True ) ) if prob : n = np . sum ( counts ) return np . array ([ label_counts [ i ] / n for i in uniques ]) return sorted_keys [ 0 ][ 0 ] fit ( X , y ) \u2693\ufe0e Fit the decision tree to the given data Parameters: Name Type Description Default X 2d ndarray The data to be used for fitting the decision tree required y 2d ndarray An array of the true labels for the data points required Source code in mlproject/decision_tree/_decision_tree.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def fit ( self , X , y ): \"\"\"Fit the decision tree to the given data Parameters ---------- X : 2d ndarray The data to be used for fitting the decision tree y : 2d ndarray An array of the true labels for the data points \"\"\" with progress as pb : t1 = pb . add_task ( \"[blue]Training\" , total = 1 ) self . root = self . _grow ( X , y ) pb . update ( t1 , advance = 1 ) if progress . finished : pb . update ( t1 , description = \"[bright_green]Training complete!\" ) predict ( X ) \u2693\ufe0e Predict class labels for the given data. For all data points in the dataset traverse the decision tree until it reaches a leaf node and return the majority class of that leaf node. Parameters: Name Type Description Default X 2d ndarray The data that we want to use to make predictions. required Returns: Type Description 1d ndarray All predicted class labels with size n, where n is the number of data points. Source code in mlproject/decision_tree/_decision_tree.py 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def predict ( self , X ): \"\"\"Predict class labels for the given data. For all data points in the dataset traverse the decision tree until it reaches a leaf node and return the majority class of that leaf node. Parameters ---------- X : 2d ndarray The data that we want to use to make predictions. Returns ------- 1d ndarray All predicted class labels with size n, where n is the number of data points. \"\"\" return np . array ([ self . _traverse ( datapoint , self . root ) for datapoint in X ]) predict_proba ( X ) \u2693\ufe0e Predict class probabilities for the given data For all data points in the dataset traverse the decision tree until it reaches a leaf node and return the class probabilities of that leaf node. Parameters: Name Type Description Default X 2d ndarray The data that we want to use to make predictions required Returns: Type Description 2d ndarray All probabilites with size n x k, where n is the number of data points and k is the number classes Source code in mlproject/decision_tree/_decision_tree.py 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def predict_proba ( self , X ): \"\"\"Predict class probabilities for the given data For all data points in the dataset traverse the decision tree until it reaches a leaf node and return the class probabilities of that leaf node. Parameters ---------- X : 2d ndarray The data that we want to use to make predictions Returns ------- 2d ndarray All probabilites with size n x k, where n is the number of data points and k is the number classes \"\"\" return np . array ( [ self . _traverse ( datapoint , self . root , prob = True ) for datapoint in X ] )","title":"_decision_tree"},{"location":"reference/mlproject/decision_tree/_decision_tree/#mlproject.decision_tree._decision_tree.DecisionTreeClassifier","text":"Decision Tree Classifier Simple decision tree classifier with user specific impurity, max depth and minimum number of samples in leaf nodes. Parameters: Name Type Description Default criterion str , optional The impurity criterion to use when splitting nodes, by default 'gini' 'gini' max_depth int , optional The maximum depth of the decision tree, by default 100 100 min_samples_in_leaf int , optional The minimum number of samples that need to be in a leaf, by default 2 2 Source code in mlproject/decision_tree/_decision_tree.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 class DecisionTreeClassifier : \"\"\"Decision Tree Classifier Simple decision tree classifier with user specific impurity, max depth and minimum number of samples in leaf nodes. Parameters ---------- criterion : str, optional The impurity criterion to use when splitting nodes, by default 'gini' max_depth : int, optional The maximum depth of the decision tree, by default 100 min_samples_in_leaf : int, optional The minimum number of samples that need to be in a leaf, by default 2 \"\"\" def __init__ ( self , criterion = \"gini\" , max_depth = 100 , min_samples_in_leaf = 2 ): self . max_depth = max_depth self . min_samples_in_leaf = min_samples_in_leaf self . root = None if criterion . lower () == \"gini\" : self . criterion = gini_impurity elif criterion . lower () == \"entropy\" : self . criterion = entropy_impurity def fit ( self , X , y ): \"\"\"Fit the decision tree to the given data Parameters ---------- X : 2d ndarray The data to be used for fitting the decision tree y : 2d ndarray An array of the true labels for the data points \"\"\" with progress as pb : t1 = pb . add_task ( \"[blue]Training\" , total = 1 ) self . root = self . _grow ( X , y ) pb . update ( t1 , advance = 1 ) if progress . finished : pb . update ( t1 , description = \"[bright_green]Training complete!\" ) def predict ( self , X ): \"\"\"Predict class labels for the given data. For all data points in the dataset traverse the decision tree until it reaches a leaf node and return the majority class of that leaf node. Parameters ---------- X : 2d ndarray The data that we want to use to make predictions. Returns ------- 1d ndarray All predicted class labels with size n, where n is the number of data points. \"\"\" return np . array ([ self . _traverse ( datapoint , self . root ) for datapoint in X ]) def predict_proba ( self , X ): \"\"\"Predict class probabilities for the given data For all data points in the dataset traverse the decision tree until it reaches a leaf node and return the class probabilities of that leaf node. Parameters ---------- X : 2d ndarray The data that we want to use to make predictions Returns ------- 2d ndarray All probabilites with size n x k, where n is the number of data points and k is the number classes \"\"\" return np . array ( [ self . _traverse ( datapoint , self . root , prob = True ) for datapoint in X ] ) def _grow ( self , X , y , cur_depth = 0 ): \"\"\"Grows a decision tree from the given data. This is the part that is doing the actual fitting of the decision tree. Parameters ---------- X : 2d ndarray The data to use when growing the decision tree y : 2d ndarray array of the true class labels cur_depth : int, optional The current depth of the decision tree, by default 0 Returns ------- Node A new node of class Node with new left and right children. \"\"\" self . n , self . p = X . shape node_unique_classes = np . unique ( y ) self . node_k = len ( node_unique_classes ) if ( cur_depth >= self . max_depth or self . n < self . min_samples_in_leaf or self . node_k == 1 ): most_common = self . _most_common_label ( y , prob = False ) class_probs = self . _most_common_label ( y , prob = True ) return Node ( majority_class = most_common , class_probs = class_probs ) cur_depth += 1 best_feature , best_threshold = self . _best_split ( X , y ) left_idxs = np . argwhere ( X [:, best_feature ] <= best_threshold ) . flatten () right_idxs = np . argwhere ( X [:, best_feature ] > best_threshold ) . flatten () left = self . _grow ( X [ left_idxs , :], y [ left_idxs ], cur_depth ) right = self . _grow ( X [ right_idxs , :], y [ right_idxs ], cur_depth ) return Node ( left , right , best_feature , best_threshold ) def _best_split ( self , X , y ): \"\"\"Calculates the best split of a node with the given data points Parameters ---------- X : 2d ndarray The data points to consider for splitting this node y : 2d ndarray The true labels to consider for splitting this node Returns ------- tuple A tuple containing the best index and threshold for the split \"\"\" best_gain = - np . inf for feat_idx in range ( X . shape [ 1 ]): feature_col = X [:, feat_idx ] possible_splits = np . unique ( feature_col ) for split in possible_splits : cur_gain = self . _information_gain ( y , feature_col , split ) if cur_gain > best_gain : best_gain = cur_gain split_idx = feat_idx split_thresh = split return split_idx , split_thresh def _information_gain ( self , y , feature_col , split_thresh ): \"\"\"Calculates the information gain of a node with the given data labels Parameters ---------- y : 2d ndarray array of true labels for this node feature_col : 2d ndarray Column of dataset containing the data points of the best feature for this split split_thresh : float or int the threshold for the best split of the data Returns ------- float The information gain from this node compared to it's parent \"\"\" parent_impurity = self . criterion ( y ) left_idxs = np . argwhere ( feature_col <= split_thresh ) . flatten () right_idxs = np . argwhere ( feature_col > split_thresh ) . flatten () if len ( left_idxs ) == 0 or len ( right_idxs ) == 0 : return 0 n = len ( y ) left_prob = len ( left_idxs ) / n right_prob = len ( right_idxs ) / n left_impurity = self . criterion ( y [ left_idxs ]) right_impurity = self . criterion ( y [ right_idxs ]) weighted_impurity = left_prob * left_impurity + right_prob * right_impurity information_gain = parent_impurity - weighted_impurity return information_gain def _traverse ( self , X , node , prob = False ): \"\"\"Traverses the tree until it reaches a leaf node and returns either the majority class of that node or the class probabilities if prob is True. Parameters ---------- X : 2d ndarray The data points to use for traversing the tree node : Node The node to start the traversal from. prob : bool, optional used to specify whether or not to return class probabilities, by default False Returns ------- int or bool Depending on `prob` it either returns the majority class or the class probabilities \"\"\" if node . is_leaf (): if prob : return np . argmax ( node . class_probs ) return node . majority_class if X [ node . feature ] <= node . threshold : return self . _traverse ( X , node . left ) elif X [ node . feature ] > node . threshold : return self . _traverse ( X , node . right ) def _most_common_label ( self , y , prob = False ): \"\"\"Calculates the most common label of a leaf node or the class probabilities Parameters ---------- y : 2d ndarray Array of true labels for this particular node prob : bool, optional used to specify whether or not to return class probabilities, by default False Returns ------- int or bool Depending on `prob` it either returns the majority class or the class probabilities \"\"\" uniques , counts = np . unique ( y , return_counts = True ) label_counts = dict ( zip ( uniques , counts )) sorted_keys = list ( sorted ( label_counts . items (), key = lambda x : x [ 1 ], reverse = True ) ) if prob : n = np . sum ( counts ) return np . array ([ label_counts [ i ] / n for i in uniques ]) return sorted_keys [ 0 ][ 0 ]","title":"DecisionTreeClassifier"},{"location":"reference/mlproject/decision_tree/_decision_tree/#mlproject.decision_tree._decision_tree.DecisionTreeClassifier.fit","text":"Fit the decision tree to the given data Parameters: Name Type Description Default X 2d ndarray The data to be used for fitting the decision tree required y 2d ndarray An array of the true labels for the data points required Source code in mlproject/decision_tree/_decision_tree.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def fit ( self , X , y ): \"\"\"Fit the decision tree to the given data Parameters ---------- X : 2d ndarray The data to be used for fitting the decision tree y : 2d ndarray An array of the true labels for the data points \"\"\" with progress as pb : t1 = pb . add_task ( \"[blue]Training\" , total = 1 ) self . root = self . _grow ( X , y ) pb . update ( t1 , advance = 1 ) if progress . finished : pb . update ( t1 , description = \"[bright_green]Training complete!\" )","title":"fit()"},{"location":"reference/mlproject/decision_tree/_decision_tree/#mlproject.decision_tree._decision_tree.DecisionTreeClassifier.predict","text":"Predict class labels for the given data. For all data points in the dataset traverse the decision tree until it reaches a leaf node and return the majority class of that leaf node. Parameters: Name Type Description Default X 2d ndarray The data that we want to use to make predictions. required Returns: Type Description 1d ndarray All predicted class labels with size n, where n is the number of data points. Source code in mlproject/decision_tree/_decision_tree.py 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def predict ( self , X ): \"\"\"Predict class labels for the given data. For all data points in the dataset traverse the decision tree until it reaches a leaf node and return the majority class of that leaf node. Parameters ---------- X : 2d ndarray The data that we want to use to make predictions. Returns ------- 1d ndarray All predicted class labels with size n, where n is the number of data points. \"\"\" return np . array ([ self . _traverse ( datapoint , self . root ) for datapoint in X ])","title":"predict()"},{"location":"reference/mlproject/decision_tree/_decision_tree/#mlproject.decision_tree._decision_tree.DecisionTreeClassifier.predict_proba","text":"Predict class probabilities for the given data For all data points in the dataset traverse the decision tree until it reaches a leaf node and return the class probabilities of that leaf node. Parameters: Name Type Description Default X 2d ndarray The data that we want to use to make predictions required Returns: Type Description 2d ndarray All probabilites with size n x k, where n is the number of data points and k is the number classes Source code in mlproject/decision_tree/_decision_tree.py 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def predict_proba ( self , X ): \"\"\"Predict class probabilities for the given data For all data points in the dataset traverse the decision tree until it reaches a leaf node and return the class probabilities of that leaf node. Parameters ---------- X : 2d ndarray The data that we want to use to make predictions Returns ------- 2d ndarray All probabilites with size n x k, where n is the number of data points and k is the number classes \"\"\" return np . array ( [ self . _traverse ( datapoint , self . root , prob = True ) for datapoint in X ] )","title":"predict_proba()"},{"location":"reference/mlproject/decision_tree/_impurity/","text":"entropy_impurity ( y ) \u2693\ufe0e Calculates the entropy of a given node Parameters: Name Type Description Default y 2d ndarray array of y labels required Returns: Type Description float entropy impurity of a given node Source code in mlproject/decision_tree/_impurity.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 def entropy_impurity ( y ): \"\"\"Calculates the entropy of a given node Parameters ---------- y : 2d ndarray array of y labels Returns ------- float entropy impurity of a given node \"\"\" epsilon = 1e-07 # flatten the array only because np.bincount expects a 1 dimensional array y = y . flatten () counts = np . bincount ( y ) N = np . sum ( counts ) p = counts / N return np . sum ( - p * np . log2 ( p + epsilon )) gini_impurity ( y ) \u2693\ufe0e Calculates the gini impurity of a given node Parameters: Name Type Description Default y 2d ndarray array of y labels required Returns: Type Description float gini impurity score for the node Source code in mlproject/decision_tree/_impurity.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 def gini_impurity ( y ): \"\"\"Calculates the gini impurity of a given node Parameters ---------- y : 2d ndarray array of y labels Returns ------- float gini impurity score for the node \"\"\" # flatten the array only because np.bincount expects a 1 dimensional array y = y . flatten () counts = np . bincount ( y ) N = np . sum ( counts ) p = counts / N return 1 - np . sum ( p ** 2 )","title":"_impurity"},{"location":"reference/mlproject/decision_tree/_impurity/#mlproject.decision_tree._impurity.entropy_impurity","text":"Calculates the entropy of a given node Parameters: Name Type Description Default y 2d ndarray array of y labels required Returns: Type Description float entropy impurity of a given node Source code in mlproject/decision_tree/_impurity.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 def entropy_impurity ( y ): \"\"\"Calculates the entropy of a given node Parameters ---------- y : 2d ndarray array of y labels Returns ------- float entropy impurity of a given node \"\"\" epsilon = 1e-07 # flatten the array only because np.bincount expects a 1 dimensional array y = y . flatten () counts = np . bincount ( y ) N = np . sum ( counts ) p = counts / N return np . sum ( - p * np . log2 ( p + epsilon ))","title":"entropy_impurity()"},{"location":"reference/mlproject/decision_tree/_impurity/#mlproject.decision_tree._impurity.gini_impurity","text":"Calculates the gini impurity of a given node Parameters: Name Type Description Default y 2d ndarray array of y labels required Returns: Type Description float gini impurity score for the node Source code in mlproject/decision_tree/_impurity.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 def gini_impurity ( y ): \"\"\"Calculates the gini impurity of a given node Parameters ---------- y : 2d ndarray array of y labels Returns ------- float gini impurity score for the node \"\"\" # flatten the array only because np.bincount expects a 1 dimensional array y = y . flatten () counts = np . bincount ( y ) N = np . sum ( counts ) p = counts / N return 1 - np . sum ( p ** 2 )","title":"gini_impurity()"},{"location":"reference/mlproject/decision_tree/_node/","text":"Node \u2693\ufe0e Node object for building a decision tree. Parameters: Name Type Description Default feature int index, optional index of the best feature for splitting this Node, by default None None threshold float , optional the threshold for the best split of the data, by default None None left Node , optional the left child of this Node also of class Node, by default None None right Node , optional the right child of this Node also of class Node, by default None None majority_class int , optional The majority class in this node, only if this Node is a leaf, by default None None class_probs 1d ndarray, optional An array of class probabilities for this node, only if this Node is a leaf, by default None None Source code in mlproject/decision_tree/_node.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 class Node : \"\"\"Node object for building a decision tree. Parameters ---------- feature : int index, optional index of the best feature for splitting this Node, by default None threshold : float, optional the threshold for the best split of the data, by default None left : Node, optional the left child of this Node also of class Node, by default None right : Node, optional the right child of this Node also of class Node, by default None majority_class : int, optional The majority class in this node, only if this Node is a leaf, by default None class_probs : 1d ndarray, optional An array of class probabilities for this node, only if this Node is a leaf, by default None \"\"\" def __init__ ( self , left = None , right = None , feature = None , threshold = None , * , majority_class = None , class_probs = None ): self . feature = feature self . threshold = threshold self . left , self . right = left , right self . majority_class = majority_class self . class_probs = class_probs def is_leaf ( self ): \"\"\"Returns True if this Node is a leaf node, otherwise False Returns ------- bool True if this Node is a leaf node, otherwise False \"\"\" return self . majority_class is not None is_leaf () \u2693\ufe0e Returns True if this Node is a leaf node, otherwise False Returns: Type Description bool True if this Node is a leaf node, otherwise False Source code in mlproject/decision_tree/_node.py 30 31 32 33 34 35 36 37 38 39 def is_leaf ( self ): \"\"\"Returns True if this Node is a leaf node, otherwise False Returns ------- bool True if this Node is a leaf node, otherwise False \"\"\" return self . majority_class is not None","title":"_node"},{"location":"reference/mlproject/decision_tree/_node/#mlproject.decision_tree._node.Node","text":"Node object for building a decision tree. Parameters: Name Type Description Default feature int index, optional index of the best feature for splitting this Node, by default None None threshold float , optional the threshold for the best split of the data, by default None None left Node , optional the left child of this Node also of class Node, by default None None right Node , optional the right child of this Node also of class Node, by default None None majority_class int , optional The majority class in this node, only if this Node is a leaf, by default None None class_probs 1d ndarray, optional An array of class probabilities for this node, only if this Node is a leaf, by default None None Source code in mlproject/decision_tree/_node.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 class Node : \"\"\"Node object for building a decision tree. Parameters ---------- feature : int index, optional index of the best feature for splitting this Node, by default None threshold : float, optional the threshold for the best split of the data, by default None left : Node, optional the left child of this Node also of class Node, by default None right : Node, optional the right child of this Node also of class Node, by default None majority_class : int, optional The majority class in this node, only if this Node is a leaf, by default None class_probs : 1d ndarray, optional An array of class probabilities for this node, only if this Node is a leaf, by default None \"\"\" def __init__ ( self , left = None , right = None , feature = None , threshold = None , * , majority_class = None , class_probs = None ): self . feature = feature self . threshold = threshold self . left , self . right = left , right self . majority_class = majority_class self . class_probs = class_probs def is_leaf ( self ): \"\"\"Returns True if this Node is a leaf node, otherwise False Returns ------- bool True if this Node is a leaf node, otherwise False \"\"\" return self . majority_class is not None","title":"Node"},{"location":"reference/mlproject/decision_tree/_node/#mlproject.decision_tree._node.Node.is_leaf","text":"Returns True if this Node is a leaf node, otherwise False Returns: Type Description bool True if this Node is a leaf node, otherwise False Source code in mlproject/decision_tree/_node.py 30 31 32 33 34 35 36 37 38 39 def is_leaf ( self ): \"\"\"Returns True if this Node is a leaf node, otherwise False Returns ------- bool True if this Node is a leaf node, otherwise False \"\"\" return self . majority_class is not None","title":"is_leaf()"},{"location":"reference/mlproject/helpers/","text":"accuracy_score ( y_true , y_pred , normalize = True ) \u2693\ufe0e Calculate the accuracy score from a given array of true labels and a given array of predicted labels. Inspired by https://stackoverflow.com/a/64680660 Parameters: Name Type Description Default y_true 2d ndarray array of shape (n_samples, 1) of true labels required y_pred 2d ndarray array of shape (n_samples, 1) of predicted labels required Returns: Name Type Description accuracy_scores float calculated accuracy score Raises: Type Description ValueError if y_true and y_pred are not of the same shape Source code in mlproject/helpers/_metrics.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def accuracy_score ( y_true , y_pred , normalize = True ): \"\"\"Calculate the accuracy score from a given array of true labels and a given array of predicted labels. Inspired by [https://stackoverflow.com/a/64680660](https://stackoverflow.com/a/64680660) Parameters ---------- y_true : 2d ndarray array of shape (n_samples, 1) of true labels y_pred : 2d ndarray array of shape (n_samples, 1) of predicted labels Returns ------- accuracy_scores : float calculated accuracy score Raises ------ ValueError if y_true and y_pred are not of the same shape \"\"\" if y_true . shape [ 0 ] != y_pred . shape [ 0 ] and y_true . shape [ 1 ] != y_pred . shape [ 1 ]: raise ValueError ( f \"Length of y_true: ( { len ( y_true ) } ) and y_pred: ( { len ( y_pred ) } ) should be the same!\" ) accuracy = [] for i in range ( len ( y_pred )): if y_pred [ i ] == y_true [ i ]: accuracy . append ( 1 ) else : accuracy . append ( 0 ) if normalize == True : return np . mean ( accuracy ) if normalize == False : return sum ( accuracy ) data_loader ( raw = True , scaled = False , pca = False ) \u2693\ufe0e Loads the fashion_mnist training and test data from the data directory. The function returns four numpy arrays containing the training and test data respectively. If specified it can also return the standard scaled version of the data or the first 10 principal components of the data. The different dimensions of the returned data is below: Raw Scaled PCA Training \\(X\\) \\((10.000 \\times 784)\\) \\((10.000 \\times 784)\\) \\((10.000 \\times 10)\\) \\(Y\\) \\((10.000 \\times 1)\\) \\((10.000 \\times 1)\\) \\((10.000 \\times 1)\\) Test \\(X\\) \\((5.000 \\times 784)\\) \\((5.000 \\times 784)\\) \\((5.000 \\times 10)\\) \\(Y\\) \\((5.000 \\times 1)\\) \\((5.000 \\times 1)\\) \\((5.000 \\times 1)\\) Returns: Type Description 2d ndarrays numpy data arrays in the order X_train, X_test, y_train, y_test. Source code in mlproject/helpers/_data_loader.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def data_loader ( raw = True , scaled = False , pca = False ): r \"\"\"Loads the fashion_mnist training and test data from the data directory. The function returns four numpy arrays containing the training and test data respectively. If specified it can also return the standard scaled version of the data or the first 10 principal components of the data. The different dimensions of the returned data is below: | | Raw | Scaled | PCA | |:------------:|:---------------------:|:---------------------:|:--------------------:| | **Training** | | | | | $X$ | $(10.000 \\times 784)$ | $(10.000 \\times 784)$ | $(10.000 \\times 10)$ | | $Y$ | $(10.000 \\times 1)$ | $(10.000 \\times 1)$ | $(10.000 \\times 1)$ | | **Test** | | | | | $X$ | $(5.000 \\times 784)$ | $(5.000 \\times 784)$ | $(5.000 \\times 10)$ | | $Y$ | $(5.000 \\times 1)$ | $(5.000 \\times 1)$ | $(5.000 \\times 1)$ | Returns ------- 2d ndarrays numpy data arrays in the order X_train, X_test, y_train, y_test. \"\"\" if raw : X_train , y_train = np . hsplit ( np . load ( f \" { ROOT_DIR } /data/fashion_train.npy\" ), [ - 1 ] ) X_test , y_test = np . hsplit ( np . load ( f \" { ROOT_DIR } /data/fashion_test.npy\" ), [ - 1 ]) elif scaled and not raw : X_train , y_train = np . hsplit ( np . load ( f \" { ROOT_DIR } /data/fashion_train_scaled.npy\" ), [ - 1 ] ) X_test , y_test = np . hsplit ( np . load ( f \" { ROOT_DIR } /data/fashion_test_scaled.npy\" ), [ - 1 ] ) # converting the y_labels back to integers from floats to avoid issues y_train , y_test = y_train . astype ( int ), y_test . astype ( int ) elif pca and not raw and not scaled : X_train , y_train = np . hsplit ( np . load ( f \" { ROOT_DIR } /data/fashion_train_pca.npy\" ), [ - 1 ] ) X_test , y_test = np . hsplit ( np . load ( f \" { ROOT_DIR } /data/fashion_test_pca.npy\" ), [ - 1 ] ) # converting the y_labels back to integers from floats to avoid issues y_train , y_test = y_train . astype ( int ), y_test . astype ( int ) return X_train , X_test , y_train , y_test","title":"helpers"},{"location":"reference/mlproject/helpers/#mlproject.helpers.accuracy_score","text":"Calculate the accuracy score from a given array of true labels and a given array of predicted labels. Inspired by https://stackoverflow.com/a/64680660 Parameters: Name Type Description Default y_true 2d ndarray array of shape (n_samples, 1) of true labels required y_pred 2d ndarray array of shape (n_samples, 1) of predicted labels required Returns: Name Type Description accuracy_scores float calculated accuracy score Raises: Type Description ValueError if y_true and y_pred are not of the same shape Source code in mlproject/helpers/_metrics.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def accuracy_score ( y_true , y_pred , normalize = True ): \"\"\"Calculate the accuracy score from a given array of true labels and a given array of predicted labels. Inspired by [https://stackoverflow.com/a/64680660](https://stackoverflow.com/a/64680660) Parameters ---------- y_true : 2d ndarray array of shape (n_samples, 1) of true labels y_pred : 2d ndarray array of shape (n_samples, 1) of predicted labels Returns ------- accuracy_scores : float calculated accuracy score Raises ------ ValueError if y_true and y_pred are not of the same shape \"\"\" if y_true . shape [ 0 ] != y_pred . shape [ 0 ] and y_true . shape [ 1 ] != y_pred . shape [ 1 ]: raise ValueError ( f \"Length of y_true: ( { len ( y_true ) } ) and y_pred: ( { len ( y_pred ) } ) should be the same!\" ) accuracy = [] for i in range ( len ( y_pred )): if y_pred [ i ] == y_true [ i ]: accuracy . append ( 1 ) else : accuracy . append ( 0 ) if normalize == True : return np . mean ( accuracy ) if normalize == False : return sum ( accuracy )","title":"accuracy_score()"},{"location":"reference/mlproject/helpers/#mlproject.helpers.data_loader","text":"Loads the fashion_mnist training and test data from the data directory. The function returns four numpy arrays containing the training and test data respectively. If specified it can also return the standard scaled version of the data or the first 10 principal components of the data. The different dimensions of the returned data is below: Raw Scaled PCA Training \\(X\\) \\((10.000 \\times 784)\\) \\((10.000 \\times 784)\\) \\((10.000 \\times 10)\\) \\(Y\\) \\((10.000 \\times 1)\\) \\((10.000 \\times 1)\\) \\((10.000 \\times 1)\\) Test \\(X\\) \\((5.000 \\times 784)\\) \\((5.000 \\times 784)\\) \\((5.000 \\times 10)\\) \\(Y\\) \\((5.000 \\times 1)\\) \\((5.000 \\times 1)\\) \\((5.000 \\times 1)\\) Returns: Type Description 2d ndarrays numpy data arrays in the order X_train, X_test, y_train, y_test. Source code in mlproject/helpers/_data_loader.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def data_loader ( raw = True , scaled = False , pca = False ): r \"\"\"Loads the fashion_mnist training and test data from the data directory. The function returns four numpy arrays containing the training and test data respectively. If specified it can also return the standard scaled version of the data or the first 10 principal components of the data. The different dimensions of the returned data is below: | | Raw | Scaled | PCA | |:------------:|:---------------------:|:---------------------:|:--------------------:| | **Training** | | | | | $X$ | $(10.000 \\times 784)$ | $(10.000 \\times 784)$ | $(10.000 \\times 10)$ | | $Y$ | $(10.000 \\times 1)$ | $(10.000 \\times 1)$ | $(10.000 \\times 1)$ | | **Test** | | | | | $X$ | $(5.000 \\times 784)$ | $(5.000 \\times 784)$ | $(5.000 \\times 10)$ | | $Y$ | $(5.000 \\times 1)$ | $(5.000 \\times 1)$ | $(5.000 \\times 1)$ | Returns ------- 2d ndarrays numpy data arrays in the order X_train, X_test, y_train, y_test. \"\"\" if raw : X_train , y_train = np . hsplit ( np . load ( f \" { ROOT_DIR } /data/fashion_train.npy\" ), [ - 1 ] ) X_test , y_test = np . hsplit ( np . load ( f \" { ROOT_DIR } /data/fashion_test.npy\" ), [ - 1 ]) elif scaled and not raw : X_train , y_train = np . hsplit ( np . load ( f \" { ROOT_DIR } /data/fashion_train_scaled.npy\" ), [ - 1 ] ) X_test , y_test = np . hsplit ( np . load ( f \" { ROOT_DIR } /data/fashion_test_scaled.npy\" ), [ - 1 ] ) # converting the y_labels back to integers from floats to avoid issues y_train , y_test = y_train . astype ( int ), y_test . astype ( int ) elif pca and not raw and not scaled : X_train , y_train = np . hsplit ( np . load ( f \" { ROOT_DIR } /data/fashion_train_pca.npy\" ), [ - 1 ] ) X_test , y_test = np . hsplit ( np . load ( f \" { ROOT_DIR } /data/fashion_test_pca.npy\" ), [ - 1 ] ) # converting the y_labels back to integers from floats to avoid issues y_train , y_test = y_train . astype ( int ), y_test . astype ( int ) return X_train , X_test , y_train , y_test","title":"data_loader()"},{"location":"reference/mlproject/helpers/_data_loader/","text":"data_loader ( raw = True , scaled = False , pca = False ) \u2693\ufe0e Loads the fashion_mnist training and test data from the data directory. The function returns four numpy arrays containing the training and test data respectively. If specified it can also return the standard scaled version of the data or the first 10 principal components of the data. The different dimensions of the returned data is below: Raw Scaled PCA Training \\(X\\) \\((10.000 \\times 784)\\) \\((10.000 \\times 784)\\) \\((10.000 \\times 10)\\) \\(Y\\) \\((10.000 \\times 1)\\) \\((10.000 \\times 1)\\) \\((10.000 \\times 1)\\) Test \\(X\\) \\((5.000 \\times 784)\\) \\((5.000 \\times 784)\\) \\((5.000 \\times 10)\\) \\(Y\\) \\((5.000 \\times 1)\\) \\((5.000 \\times 1)\\) \\((5.000 \\times 1)\\) Returns: Type Description 2d ndarrays numpy data arrays in the order X_train, X_test, y_train, y_test. Source code in mlproject/helpers/_data_loader.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def data_loader ( raw = True , scaled = False , pca = False ): r \"\"\"Loads the fashion_mnist training and test data from the data directory. The function returns four numpy arrays containing the training and test data respectively. If specified it can also return the standard scaled version of the data or the first 10 principal components of the data. The different dimensions of the returned data is below: | | Raw | Scaled | PCA | |:------------:|:---------------------:|:---------------------:|:--------------------:| | **Training** | | | | | $X$ | $(10.000 \\times 784)$ | $(10.000 \\times 784)$ | $(10.000 \\times 10)$ | | $Y$ | $(10.000 \\times 1)$ | $(10.000 \\times 1)$ | $(10.000 \\times 1)$ | | **Test** | | | | | $X$ | $(5.000 \\times 784)$ | $(5.000 \\times 784)$ | $(5.000 \\times 10)$ | | $Y$ | $(5.000 \\times 1)$ | $(5.000 \\times 1)$ | $(5.000 \\times 1)$ | Returns ------- 2d ndarrays numpy data arrays in the order X_train, X_test, y_train, y_test. \"\"\" if raw : X_train , y_train = np . hsplit ( np . load ( f \" { ROOT_DIR } /data/fashion_train.npy\" ), [ - 1 ] ) X_test , y_test = np . hsplit ( np . load ( f \" { ROOT_DIR } /data/fashion_test.npy\" ), [ - 1 ]) elif scaled and not raw : X_train , y_train = np . hsplit ( np . load ( f \" { ROOT_DIR } /data/fashion_train_scaled.npy\" ), [ - 1 ] ) X_test , y_test = np . hsplit ( np . load ( f \" { ROOT_DIR } /data/fashion_test_scaled.npy\" ), [ - 1 ] ) # converting the y_labels back to integers from floats to avoid issues y_train , y_test = y_train . astype ( int ), y_test . astype ( int ) elif pca and not raw and not scaled : X_train , y_train = np . hsplit ( np . load ( f \" { ROOT_DIR } /data/fashion_train_pca.npy\" ), [ - 1 ] ) X_test , y_test = np . hsplit ( np . load ( f \" { ROOT_DIR } /data/fashion_test_pca.npy\" ), [ - 1 ] ) # converting the y_labels back to integers from floats to avoid issues y_train , y_test = y_train . astype ( int ), y_test . astype ( int ) return X_train , X_test , y_train , y_test","title":"_data_loader"},{"location":"reference/mlproject/helpers/_data_loader/#mlproject.helpers._data_loader.data_loader","text":"Loads the fashion_mnist training and test data from the data directory. The function returns four numpy arrays containing the training and test data respectively. If specified it can also return the standard scaled version of the data or the first 10 principal components of the data. The different dimensions of the returned data is below: Raw Scaled PCA Training \\(X\\) \\((10.000 \\times 784)\\) \\((10.000 \\times 784)\\) \\((10.000 \\times 10)\\) \\(Y\\) \\((10.000 \\times 1)\\) \\((10.000 \\times 1)\\) \\((10.000 \\times 1)\\) Test \\(X\\) \\((5.000 \\times 784)\\) \\((5.000 \\times 784)\\) \\((5.000 \\times 10)\\) \\(Y\\) \\((5.000 \\times 1)\\) \\((5.000 \\times 1)\\) \\((5.000 \\times 1)\\) Returns: Type Description 2d ndarrays numpy data arrays in the order X_train, X_test, y_train, y_test. Source code in mlproject/helpers/_data_loader.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def data_loader ( raw = True , scaled = False , pca = False ): r \"\"\"Loads the fashion_mnist training and test data from the data directory. The function returns four numpy arrays containing the training and test data respectively. If specified it can also return the standard scaled version of the data or the first 10 principal components of the data. The different dimensions of the returned data is below: | | Raw | Scaled | PCA | |:------------:|:---------------------:|:---------------------:|:--------------------:| | **Training** | | | | | $X$ | $(10.000 \\times 784)$ | $(10.000 \\times 784)$ | $(10.000 \\times 10)$ | | $Y$ | $(10.000 \\times 1)$ | $(10.000 \\times 1)$ | $(10.000 \\times 1)$ | | **Test** | | | | | $X$ | $(5.000 \\times 784)$ | $(5.000 \\times 784)$ | $(5.000 \\times 10)$ | | $Y$ | $(5.000 \\times 1)$ | $(5.000 \\times 1)$ | $(5.000 \\times 1)$ | Returns ------- 2d ndarrays numpy data arrays in the order X_train, X_test, y_train, y_test. \"\"\" if raw : X_train , y_train = np . hsplit ( np . load ( f \" { ROOT_DIR } /data/fashion_train.npy\" ), [ - 1 ] ) X_test , y_test = np . hsplit ( np . load ( f \" { ROOT_DIR } /data/fashion_test.npy\" ), [ - 1 ]) elif scaled and not raw : X_train , y_train = np . hsplit ( np . load ( f \" { ROOT_DIR } /data/fashion_train_scaled.npy\" ), [ - 1 ] ) X_test , y_test = np . hsplit ( np . load ( f \" { ROOT_DIR } /data/fashion_test_scaled.npy\" ), [ - 1 ] ) # converting the y_labels back to integers from floats to avoid issues y_train , y_test = y_train . astype ( int ), y_test . astype ( int ) elif pca and not raw and not scaled : X_train , y_train = np . hsplit ( np . load ( f \" { ROOT_DIR } /data/fashion_train_pca.npy\" ), [ - 1 ] ) X_test , y_test = np . hsplit ( np . load ( f \" { ROOT_DIR } /data/fashion_test_pca.npy\" ), [ - 1 ] ) # converting the y_labels back to integers from floats to avoid issues y_train , y_test = y_train . astype ( int ), y_test . astype ( int ) return X_train , X_test , y_train , y_test","title":"data_loader()"},{"location":"reference/mlproject/helpers/_metrics/","text":"accuracy_score ( y_true , y_pred , normalize = True ) \u2693\ufe0e Calculate the accuracy score from a given array of true labels and a given array of predicted labels. Inspired by https://stackoverflow.com/a/64680660 Parameters: Name Type Description Default y_true 2d ndarray array of shape (n_samples, 1) of true labels required y_pred 2d ndarray array of shape (n_samples, 1) of predicted labels required Returns: Name Type Description accuracy_scores float calculated accuracy score Raises: Type Description ValueError if y_true and y_pred are not of the same shape Source code in mlproject/helpers/_metrics.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def accuracy_score ( y_true , y_pred , normalize = True ): \"\"\"Calculate the accuracy score from a given array of true labels and a given array of predicted labels. Inspired by [https://stackoverflow.com/a/64680660](https://stackoverflow.com/a/64680660) Parameters ---------- y_true : 2d ndarray array of shape (n_samples, 1) of true labels y_pred : 2d ndarray array of shape (n_samples, 1) of predicted labels Returns ------- accuracy_scores : float calculated accuracy score Raises ------ ValueError if y_true and y_pred are not of the same shape \"\"\" if y_true . shape [ 0 ] != y_pred . shape [ 0 ] and y_true . shape [ 1 ] != y_pred . shape [ 1 ]: raise ValueError ( f \"Length of y_true: ( { len ( y_true ) } ) and y_pred: ( { len ( y_pred ) } ) should be the same!\" ) accuracy = [] for i in range ( len ( y_pred )): if y_pred [ i ] == y_true [ i ]: accuracy . append ( 1 ) else : accuracy . append ( 0 ) if normalize == True : return np . mean ( accuracy ) if normalize == False : return sum ( accuracy )","title":"_metrics"},{"location":"reference/mlproject/helpers/_metrics/#mlproject.helpers._metrics.accuracy_score","text":"Calculate the accuracy score from a given array of true labels and a given array of predicted labels. Inspired by https://stackoverflow.com/a/64680660 Parameters: Name Type Description Default y_true 2d ndarray array of shape (n_samples, 1) of true labels required y_pred 2d ndarray array of shape (n_samples, 1) of predicted labels required Returns: Name Type Description accuracy_scores float calculated accuracy score Raises: Type Description ValueError if y_true and y_pred are not of the same shape Source code in mlproject/helpers/_metrics.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def accuracy_score ( y_true , y_pred , normalize = True ): \"\"\"Calculate the accuracy score from a given array of true labels and a given array of predicted labels. Inspired by [https://stackoverflow.com/a/64680660](https://stackoverflow.com/a/64680660) Parameters ---------- y_true : 2d ndarray array of shape (n_samples, 1) of true labels y_pred : 2d ndarray array of shape (n_samples, 1) of predicted labels Returns ------- accuracy_scores : float calculated accuracy score Raises ------ ValueError if y_true and y_pred are not of the same shape \"\"\" if y_true . shape [ 0 ] != y_pred . shape [ 0 ] and y_true . shape [ 1 ] != y_pred . shape [ 1 ]: raise ValueError ( f \"Length of y_true: ( { len ( y_true ) } ) and y_pred: ( { len ( y_pred ) } ) should be the same!\" ) accuracy = [] for i in range ( len ( y_pred )): if y_pred [ i ] == y_true [ i ]: accuracy . append ( 1 ) else : accuracy . append ( 0 ) if normalize == True : return np . mean ( accuracy ) if normalize == False : return sum ( accuracy )","title":"accuracy_score()"},{"location":"reference/mlproject/neural_net/","text":"DenseLayer \u2693\ufe0e Fully connected layer of a neural network Parameters: Name Type Description Default input_n int The amount of inputs to the DenseLayer required output_n int The amount of outputs to the DenseLayer required activation str The activation function to use for all of the neurons in the DenseLayer, either 'leaky_relu' or 'softmax' required Source code in mlproject/neural_net/_dense_layer.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 class DenseLayer : \"\"\"Fully connected layer of a neural network Parameters ---------- input_n : int The amount of inputs to the DenseLayer output_n : int The amount of outputs to the DenseLayer activation : str The activation function to use for all of the neurons in the DenseLayer, either 'leaky_relu' or 'softmax' \"\"\" def __init__ ( self , input_n : int , output_n : int , activation : str ): # he initiliasation of weights and biases # see https://keras.io/api/layers/initializers/#henormal-class self . output_n , self . input_n = output_n , input_n stddev = np . sqrt ( 2 / input_n ) self . weights = np . random . normal ( loc = 0 , scale = stddev , size = ( input_n , output_n )) self . z = None self . biases = np . zeros ( shape = ( output_n )) if activation == \"leaky_relu\" : self . activation = leaky_relu elif activation == \"softmax\" or activation == \"stable_softmax\" : self . activation = stable_softmax else : raise NotImplementedError ( f \" { activation } not implemented yet. Choose from ['leaky_relu', 'stable_softmax']\" ) def forward ( self , X ): \"\"\"Computes a single forward pass of the DenseLayer Parameters ---------- X : 2d ndarray An n x p matrix of data points where n is the number of data points and p is the number of features. Returns ------- 2d ndarray An n x output_n numpy array where n is the number of samples and output_n is the number of neurons in the DenseLayer \"\"\" self . z = X @ self . weights + self . biases return self . activation ( self . z ) def out_neurons ( self ): \"\"\"Return the number of output neurons in the DenseLayer Returns ------- int The total number of output neurons in the DenseLayer \"\"\" return self . output_n def in_neurons ( self ): \"\"\"Return the number of input neurons in the DenseLayer Returns ------- int The total number of input neurons in the DenseLayer \"\"\" return self . input_n def activation_function ( self ): \"\"\"Return a string representing the activation function of the given DenseLayer Returns ------- string string representing the activation function of the given DenseLayer \"\"\" if self . activation == stable_softmax : return \"softmax\" elif self . activation == leaky_relu : return \"leaky_relu\" activation_function () \u2693\ufe0e Return a string representing the activation function of the given DenseLayer Returns: Type Description string string representing the activation function of the given DenseLayer Source code in mlproject/neural_net/_dense_layer.py 75 76 77 78 79 80 81 82 83 84 85 86 def activation_function ( self ): \"\"\"Return a string representing the activation function of the given DenseLayer Returns ------- string string representing the activation function of the given DenseLayer \"\"\" if self . activation == stable_softmax : return \"softmax\" elif self . activation == leaky_relu : return \"leaky_relu\" forward ( X ) \u2693\ufe0e Computes a single forward pass of the DenseLayer Parameters: Name Type Description Default X 2d ndarray An n x p matrix of data points where n is the number of data points and p is the number of features. required Returns: Type Description 2d ndarray An n x output_n numpy array where n is the number of samples and output_n is the number of neurons in the DenseLayer Source code in mlproject/neural_net/_dense_layer.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def forward ( self , X ): \"\"\"Computes a single forward pass of the DenseLayer Parameters ---------- X : 2d ndarray An n x p matrix of data points where n is the number of data points and p is the number of features. Returns ------- 2d ndarray An n x output_n numpy array where n is the number of samples and output_n is the number of neurons in the DenseLayer \"\"\" self . z = X @ self . weights + self . biases return self . activation ( self . z ) in_neurons () \u2693\ufe0e Return the number of input neurons in the DenseLayer Returns: Type Description int The total number of input neurons in the DenseLayer Source code in mlproject/neural_net/_dense_layer.py 65 66 67 68 69 70 71 72 73 def in_neurons ( self ): \"\"\"Return the number of input neurons in the DenseLayer Returns ------- int The total number of input neurons in the DenseLayer \"\"\" return self . input_n out_neurons () \u2693\ufe0e Return the number of output neurons in the DenseLayer Returns: Type Description int The total number of output neurons in the DenseLayer Source code in mlproject/neural_net/_dense_layer.py 55 56 57 58 59 60 61 62 63 def out_neurons ( self ): \"\"\"Return the number of output neurons in the DenseLayer Returns ------- int The total number of output neurons in the DenseLayer \"\"\" return self . output_n NeuralNetworkClassifier \u2693\ufe0e NeuralNetworkClassifier Feed Forward Neural Network Classifier with however many dense layers (fully connected layers) of class DenseLayer each with own activation function and a network wide loss function. The layers of the network can either be added when initilizing the network, as a list or added individually with the add method after initialization. Parameters: Name Type Description Default layers list , optional A list of class DenseLayer [] loss str , optional The loss function to be used, currently only cross_entropy is supported. 'cross_entropy' Attributes: Name Type Description X 2d ndarray Data points to use for training the neural network y 1d ndarray Target classes n int Number of data points (X.shape[0]) p int Number of features (X.shape[1]) Source code in mlproject/neural_net/_neural_net.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 class NeuralNetworkClassifier : \"\"\"NeuralNetworkClassifier Feed Forward Neural Network Classifier with however many dense layers (fully connected layers) of class [`DenseLayer`][mlproject.neural_net._dense_layer.DenseLayer] each with own activation function and a network wide loss function. The layers of the network can either be added when initilizing the network, as a list or added individually with the [`add`][mlproject.neural_net._neural_net.NeuralNetworkClassifier.add] method after initialization. Parameters ---------- layers : list, optional A list of class [`DenseLayer`][mlproject.neural_net._dense_layer.DenseLayer] loss : str, optional The loss function to be used, currently only [`cross_entropy`][mlproject.neural_net._loss.cross_entropy_loss] is supported. Attributes ---------- X : 2d ndarray Data points to use for training the neural network y : 1d ndarray Target classes n : int Number of data points (X.shape[0]) p : int Number of features (X.shape[1]) \"\"\" def __init__ ( self , layers = [], loss = \"cross_entropy\" ): self . X = None self . n , self . p = None , None self . y = None self . k = None self . layers = layers self . activations , self . sums = [], [] if loss == \"cross_entropy\" : self . loss_str = \"cross_entropy_loss\" self . loss = cross_entropy_loss else : raise NotImplementedError ( f \" { loss } not implemented yet. Choose from ['cross_entropy']\" ) def add ( self , layer : DenseLayer ): \"\"\"Add a new layer to the network, after the current layer. Parameters ---------- layer : DenseLayer Fully connected layer. Example ------- ``` py >>> NN = NeuralNetworkClassifier(loss='cross_entropy') >>> NN.add(DenseLayer(784,128,\"leaky_relu\")) >>> NN.add(DenseLayer(128,5,\"softmax\")) >>> print(NN) NeuralNetworkClassifier -------------------------------- Loss function: cross_entropy_loss Input layer: Input: 784, Output: 128 , Activation: leaky_relu Output layer: Input: 128, Output: 5 , Activation: softmax ``` \"\"\" self . layers . append ( layer ) def forward ( self , X ): \"\"\"Compute a single forward pass of the network. Parameters ---------- X : 2d ndarray The data to use for the forward pass. Must be of size n x input_n where input_n must come from the first [`DenseLayer`][mlproject.neural_net._dense_layer.DenseLayer] in the network. Returns ------- 2d ndarray An n x output_n array where output_n corresponds to the output_n of the last [`DenseLayer`][mlproject.neural_net._dense_layer.DenseLayer] in the network and n is the number of data points. \"\"\" self . activations . append ( X ) for layer in self . layers : X = layer . forward ( X ) self . activations . append ( X ) self . sums . append ( layer . z ) return X def predict ( self , X ): \"\"\"Predict class labels for the given data. Parameters ---------- X : 2d ndarray The data that we want to use to make predictions. Returns ------- 1d ndarray All predicted class labels with size n, where n is the number of data points. \"\"\" probabilities = self . predict_proba ( X ) return np . array ( [ self . label [ pred ] for pred in np . argmax ( probabilities , axis = 1 ) . astype ( int )] ) def predict_proba ( self , X ): \"\"\"Predict class probabilities for the given data Parameters ---------- X : 2d ndarray The data that we want to use to make predictions Returns ------- 2d ndarray All probabilites with size n x k, where n is the number of data points and k is the number classes \"\"\" return self . forward ( X ) def fit ( self , X , y , batches : int = 1 , epochs : int = 1000 , lr : float = 0.01 ): r \"\"\"The actual training of the network to the given data Parameters ---------- X : 2d ndarray An $N \\times P$ matrix of data points where n is the number of data points and p is the number of features. y : 1d ndarray $N \\times 1$ vector of target class labels batches : int, optional The number of batches to use for training in each epoch, an integer indicating the number of splits to split the data into, by default $1$ which corresponds to training on the entire dataset in every epoch. epochs : int, optional The number of iterations to train for lr : float, optional The learning rate for gradient descent \"\"\" self . X = X self . n , self . p = self . X . shape self . y = y self . learning_rate = lr unique_classes = np . unique ( y ) self . k = len ( unique_classes ) one_hot = OneHotEncoder ( categories = [ unique_classes ]) self . y_hot_encoded = one_hot . fit_transform ( self . y ) . toarray () if self . layers [ - 1 ] . out_neurons () != self . k : raise ValueError ( f \"The number of neurons in the output layer, output_n: ( { self . layers [ - 1 ] . out_neurons () } ) must be equal to the number of classes, k: ( { self . k } )\" ) if self . layers [ 0 ] . in_neurons () != self . X . shape [ 1 ]: raise ValueError ( f \"The number of neurons in the input layer, input_n: ( { self . layers [ 0 ] . in_neurons () } ) must be equal to the number features in the dataset: ( { self . X . shape [ 1 ] } )\" ) # populate label-intcode dictionaries self . label = { k : unique_classes [ k ] for k in range ( self . k )} self . intcode = { unique_classes [ k ]: k for k in range ( self . k )} self . loss_history = [] self . accuracy_history = [] # get indices of every data point idxs = np . arange ( self . n ) with progress as pb : t1 = pb . add_task ( \"[blue]Training\" , total = epochs ) for epoch in range ( epochs ): # randomly shuffle the data --> split it into number of batches # here np.array_split returns an array of arrays of indices # of the different splits np . random . shuffle ( idxs ) batch_idxs = np . array_split ( idxs , batches ) for batch in batch_idxs : X_batch = self . X [ batch ] y_batch = self . y_hot_encoded [ batch ] # compute the initial class probabilities by doing a single forward pass # note: this should come 'automatically' from defining the last layer # in the model as a layer with output_n = k with softmax activation # where k is the number of classes. init_probs = self . forward ( X_batch ) # dividide by the number of data points in this specific batch to get the average loss. loss = self . loss ( y_batch , init_probs ) / len ( y_batch ) self . backward ( y_batch ) # add the latest loss to the history self . loss_history . append ( loss ) # predict with the current weights and biases on the whole data set batch_predict = self . predict ( self . X ) # calculate the accuracy score of the prediction train_accuracy = accuracy_score ( self . y , batch_predict ) # add accuracy to the history self . accuracy_history . append ( train_accuracy ) # update rich progress bar for each epoch pb . update ( t1 , advance = 1 ) if progress . finished : pb . update ( t1 , description = \"[bright_green]Training complete!\" ) def backward ( self , y_batch ): \"\"\"Computes a single backward pass all the way through the network. as well as updating the weights and biases. Parameters ---------- y_batch : 2d ndarray array of one-hot encoded ground_truth labels \"\"\" delta = self . activations [ - 1 ] - y_batch grad_bias = delta . sum ( 0 ) grad_weight = self . activations [ - 2 ] . T @ delta grad_biases , grad_weights = [], [] grad_weights . append ( grad_weight ) grad_biases . append ( grad_bias ) for i in range ( 2 , len ( self . layers ) + 1 ): layer = self . layers [ - i + 1 ] dzda = delta @ layer . weights . T delta = dzda * leaky_relu_der ( self . sums [ - i ]) grad_bias = delta . sum ( 0 ) grad_weight = self . activations [ - i - 1 ] . T @ delta grad_weights . append ( grad_weight ) grad_biases . append ( grad_bias ) # reverse the gradient lists so we can index them normally. grad_biases_rev = list ( reversed ( grad_biases )) grad_weights_rev = list ( reversed ( grad_weights )) for i in range ( 0 , len ( self . layers )): self . layers [ i ] . weights -= self . learning_rate * grad_weights_rev [ i ] self . layers [ i ] . biases -= self . learning_rate * grad_biases_rev [ i ] def __str__ ( self ): s = \" \\n NeuralNetworkClassifier \\n \" s += \"-------------------------------- \\n \" s += f \"Loss function: { self . loss_str } \\n\\n \" layers = [ self . layers [ i ] for i in range ( 0 , len ( self . layers ))] layers_neu = [ f \" \\t Input: { i . input_n } , Output: { i . output_n } , Activation: { i . activation_function () } \" for i in layers ] layer_num = 0 for layer in layers_neu : if layer_num == 0 : s += \"Input layer: \\n \" + layer + \" \\n\\n \" elif layer_num == len ( self . layers ) - 1 : s += f \"Output layer: \\n \" + layer else : s += f \"Layer: { layer_num } \\n \" + layer + \" \\n\\n \" layer_num += 1 return s add ( layer ) \u2693\ufe0e Add a new layer to the network, after the current layer. Parameters: Name Type Description Default layer DenseLayer Fully connected layer. required Example \u2693\ufe0e >>> NN = NeuralNetworkClassifier ( loss = 'cross_entropy' ) >>> NN . add ( DenseLayer ( 784 , 128 , \"leaky_relu\" )) >>> NN . add ( DenseLayer ( 128 , 5 , \"softmax\" )) >>> print ( NN ) NeuralNetworkClassifier -------------------------------- Loss function : cross_entropy_loss Input layer : Input : 784 , Output : 128 , Activation : leaky_relu Output layer : Input : 128 , Output : 5 , Activation : softmax Source code in mlproject/neural_net/_neural_net.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 def add ( self , layer : DenseLayer ): \"\"\"Add a new layer to the network, after the current layer. Parameters ---------- layer : DenseLayer Fully connected layer. Example ------- ``` py >>> NN = NeuralNetworkClassifier(loss='cross_entropy') >>> NN.add(DenseLayer(784,128,\"leaky_relu\")) >>> NN.add(DenseLayer(128,5,\"softmax\")) >>> print(NN) NeuralNetworkClassifier -------------------------------- Loss function: cross_entropy_loss Input layer: Input: 784, Output: 128 , Activation: leaky_relu Output layer: Input: 128, Output: 5 , Activation: softmax ``` \"\"\" self . layers . append ( layer ) backward ( y_batch ) \u2693\ufe0e Computes a single backward pass all the way through the network. as well as updating the weights and biases. Parameters: Name Type Description Default y_batch 2d ndarray array of one-hot encoded ground_truth labels required Source code in mlproject/neural_net/_neural_net.py 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 def backward ( self , y_batch ): \"\"\"Computes a single backward pass all the way through the network. as well as updating the weights and biases. Parameters ---------- y_batch : 2d ndarray array of one-hot encoded ground_truth labels \"\"\" delta = self . activations [ - 1 ] - y_batch grad_bias = delta . sum ( 0 ) grad_weight = self . activations [ - 2 ] . T @ delta grad_biases , grad_weights = [], [] grad_weights . append ( grad_weight ) grad_biases . append ( grad_bias ) for i in range ( 2 , len ( self . layers ) + 1 ): layer = self . layers [ - i + 1 ] dzda = delta @ layer . weights . T delta = dzda * leaky_relu_der ( self . sums [ - i ]) grad_bias = delta . sum ( 0 ) grad_weight = self . activations [ - i - 1 ] . T @ delta grad_weights . append ( grad_weight ) grad_biases . append ( grad_bias ) # reverse the gradient lists so we can index them normally. grad_biases_rev = list ( reversed ( grad_biases )) grad_weights_rev = list ( reversed ( grad_weights )) for i in range ( 0 , len ( self . layers )): self . layers [ i ] . weights -= self . learning_rate * grad_weights_rev [ i ] self . layers [ i ] . biases -= self . learning_rate * grad_biases_rev [ i ] fit ( X , y , batches = 1 , epochs = 1000 , lr = 0.01 ) \u2693\ufe0e The actual training of the network to the given data Parameters: Name Type Description Default X 2d ndarray An \\(N \\times P\\) matrix of data points where n is the number of data points and p is the number of features. required y 1d ndarray \\(N \\times 1\\) vector of target class labels required batches int , optional The number of batches to use for training in each epoch, an integer indicating the number of splits to split the data into, by default \\(1\\) which corresponds to training on the entire dataset in every epoch. 1 epochs int , optional The number of iterations to train for 1000 lr float , optional The learning rate for gradient descent 0.01 Source code in mlproject/neural_net/_neural_net.py 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 def fit ( self , X , y , batches : int = 1 , epochs : int = 1000 , lr : float = 0.01 ): r \"\"\"The actual training of the network to the given data Parameters ---------- X : 2d ndarray An $N \\times P$ matrix of data points where n is the number of data points and p is the number of features. y : 1d ndarray $N \\times 1$ vector of target class labels batches : int, optional The number of batches to use for training in each epoch, an integer indicating the number of splits to split the data into, by default $1$ which corresponds to training on the entire dataset in every epoch. epochs : int, optional The number of iterations to train for lr : float, optional The learning rate for gradient descent \"\"\" self . X = X self . n , self . p = self . X . shape self . y = y self . learning_rate = lr unique_classes = np . unique ( y ) self . k = len ( unique_classes ) one_hot = OneHotEncoder ( categories = [ unique_classes ]) self . y_hot_encoded = one_hot . fit_transform ( self . y ) . toarray () if self . layers [ - 1 ] . out_neurons () != self . k : raise ValueError ( f \"The number of neurons in the output layer, output_n: ( { self . layers [ - 1 ] . out_neurons () } ) must be equal to the number of classes, k: ( { self . k } )\" ) if self . layers [ 0 ] . in_neurons () != self . X . shape [ 1 ]: raise ValueError ( f \"The number of neurons in the input layer, input_n: ( { self . layers [ 0 ] . in_neurons () } ) must be equal to the number features in the dataset: ( { self . X . shape [ 1 ] } )\" ) # populate label-intcode dictionaries self . label = { k : unique_classes [ k ] for k in range ( self . k )} self . intcode = { unique_classes [ k ]: k for k in range ( self . k )} self . loss_history = [] self . accuracy_history = [] # get indices of every data point idxs = np . arange ( self . n ) with progress as pb : t1 = pb . add_task ( \"[blue]Training\" , total = epochs ) for epoch in range ( epochs ): # randomly shuffle the data --> split it into number of batches # here np.array_split returns an array of arrays of indices # of the different splits np . random . shuffle ( idxs ) batch_idxs = np . array_split ( idxs , batches ) for batch in batch_idxs : X_batch = self . X [ batch ] y_batch = self . y_hot_encoded [ batch ] # compute the initial class probabilities by doing a single forward pass # note: this should come 'automatically' from defining the last layer # in the model as a layer with output_n = k with softmax activation # where k is the number of classes. init_probs = self . forward ( X_batch ) # dividide by the number of data points in this specific batch to get the average loss. loss = self . loss ( y_batch , init_probs ) / len ( y_batch ) self . backward ( y_batch ) # add the latest loss to the history self . loss_history . append ( loss ) # predict with the current weights and biases on the whole data set batch_predict = self . predict ( self . X ) # calculate the accuracy score of the prediction train_accuracy = accuracy_score ( self . y , batch_predict ) # add accuracy to the history self . accuracy_history . append ( train_accuracy ) # update rich progress bar for each epoch pb . update ( t1 , advance = 1 ) if progress . finished : pb . update ( t1 , description = \"[bright_green]Training complete!\" ) forward ( X ) \u2693\ufe0e Compute a single forward pass of the network. Parameters: Name Type Description Default X 2d ndarray The data to use for the forward pass. Must be of size n x input_n where input_n must come from the first DenseLayer in the network. required Returns: Type Description 2d ndarray An n x output_n array where output_n corresponds to the output_n of the last DenseLayer in the network and n is the number of data points. Source code in mlproject/neural_net/_neural_net.py 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 def forward ( self , X ): \"\"\"Compute a single forward pass of the network. Parameters ---------- X : 2d ndarray The data to use for the forward pass. Must be of size n x input_n where input_n must come from the first [`DenseLayer`][mlproject.neural_net._dense_layer.DenseLayer] in the network. Returns ------- 2d ndarray An n x output_n array where output_n corresponds to the output_n of the last [`DenseLayer`][mlproject.neural_net._dense_layer.DenseLayer] in the network and n is the number of data points. \"\"\" self . activations . append ( X ) for layer in self . layers : X = layer . forward ( X ) self . activations . append ( X ) self . sums . append ( layer . z ) return X predict ( X ) \u2693\ufe0e Predict class labels for the given data. Parameters: Name Type Description Default X 2d ndarray The data that we want to use to make predictions. required Returns: Type Description 1d ndarray All predicted class labels with size n, where n is the number of data points. Source code in mlproject/neural_net/_neural_net.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def predict ( self , X ): \"\"\"Predict class labels for the given data. Parameters ---------- X : 2d ndarray The data that we want to use to make predictions. Returns ------- 1d ndarray All predicted class labels with size n, where n is the number of data points. \"\"\" probabilities = self . predict_proba ( X ) return np . array ( [ self . label [ pred ] for pred in np . argmax ( probabilities , axis = 1 ) . astype ( int )] ) predict_proba ( X ) \u2693\ufe0e Predict class probabilities for the given data Parameters: Name Type Description Default X 2d ndarray The data that we want to use to make predictions required Returns: Type Description 2d ndarray All probabilites with size n x k, where n is the number of data points and k is the number classes Source code in mlproject/neural_net/_neural_net.py 146 147 148 149 150 151 152 153 154 155 156 157 158 159 def predict_proba ( self , X ): \"\"\"Predict class probabilities for the given data Parameters ---------- X : 2d ndarray The data that we want to use to make predictions Returns ------- 2d ndarray All probabilites with size n x k, where n is the number of data points and k is the number classes \"\"\" return self . forward ( X )","title":"neural_net"},{"location":"reference/mlproject/neural_net/#mlproject.neural_net.DenseLayer","text":"Fully connected layer of a neural network Parameters: Name Type Description Default input_n int The amount of inputs to the DenseLayer required output_n int The amount of outputs to the DenseLayer required activation str The activation function to use for all of the neurons in the DenseLayer, either 'leaky_relu' or 'softmax' required Source code in mlproject/neural_net/_dense_layer.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 class DenseLayer : \"\"\"Fully connected layer of a neural network Parameters ---------- input_n : int The amount of inputs to the DenseLayer output_n : int The amount of outputs to the DenseLayer activation : str The activation function to use for all of the neurons in the DenseLayer, either 'leaky_relu' or 'softmax' \"\"\" def __init__ ( self , input_n : int , output_n : int , activation : str ): # he initiliasation of weights and biases # see https://keras.io/api/layers/initializers/#henormal-class self . output_n , self . input_n = output_n , input_n stddev = np . sqrt ( 2 / input_n ) self . weights = np . random . normal ( loc = 0 , scale = stddev , size = ( input_n , output_n )) self . z = None self . biases = np . zeros ( shape = ( output_n )) if activation == \"leaky_relu\" : self . activation = leaky_relu elif activation == \"softmax\" or activation == \"stable_softmax\" : self . activation = stable_softmax else : raise NotImplementedError ( f \" { activation } not implemented yet. Choose from ['leaky_relu', 'stable_softmax']\" ) def forward ( self , X ): \"\"\"Computes a single forward pass of the DenseLayer Parameters ---------- X : 2d ndarray An n x p matrix of data points where n is the number of data points and p is the number of features. Returns ------- 2d ndarray An n x output_n numpy array where n is the number of samples and output_n is the number of neurons in the DenseLayer \"\"\" self . z = X @ self . weights + self . biases return self . activation ( self . z ) def out_neurons ( self ): \"\"\"Return the number of output neurons in the DenseLayer Returns ------- int The total number of output neurons in the DenseLayer \"\"\" return self . output_n def in_neurons ( self ): \"\"\"Return the number of input neurons in the DenseLayer Returns ------- int The total number of input neurons in the DenseLayer \"\"\" return self . input_n def activation_function ( self ): \"\"\"Return a string representing the activation function of the given DenseLayer Returns ------- string string representing the activation function of the given DenseLayer \"\"\" if self . activation == stable_softmax : return \"softmax\" elif self . activation == leaky_relu : return \"leaky_relu\"","title":"DenseLayer"},{"location":"reference/mlproject/neural_net/#mlproject.neural_net._dense_layer.DenseLayer.activation_function","text":"Return a string representing the activation function of the given DenseLayer Returns: Type Description string string representing the activation function of the given DenseLayer Source code in mlproject/neural_net/_dense_layer.py 75 76 77 78 79 80 81 82 83 84 85 86 def activation_function ( self ): \"\"\"Return a string representing the activation function of the given DenseLayer Returns ------- string string representing the activation function of the given DenseLayer \"\"\" if self . activation == stable_softmax : return \"softmax\" elif self . activation == leaky_relu : return \"leaky_relu\"","title":"activation_function()"},{"location":"reference/mlproject/neural_net/#mlproject.neural_net._dense_layer.DenseLayer.forward","text":"Computes a single forward pass of the DenseLayer Parameters: Name Type Description Default X 2d ndarray An n x p matrix of data points where n is the number of data points and p is the number of features. required Returns: Type Description 2d ndarray An n x output_n numpy array where n is the number of samples and output_n is the number of neurons in the DenseLayer Source code in mlproject/neural_net/_dense_layer.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def forward ( self , X ): \"\"\"Computes a single forward pass of the DenseLayer Parameters ---------- X : 2d ndarray An n x p matrix of data points where n is the number of data points and p is the number of features. Returns ------- 2d ndarray An n x output_n numpy array where n is the number of samples and output_n is the number of neurons in the DenseLayer \"\"\" self . z = X @ self . weights + self . biases return self . activation ( self . z )","title":"forward()"},{"location":"reference/mlproject/neural_net/#mlproject.neural_net._dense_layer.DenseLayer.in_neurons","text":"Return the number of input neurons in the DenseLayer Returns: Type Description int The total number of input neurons in the DenseLayer Source code in mlproject/neural_net/_dense_layer.py 65 66 67 68 69 70 71 72 73 def in_neurons ( self ): \"\"\"Return the number of input neurons in the DenseLayer Returns ------- int The total number of input neurons in the DenseLayer \"\"\" return self . input_n","title":"in_neurons()"},{"location":"reference/mlproject/neural_net/#mlproject.neural_net._dense_layer.DenseLayer.out_neurons","text":"Return the number of output neurons in the DenseLayer Returns: Type Description int The total number of output neurons in the DenseLayer Source code in mlproject/neural_net/_dense_layer.py 55 56 57 58 59 60 61 62 63 def out_neurons ( self ): \"\"\"Return the number of output neurons in the DenseLayer Returns ------- int The total number of output neurons in the DenseLayer \"\"\" return self . output_n","title":"out_neurons()"},{"location":"reference/mlproject/neural_net/#mlproject.neural_net.NeuralNetworkClassifier","text":"NeuralNetworkClassifier Feed Forward Neural Network Classifier with however many dense layers (fully connected layers) of class DenseLayer each with own activation function and a network wide loss function. The layers of the network can either be added when initilizing the network, as a list or added individually with the add method after initialization. Parameters: Name Type Description Default layers list , optional A list of class DenseLayer [] loss str , optional The loss function to be used, currently only cross_entropy is supported. 'cross_entropy' Attributes: Name Type Description X 2d ndarray Data points to use for training the neural network y 1d ndarray Target classes n int Number of data points (X.shape[0]) p int Number of features (X.shape[1]) Source code in mlproject/neural_net/_neural_net.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 class NeuralNetworkClassifier : \"\"\"NeuralNetworkClassifier Feed Forward Neural Network Classifier with however many dense layers (fully connected layers) of class [`DenseLayer`][mlproject.neural_net._dense_layer.DenseLayer] each with own activation function and a network wide loss function. The layers of the network can either be added when initilizing the network, as a list or added individually with the [`add`][mlproject.neural_net._neural_net.NeuralNetworkClassifier.add] method after initialization. Parameters ---------- layers : list, optional A list of class [`DenseLayer`][mlproject.neural_net._dense_layer.DenseLayer] loss : str, optional The loss function to be used, currently only [`cross_entropy`][mlproject.neural_net._loss.cross_entropy_loss] is supported. Attributes ---------- X : 2d ndarray Data points to use for training the neural network y : 1d ndarray Target classes n : int Number of data points (X.shape[0]) p : int Number of features (X.shape[1]) \"\"\" def __init__ ( self , layers = [], loss = \"cross_entropy\" ): self . X = None self . n , self . p = None , None self . y = None self . k = None self . layers = layers self . activations , self . sums = [], [] if loss == \"cross_entropy\" : self . loss_str = \"cross_entropy_loss\" self . loss = cross_entropy_loss else : raise NotImplementedError ( f \" { loss } not implemented yet. Choose from ['cross_entropy']\" ) def add ( self , layer : DenseLayer ): \"\"\"Add a new layer to the network, after the current layer. Parameters ---------- layer : DenseLayer Fully connected layer. Example ------- ``` py >>> NN = NeuralNetworkClassifier(loss='cross_entropy') >>> NN.add(DenseLayer(784,128,\"leaky_relu\")) >>> NN.add(DenseLayer(128,5,\"softmax\")) >>> print(NN) NeuralNetworkClassifier -------------------------------- Loss function: cross_entropy_loss Input layer: Input: 784, Output: 128 , Activation: leaky_relu Output layer: Input: 128, Output: 5 , Activation: softmax ``` \"\"\" self . layers . append ( layer ) def forward ( self , X ): \"\"\"Compute a single forward pass of the network. Parameters ---------- X : 2d ndarray The data to use for the forward pass. Must be of size n x input_n where input_n must come from the first [`DenseLayer`][mlproject.neural_net._dense_layer.DenseLayer] in the network. Returns ------- 2d ndarray An n x output_n array where output_n corresponds to the output_n of the last [`DenseLayer`][mlproject.neural_net._dense_layer.DenseLayer] in the network and n is the number of data points. \"\"\" self . activations . append ( X ) for layer in self . layers : X = layer . forward ( X ) self . activations . append ( X ) self . sums . append ( layer . z ) return X def predict ( self , X ): \"\"\"Predict class labels for the given data. Parameters ---------- X : 2d ndarray The data that we want to use to make predictions. Returns ------- 1d ndarray All predicted class labels with size n, where n is the number of data points. \"\"\" probabilities = self . predict_proba ( X ) return np . array ( [ self . label [ pred ] for pred in np . argmax ( probabilities , axis = 1 ) . astype ( int )] ) def predict_proba ( self , X ): \"\"\"Predict class probabilities for the given data Parameters ---------- X : 2d ndarray The data that we want to use to make predictions Returns ------- 2d ndarray All probabilites with size n x k, where n is the number of data points and k is the number classes \"\"\" return self . forward ( X ) def fit ( self , X , y , batches : int = 1 , epochs : int = 1000 , lr : float = 0.01 ): r \"\"\"The actual training of the network to the given data Parameters ---------- X : 2d ndarray An $N \\times P$ matrix of data points where n is the number of data points and p is the number of features. y : 1d ndarray $N \\times 1$ vector of target class labels batches : int, optional The number of batches to use for training in each epoch, an integer indicating the number of splits to split the data into, by default $1$ which corresponds to training on the entire dataset in every epoch. epochs : int, optional The number of iterations to train for lr : float, optional The learning rate for gradient descent \"\"\" self . X = X self . n , self . p = self . X . shape self . y = y self . learning_rate = lr unique_classes = np . unique ( y ) self . k = len ( unique_classes ) one_hot = OneHotEncoder ( categories = [ unique_classes ]) self . y_hot_encoded = one_hot . fit_transform ( self . y ) . toarray () if self . layers [ - 1 ] . out_neurons () != self . k : raise ValueError ( f \"The number of neurons in the output layer, output_n: ( { self . layers [ - 1 ] . out_neurons () } ) must be equal to the number of classes, k: ( { self . k } )\" ) if self . layers [ 0 ] . in_neurons () != self . X . shape [ 1 ]: raise ValueError ( f \"The number of neurons in the input layer, input_n: ( { self . layers [ 0 ] . in_neurons () } ) must be equal to the number features in the dataset: ( { self . X . shape [ 1 ] } )\" ) # populate label-intcode dictionaries self . label = { k : unique_classes [ k ] for k in range ( self . k )} self . intcode = { unique_classes [ k ]: k for k in range ( self . k )} self . loss_history = [] self . accuracy_history = [] # get indices of every data point idxs = np . arange ( self . n ) with progress as pb : t1 = pb . add_task ( \"[blue]Training\" , total = epochs ) for epoch in range ( epochs ): # randomly shuffle the data --> split it into number of batches # here np.array_split returns an array of arrays of indices # of the different splits np . random . shuffle ( idxs ) batch_idxs = np . array_split ( idxs , batches ) for batch in batch_idxs : X_batch = self . X [ batch ] y_batch = self . y_hot_encoded [ batch ] # compute the initial class probabilities by doing a single forward pass # note: this should come 'automatically' from defining the last layer # in the model as a layer with output_n = k with softmax activation # where k is the number of classes. init_probs = self . forward ( X_batch ) # dividide by the number of data points in this specific batch to get the average loss. loss = self . loss ( y_batch , init_probs ) / len ( y_batch ) self . backward ( y_batch ) # add the latest loss to the history self . loss_history . append ( loss ) # predict with the current weights and biases on the whole data set batch_predict = self . predict ( self . X ) # calculate the accuracy score of the prediction train_accuracy = accuracy_score ( self . y , batch_predict ) # add accuracy to the history self . accuracy_history . append ( train_accuracy ) # update rich progress bar for each epoch pb . update ( t1 , advance = 1 ) if progress . finished : pb . update ( t1 , description = \"[bright_green]Training complete!\" ) def backward ( self , y_batch ): \"\"\"Computes a single backward pass all the way through the network. as well as updating the weights and biases. Parameters ---------- y_batch : 2d ndarray array of one-hot encoded ground_truth labels \"\"\" delta = self . activations [ - 1 ] - y_batch grad_bias = delta . sum ( 0 ) grad_weight = self . activations [ - 2 ] . T @ delta grad_biases , grad_weights = [], [] grad_weights . append ( grad_weight ) grad_biases . append ( grad_bias ) for i in range ( 2 , len ( self . layers ) + 1 ): layer = self . layers [ - i + 1 ] dzda = delta @ layer . weights . T delta = dzda * leaky_relu_der ( self . sums [ - i ]) grad_bias = delta . sum ( 0 ) grad_weight = self . activations [ - i - 1 ] . T @ delta grad_weights . append ( grad_weight ) grad_biases . append ( grad_bias ) # reverse the gradient lists so we can index them normally. grad_biases_rev = list ( reversed ( grad_biases )) grad_weights_rev = list ( reversed ( grad_weights )) for i in range ( 0 , len ( self . layers )): self . layers [ i ] . weights -= self . learning_rate * grad_weights_rev [ i ] self . layers [ i ] . biases -= self . learning_rate * grad_biases_rev [ i ] def __str__ ( self ): s = \" \\n NeuralNetworkClassifier \\n \" s += \"-------------------------------- \\n \" s += f \"Loss function: { self . loss_str } \\n\\n \" layers = [ self . layers [ i ] for i in range ( 0 , len ( self . layers ))] layers_neu = [ f \" \\t Input: { i . input_n } , Output: { i . output_n } , Activation: { i . activation_function () } \" for i in layers ] layer_num = 0 for layer in layers_neu : if layer_num == 0 : s += \"Input layer: \\n \" + layer + \" \\n\\n \" elif layer_num == len ( self . layers ) - 1 : s += f \"Output layer: \\n \" + layer else : s += f \"Layer: { layer_num } \\n \" + layer + \" \\n\\n \" layer_num += 1 return s","title":"NeuralNetworkClassifier"},{"location":"reference/mlproject/neural_net/#mlproject.neural_net._neural_net.NeuralNetworkClassifier.add","text":"Add a new layer to the network, after the current layer. Parameters: Name Type Description Default layer DenseLayer Fully connected layer. required","title":"add()"},{"location":"reference/mlproject/neural_net/#mlproject.neural_net._neural_net.NeuralNetworkClassifier.add--example","text":">>> NN = NeuralNetworkClassifier ( loss = 'cross_entropy' ) >>> NN . add ( DenseLayer ( 784 , 128 , \"leaky_relu\" )) >>> NN . add ( DenseLayer ( 128 , 5 , \"softmax\" )) >>> print ( NN ) NeuralNetworkClassifier -------------------------------- Loss function : cross_entropy_loss Input layer : Input : 784 , Output : 128 , Activation : leaky_relu Output layer : Input : 128 , Output : 5 , Activation : softmax Source code in mlproject/neural_net/_neural_net.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 def add ( self , layer : DenseLayer ): \"\"\"Add a new layer to the network, after the current layer. Parameters ---------- layer : DenseLayer Fully connected layer. Example ------- ``` py >>> NN = NeuralNetworkClassifier(loss='cross_entropy') >>> NN.add(DenseLayer(784,128,\"leaky_relu\")) >>> NN.add(DenseLayer(128,5,\"softmax\")) >>> print(NN) NeuralNetworkClassifier -------------------------------- Loss function: cross_entropy_loss Input layer: Input: 784, Output: 128 , Activation: leaky_relu Output layer: Input: 128, Output: 5 , Activation: softmax ``` \"\"\" self . layers . append ( layer )","title":"Example"},{"location":"reference/mlproject/neural_net/#mlproject.neural_net._neural_net.NeuralNetworkClassifier.backward","text":"Computes a single backward pass all the way through the network. as well as updating the weights and biases. Parameters: Name Type Description Default y_batch 2d ndarray array of one-hot encoded ground_truth labels required Source code in mlproject/neural_net/_neural_net.py 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 def backward ( self , y_batch ): \"\"\"Computes a single backward pass all the way through the network. as well as updating the weights and biases. Parameters ---------- y_batch : 2d ndarray array of one-hot encoded ground_truth labels \"\"\" delta = self . activations [ - 1 ] - y_batch grad_bias = delta . sum ( 0 ) grad_weight = self . activations [ - 2 ] . T @ delta grad_biases , grad_weights = [], [] grad_weights . append ( grad_weight ) grad_biases . append ( grad_bias ) for i in range ( 2 , len ( self . layers ) + 1 ): layer = self . layers [ - i + 1 ] dzda = delta @ layer . weights . T delta = dzda * leaky_relu_der ( self . sums [ - i ]) grad_bias = delta . sum ( 0 ) grad_weight = self . activations [ - i - 1 ] . T @ delta grad_weights . append ( grad_weight ) grad_biases . append ( grad_bias ) # reverse the gradient lists so we can index them normally. grad_biases_rev = list ( reversed ( grad_biases )) grad_weights_rev = list ( reversed ( grad_weights )) for i in range ( 0 , len ( self . layers )): self . layers [ i ] . weights -= self . learning_rate * grad_weights_rev [ i ] self . layers [ i ] . biases -= self . learning_rate * grad_biases_rev [ i ]","title":"backward()"},{"location":"reference/mlproject/neural_net/#mlproject.neural_net._neural_net.NeuralNetworkClassifier.fit","text":"The actual training of the network to the given data Parameters: Name Type Description Default X 2d ndarray An \\(N \\times P\\) matrix of data points where n is the number of data points and p is the number of features. required y 1d ndarray \\(N \\times 1\\) vector of target class labels required batches int , optional The number of batches to use for training in each epoch, an integer indicating the number of splits to split the data into, by default \\(1\\) which corresponds to training on the entire dataset in every epoch. 1 epochs int , optional The number of iterations to train for 1000 lr float , optional The learning rate for gradient descent 0.01 Source code in mlproject/neural_net/_neural_net.py 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 def fit ( self , X , y , batches : int = 1 , epochs : int = 1000 , lr : float = 0.01 ): r \"\"\"The actual training of the network to the given data Parameters ---------- X : 2d ndarray An $N \\times P$ matrix of data points where n is the number of data points and p is the number of features. y : 1d ndarray $N \\times 1$ vector of target class labels batches : int, optional The number of batches to use for training in each epoch, an integer indicating the number of splits to split the data into, by default $1$ which corresponds to training on the entire dataset in every epoch. epochs : int, optional The number of iterations to train for lr : float, optional The learning rate for gradient descent \"\"\" self . X = X self . n , self . p = self . X . shape self . y = y self . learning_rate = lr unique_classes = np . unique ( y ) self . k = len ( unique_classes ) one_hot = OneHotEncoder ( categories = [ unique_classes ]) self . y_hot_encoded = one_hot . fit_transform ( self . y ) . toarray () if self . layers [ - 1 ] . out_neurons () != self . k : raise ValueError ( f \"The number of neurons in the output layer, output_n: ( { self . layers [ - 1 ] . out_neurons () } ) must be equal to the number of classes, k: ( { self . k } )\" ) if self . layers [ 0 ] . in_neurons () != self . X . shape [ 1 ]: raise ValueError ( f \"The number of neurons in the input layer, input_n: ( { self . layers [ 0 ] . in_neurons () } ) must be equal to the number features in the dataset: ( { self . X . shape [ 1 ] } )\" ) # populate label-intcode dictionaries self . label = { k : unique_classes [ k ] for k in range ( self . k )} self . intcode = { unique_classes [ k ]: k for k in range ( self . k )} self . loss_history = [] self . accuracy_history = [] # get indices of every data point idxs = np . arange ( self . n ) with progress as pb : t1 = pb . add_task ( \"[blue]Training\" , total = epochs ) for epoch in range ( epochs ): # randomly shuffle the data --> split it into number of batches # here np.array_split returns an array of arrays of indices # of the different splits np . random . shuffle ( idxs ) batch_idxs = np . array_split ( idxs , batches ) for batch in batch_idxs : X_batch = self . X [ batch ] y_batch = self . y_hot_encoded [ batch ] # compute the initial class probabilities by doing a single forward pass # note: this should come 'automatically' from defining the last layer # in the model as a layer with output_n = k with softmax activation # where k is the number of classes. init_probs = self . forward ( X_batch ) # dividide by the number of data points in this specific batch to get the average loss. loss = self . loss ( y_batch , init_probs ) / len ( y_batch ) self . backward ( y_batch ) # add the latest loss to the history self . loss_history . append ( loss ) # predict with the current weights and biases on the whole data set batch_predict = self . predict ( self . X ) # calculate the accuracy score of the prediction train_accuracy = accuracy_score ( self . y , batch_predict ) # add accuracy to the history self . accuracy_history . append ( train_accuracy ) # update rich progress bar for each epoch pb . update ( t1 , advance = 1 ) if progress . finished : pb . update ( t1 , description = \"[bright_green]Training complete!\" )","title":"fit()"},{"location":"reference/mlproject/neural_net/#mlproject.neural_net._neural_net.NeuralNetworkClassifier.forward","text":"Compute a single forward pass of the network. Parameters: Name Type Description Default X 2d ndarray The data to use for the forward pass. Must be of size n x input_n where input_n must come from the first DenseLayer in the network. required Returns: Type Description 2d ndarray An n x output_n array where output_n corresponds to the output_n of the last DenseLayer in the network and n is the number of data points. Source code in mlproject/neural_net/_neural_net.py 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 def forward ( self , X ): \"\"\"Compute a single forward pass of the network. Parameters ---------- X : 2d ndarray The data to use for the forward pass. Must be of size n x input_n where input_n must come from the first [`DenseLayer`][mlproject.neural_net._dense_layer.DenseLayer] in the network. Returns ------- 2d ndarray An n x output_n array where output_n corresponds to the output_n of the last [`DenseLayer`][mlproject.neural_net._dense_layer.DenseLayer] in the network and n is the number of data points. \"\"\" self . activations . append ( X ) for layer in self . layers : X = layer . forward ( X ) self . activations . append ( X ) self . sums . append ( layer . z ) return X","title":"forward()"},{"location":"reference/mlproject/neural_net/#mlproject.neural_net._neural_net.NeuralNetworkClassifier.predict","text":"Predict class labels for the given data. Parameters: Name Type Description Default X 2d ndarray The data that we want to use to make predictions. required Returns: Type Description 1d ndarray All predicted class labels with size n, where n is the number of data points. Source code in mlproject/neural_net/_neural_net.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def predict ( self , X ): \"\"\"Predict class labels for the given data. Parameters ---------- X : 2d ndarray The data that we want to use to make predictions. Returns ------- 1d ndarray All predicted class labels with size n, where n is the number of data points. \"\"\" probabilities = self . predict_proba ( X ) return np . array ( [ self . label [ pred ] for pred in np . argmax ( probabilities , axis = 1 ) . astype ( int )] )","title":"predict()"},{"location":"reference/mlproject/neural_net/#mlproject.neural_net._neural_net.NeuralNetworkClassifier.predict_proba","text":"Predict class probabilities for the given data Parameters: Name Type Description Default X 2d ndarray The data that we want to use to make predictions required Returns: Type Description 2d ndarray All probabilites with size n x k, where n is the number of data points and k is the number classes Source code in mlproject/neural_net/_neural_net.py 146 147 148 149 150 151 152 153 154 155 156 157 158 159 def predict_proba ( self , X ): \"\"\"Predict class probabilities for the given data Parameters ---------- X : 2d ndarray The data that we want to use to make predictions Returns ------- 2d ndarray All probabilites with size n x k, where n is the number of data points and k is the number classes \"\"\" return self . forward ( X )","title":"predict_proba()"},{"location":"reference/mlproject/neural_net/_activations/","text":"leaky_relu ( z ) \u2693\ufe0e Leaky relu activation function Parameters: Name Type Description Default z 2d ndarray input to the leaky relu activation function required Returns: Type Description 2d ndarray leaky relu 'activated' version of the input z Source code in mlproject/neural_net/_activations.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def leaky_relu ( z ): \"\"\"Leaky relu activation function Parameters ---------- z : 2d ndarray input to the leaky relu activation function Returns ------- 2d ndarray leaky relu 'activated' version of the input `z` \"\"\" return np . where ( z > 0 , z , z * 0.01 ) leaky_relu_der ( z ) \u2693\ufe0e Derivative of the leaky relu activation function Parameters: Name Type Description Default z 2d ndarray input to calculate the derivative of required Returns: Type Description 2d ndarray derivative of the specific neuron with a leaky relu activation function Source code in mlproject/neural_net/_activations.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 def leaky_relu_der ( z ): \"\"\"Derivative of the leaky relu activation function Parameters ---------- z : 2d ndarray input to calculate the derivative of Returns ------- 2d ndarray derivative of the specific neuron with a leaky relu activation function \"\"\" return np . where ( z > 0 , 1 , 0.01 ) stable_softmax ( z ) \u2693\ufe0e Numerically stable softmax activation function Inspired by https://stackoverflow.com/a/50425683 & https://github.com/scipy/scipy/blob/v1.9.3/scipy/special/_logsumexp.py#L130-L223 Parameters: Name Type Description Default z 2d ndarray input to the softmax activation function required Returns: Type Description 2d ndarray softmax 'activated' version of the input z Source code in mlproject/neural_net/_activations.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 def stable_softmax ( z ): \"\"\"Numerically stable softmax activation function Inspired by https://stackoverflow.com/a/50425683 & https://github.com/scipy/scipy/blob/v1.9.3/scipy/special/_logsumexp.py#L130-L223 Parameters ---------- z : 2d ndarray input to the softmax activation function Returns ------- 2d ndarray softmax 'activated' version of the input `z` \"\"\" # When keepdims is set to True we keep the original dimensions/shape of the input. # axis = 1 means that we find the maximum value along the first axis i.e. the rows. e = z - np . amax ( z , axis = 1 , keepdims = True ) e = np . exp ( e ) return e / np . sum ( e , axis = 1 , keepdims = True )","title":"_activations"},{"location":"reference/mlproject/neural_net/_activations/#mlproject.neural_net._activations.leaky_relu","text":"Leaky relu activation function Parameters: Name Type Description Default z 2d ndarray input to the leaky relu activation function required Returns: Type Description 2d ndarray leaky relu 'activated' version of the input z Source code in mlproject/neural_net/_activations.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def leaky_relu ( z ): \"\"\"Leaky relu activation function Parameters ---------- z : 2d ndarray input to the leaky relu activation function Returns ------- 2d ndarray leaky relu 'activated' version of the input `z` \"\"\" return np . where ( z > 0 , z , z * 0.01 )","title":"leaky_relu()"},{"location":"reference/mlproject/neural_net/_activations/#mlproject.neural_net._activations.leaky_relu_der","text":"Derivative of the leaky relu activation function Parameters: Name Type Description Default z 2d ndarray input to calculate the derivative of required Returns: Type Description 2d ndarray derivative of the specific neuron with a leaky relu activation function Source code in mlproject/neural_net/_activations.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 def leaky_relu_der ( z ): \"\"\"Derivative of the leaky relu activation function Parameters ---------- z : 2d ndarray input to calculate the derivative of Returns ------- 2d ndarray derivative of the specific neuron with a leaky relu activation function \"\"\" return np . where ( z > 0 , 1 , 0.01 )","title":"leaky_relu_der()"},{"location":"reference/mlproject/neural_net/_activations/#mlproject.neural_net._activations.stable_softmax","text":"Numerically stable softmax activation function Inspired by https://stackoverflow.com/a/50425683 & https://github.com/scipy/scipy/blob/v1.9.3/scipy/special/_logsumexp.py#L130-L223 Parameters: Name Type Description Default z 2d ndarray input to the softmax activation function required Returns: Type Description 2d ndarray softmax 'activated' version of the input z Source code in mlproject/neural_net/_activations.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 def stable_softmax ( z ): \"\"\"Numerically stable softmax activation function Inspired by https://stackoverflow.com/a/50425683 & https://github.com/scipy/scipy/blob/v1.9.3/scipy/special/_logsumexp.py#L130-L223 Parameters ---------- z : 2d ndarray input to the softmax activation function Returns ------- 2d ndarray softmax 'activated' version of the input `z` \"\"\" # When keepdims is set to True we keep the original dimensions/shape of the input. # axis = 1 means that we find the maximum value along the first axis i.e. the rows. e = z - np . amax ( z , axis = 1 , keepdims = True ) e = np . exp ( e ) return e / np . sum ( e , axis = 1 , keepdims = True )","title":"stable_softmax()"},{"location":"reference/mlproject/neural_net/_dense_layer/","text":"DenseLayer \u2693\ufe0e Fully connected layer of a neural network Parameters: Name Type Description Default input_n int The amount of inputs to the DenseLayer required output_n int The amount of outputs to the DenseLayer required activation str The activation function to use for all of the neurons in the DenseLayer, either 'leaky_relu' or 'softmax' required Source code in mlproject/neural_net/_dense_layer.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 class DenseLayer : \"\"\"Fully connected layer of a neural network Parameters ---------- input_n : int The amount of inputs to the DenseLayer output_n : int The amount of outputs to the DenseLayer activation : str The activation function to use for all of the neurons in the DenseLayer, either 'leaky_relu' or 'softmax' \"\"\" def __init__ ( self , input_n : int , output_n : int , activation : str ): # he initiliasation of weights and biases # see https://keras.io/api/layers/initializers/#henormal-class self . output_n , self . input_n = output_n , input_n stddev = np . sqrt ( 2 / input_n ) self . weights = np . random . normal ( loc = 0 , scale = stddev , size = ( input_n , output_n )) self . z = None self . biases = np . zeros ( shape = ( output_n )) if activation == \"leaky_relu\" : self . activation = leaky_relu elif activation == \"softmax\" or activation == \"stable_softmax\" : self . activation = stable_softmax else : raise NotImplementedError ( f \" { activation } not implemented yet. Choose from ['leaky_relu', 'stable_softmax']\" ) def forward ( self , X ): \"\"\"Computes a single forward pass of the DenseLayer Parameters ---------- X : 2d ndarray An n x p matrix of data points where n is the number of data points and p is the number of features. Returns ------- 2d ndarray An n x output_n numpy array where n is the number of samples and output_n is the number of neurons in the DenseLayer \"\"\" self . z = X @ self . weights + self . biases return self . activation ( self . z ) def out_neurons ( self ): \"\"\"Return the number of output neurons in the DenseLayer Returns ------- int The total number of output neurons in the DenseLayer \"\"\" return self . output_n def in_neurons ( self ): \"\"\"Return the number of input neurons in the DenseLayer Returns ------- int The total number of input neurons in the DenseLayer \"\"\" return self . input_n def activation_function ( self ): \"\"\"Return a string representing the activation function of the given DenseLayer Returns ------- string string representing the activation function of the given DenseLayer \"\"\" if self . activation == stable_softmax : return \"softmax\" elif self . activation == leaky_relu : return \"leaky_relu\" activation_function () \u2693\ufe0e Return a string representing the activation function of the given DenseLayer Returns: Type Description string string representing the activation function of the given DenseLayer Source code in mlproject/neural_net/_dense_layer.py 75 76 77 78 79 80 81 82 83 84 85 86 def activation_function ( self ): \"\"\"Return a string representing the activation function of the given DenseLayer Returns ------- string string representing the activation function of the given DenseLayer \"\"\" if self . activation == stable_softmax : return \"softmax\" elif self . activation == leaky_relu : return \"leaky_relu\" forward ( X ) \u2693\ufe0e Computes a single forward pass of the DenseLayer Parameters: Name Type Description Default X 2d ndarray An n x p matrix of data points where n is the number of data points and p is the number of features. required Returns: Type Description 2d ndarray An n x output_n numpy array where n is the number of samples and output_n is the number of neurons in the DenseLayer Source code in mlproject/neural_net/_dense_layer.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def forward ( self , X ): \"\"\"Computes a single forward pass of the DenseLayer Parameters ---------- X : 2d ndarray An n x p matrix of data points where n is the number of data points and p is the number of features. Returns ------- 2d ndarray An n x output_n numpy array where n is the number of samples and output_n is the number of neurons in the DenseLayer \"\"\" self . z = X @ self . weights + self . biases return self . activation ( self . z ) in_neurons () \u2693\ufe0e Return the number of input neurons in the DenseLayer Returns: Type Description int The total number of input neurons in the DenseLayer Source code in mlproject/neural_net/_dense_layer.py 65 66 67 68 69 70 71 72 73 def in_neurons ( self ): \"\"\"Return the number of input neurons in the DenseLayer Returns ------- int The total number of input neurons in the DenseLayer \"\"\" return self . input_n out_neurons () \u2693\ufe0e Return the number of output neurons in the DenseLayer Returns: Type Description int The total number of output neurons in the DenseLayer Source code in mlproject/neural_net/_dense_layer.py 55 56 57 58 59 60 61 62 63 def out_neurons ( self ): \"\"\"Return the number of output neurons in the DenseLayer Returns ------- int The total number of output neurons in the DenseLayer \"\"\" return self . output_n","title":"_dense_layer"},{"location":"reference/mlproject/neural_net/_dense_layer/#mlproject.neural_net._dense_layer.DenseLayer","text":"Fully connected layer of a neural network Parameters: Name Type Description Default input_n int The amount of inputs to the DenseLayer required output_n int The amount of outputs to the DenseLayer required activation str The activation function to use for all of the neurons in the DenseLayer, either 'leaky_relu' or 'softmax' required Source code in mlproject/neural_net/_dense_layer.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 class DenseLayer : \"\"\"Fully connected layer of a neural network Parameters ---------- input_n : int The amount of inputs to the DenseLayer output_n : int The amount of outputs to the DenseLayer activation : str The activation function to use for all of the neurons in the DenseLayer, either 'leaky_relu' or 'softmax' \"\"\" def __init__ ( self , input_n : int , output_n : int , activation : str ): # he initiliasation of weights and biases # see https://keras.io/api/layers/initializers/#henormal-class self . output_n , self . input_n = output_n , input_n stddev = np . sqrt ( 2 / input_n ) self . weights = np . random . normal ( loc = 0 , scale = stddev , size = ( input_n , output_n )) self . z = None self . biases = np . zeros ( shape = ( output_n )) if activation == \"leaky_relu\" : self . activation = leaky_relu elif activation == \"softmax\" or activation == \"stable_softmax\" : self . activation = stable_softmax else : raise NotImplementedError ( f \" { activation } not implemented yet. Choose from ['leaky_relu', 'stable_softmax']\" ) def forward ( self , X ): \"\"\"Computes a single forward pass of the DenseLayer Parameters ---------- X : 2d ndarray An n x p matrix of data points where n is the number of data points and p is the number of features. Returns ------- 2d ndarray An n x output_n numpy array where n is the number of samples and output_n is the number of neurons in the DenseLayer \"\"\" self . z = X @ self . weights + self . biases return self . activation ( self . z ) def out_neurons ( self ): \"\"\"Return the number of output neurons in the DenseLayer Returns ------- int The total number of output neurons in the DenseLayer \"\"\" return self . output_n def in_neurons ( self ): \"\"\"Return the number of input neurons in the DenseLayer Returns ------- int The total number of input neurons in the DenseLayer \"\"\" return self . input_n def activation_function ( self ): \"\"\"Return a string representing the activation function of the given DenseLayer Returns ------- string string representing the activation function of the given DenseLayer \"\"\" if self . activation == stable_softmax : return \"softmax\" elif self . activation == leaky_relu : return \"leaky_relu\"","title":"DenseLayer"},{"location":"reference/mlproject/neural_net/_dense_layer/#mlproject.neural_net._dense_layer.DenseLayer.activation_function","text":"Return a string representing the activation function of the given DenseLayer Returns: Type Description string string representing the activation function of the given DenseLayer Source code in mlproject/neural_net/_dense_layer.py 75 76 77 78 79 80 81 82 83 84 85 86 def activation_function ( self ): \"\"\"Return a string representing the activation function of the given DenseLayer Returns ------- string string representing the activation function of the given DenseLayer \"\"\" if self . activation == stable_softmax : return \"softmax\" elif self . activation == leaky_relu : return \"leaky_relu\"","title":"activation_function()"},{"location":"reference/mlproject/neural_net/_dense_layer/#mlproject.neural_net._dense_layer.DenseLayer.forward","text":"Computes a single forward pass of the DenseLayer Parameters: Name Type Description Default X 2d ndarray An n x p matrix of data points where n is the number of data points and p is the number of features. required Returns: Type Description 2d ndarray An n x output_n numpy array where n is the number of samples and output_n is the number of neurons in the DenseLayer Source code in mlproject/neural_net/_dense_layer.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def forward ( self , X ): \"\"\"Computes a single forward pass of the DenseLayer Parameters ---------- X : 2d ndarray An n x p matrix of data points where n is the number of data points and p is the number of features. Returns ------- 2d ndarray An n x output_n numpy array where n is the number of samples and output_n is the number of neurons in the DenseLayer \"\"\" self . z = X @ self . weights + self . biases return self . activation ( self . z )","title":"forward()"},{"location":"reference/mlproject/neural_net/_dense_layer/#mlproject.neural_net._dense_layer.DenseLayer.in_neurons","text":"Return the number of input neurons in the DenseLayer Returns: Type Description int The total number of input neurons in the DenseLayer Source code in mlproject/neural_net/_dense_layer.py 65 66 67 68 69 70 71 72 73 def in_neurons ( self ): \"\"\"Return the number of input neurons in the DenseLayer Returns ------- int The total number of input neurons in the DenseLayer \"\"\" return self . input_n","title":"in_neurons()"},{"location":"reference/mlproject/neural_net/_dense_layer/#mlproject.neural_net._dense_layer.DenseLayer.out_neurons","text":"Return the number of output neurons in the DenseLayer Returns: Type Description int The total number of output neurons in the DenseLayer Source code in mlproject/neural_net/_dense_layer.py 55 56 57 58 59 60 61 62 63 def out_neurons ( self ): \"\"\"Return the number of output neurons in the DenseLayer Returns ------- int The total number of output neurons in the DenseLayer \"\"\" return self . output_n","title":"out_neurons()"},{"location":"reference/mlproject/neural_net/_loss/","text":"cross_entropy_loss ( y_true , y_pred ) \u2693\ufe0e Compute the categorical cross entropy loss from the given true labels and predicted labels. We add \\(1\\mathrm{e}{-7}\\) (epsilon) to the prediction to avoid taking the log of \\(0\\) Inspired by keras implemenation: Keras implementation where epsilon is defined here Parameters: Name Type Description Default y_true 1d ndarray true class labels of size 1 x n where n is the number of data points. required y_pred 1d ndarray predicted class labels of size 1 x n where n is the number of data points. required Returns: Type Description float Cross entropy score for the given prediction Source code in mlproject/neural_net/_loss.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 def cross_entropy_loss ( y_true , y_pred ): r \"\"\"Compute the categorical cross entropy loss from the given true labels and predicted labels. We add $1\\mathrm{e}{-7}$ (epsilon) to the prediction to avoid taking the log of $0$ - Inspired by keras implemenation: [Keras implementation](https://github.com/keras-team/keras/blob/master/keras/backend.py#L5487-L5547) where epsilon is defined [here](https://github.com/keras-team/keras/blob/master/keras/backend_config.py#L34-L44) Parameters ---------- y_true : 1d ndarray true class labels of size 1 x n where n is the number of data points. y_pred : 1d ndarray predicted class labels of size 1 x n where n is the number of data points. Returns ------- float Cross entropy score for the given prediction \"\"\" epsilon = 1e-07 return - np . sum ( np . log ( y_pred + epsilon ) * y_true )","title":"_loss"},{"location":"reference/mlproject/neural_net/_loss/#mlproject.neural_net._loss.cross_entropy_loss","text":"Compute the categorical cross entropy loss from the given true labels and predicted labels. We add \\(1\\mathrm{e}{-7}\\) (epsilon) to the prediction to avoid taking the log of \\(0\\) Inspired by keras implemenation: Keras implementation where epsilon is defined here Parameters: Name Type Description Default y_true 1d ndarray true class labels of size 1 x n where n is the number of data points. required y_pred 1d ndarray predicted class labels of size 1 x n where n is the number of data points. required Returns: Type Description float Cross entropy score for the given prediction Source code in mlproject/neural_net/_loss.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 def cross_entropy_loss ( y_true , y_pred ): r \"\"\"Compute the categorical cross entropy loss from the given true labels and predicted labels. We add $1\\mathrm{e}{-7}$ (epsilon) to the prediction to avoid taking the log of $0$ - Inspired by keras implemenation: [Keras implementation](https://github.com/keras-team/keras/blob/master/keras/backend.py#L5487-L5547) where epsilon is defined [here](https://github.com/keras-team/keras/blob/master/keras/backend_config.py#L34-L44) Parameters ---------- y_true : 1d ndarray true class labels of size 1 x n where n is the number of data points. y_pred : 1d ndarray predicted class labels of size 1 x n where n is the number of data points. Returns ------- float Cross entropy score for the given prediction \"\"\" epsilon = 1e-07 return - np . sum ( np . log ( y_pred + epsilon ) * y_true )","title":"cross_entropy_loss()"},{"location":"reference/mlproject/neural_net/_neural_net/","text":"NeuralNetworkClassifier \u2693\ufe0e NeuralNetworkClassifier Feed Forward Neural Network Classifier with however many dense layers (fully connected layers) of class DenseLayer each with own activation function and a network wide loss function. The layers of the network can either be added when initilizing the network, as a list or added individually with the add method after initialization. Parameters: Name Type Description Default layers list , optional A list of class DenseLayer [] loss str , optional The loss function to be used, currently only cross_entropy is supported. 'cross_entropy' Attributes: Name Type Description X 2d ndarray Data points to use for training the neural network y 1d ndarray Target classes n int Number of data points (X.shape[0]) p int Number of features (X.shape[1]) Source code in mlproject/neural_net/_neural_net.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 class NeuralNetworkClassifier : \"\"\"NeuralNetworkClassifier Feed Forward Neural Network Classifier with however many dense layers (fully connected layers) of class [`DenseLayer`][mlproject.neural_net._dense_layer.DenseLayer] each with own activation function and a network wide loss function. The layers of the network can either be added when initilizing the network, as a list or added individually with the [`add`][mlproject.neural_net._neural_net.NeuralNetworkClassifier.add] method after initialization. Parameters ---------- layers : list, optional A list of class [`DenseLayer`][mlproject.neural_net._dense_layer.DenseLayer] loss : str, optional The loss function to be used, currently only [`cross_entropy`][mlproject.neural_net._loss.cross_entropy_loss] is supported. Attributes ---------- X : 2d ndarray Data points to use for training the neural network y : 1d ndarray Target classes n : int Number of data points (X.shape[0]) p : int Number of features (X.shape[1]) \"\"\" def __init__ ( self , layers = [], loss = \"cross_entropy\" ): self . X = None self . n , self . p = None , None self . y = None self . k = None self . layers = layers self . activations , self . sums = [], [] if loss == \"cross_entropy\" : self . loss_str = \"cross_entropy_loss\" self . loss = cross_entropy_loss else : raise NotImplementedError ( f \" { loss } not implemented yet. Choose from ['cross_entropy']\" ) def add ( self , layer : DenseLayer ): \"\"\"Add a new layer to the network, after the current layer. Parameters ---------- layer : DenseLayer Fully connected layer. Example ------- ``` py >>> NN = NeuralNetworkClassifier(loss='cross_entropy') >>> NN.add(DenseLayer(784,128,\"leaky_relu\")) >>> NN.add(DenseLayer(128,5,\"softmax\")) >>> print(NN) NeuralNetworkClassifier -------------------------------- Loss function: cross_entropy_loss Input layer: Input: 784, Output: 128 , Activation: leaky_relu Output layer: Input: 128, Output: 5 , Activation: softmax ``` \"\"\" self . layers . append ( layer ) def forward ( self , X ): \"\"\"Compute a single forward pass of the network. Parameters ---------- X : 2d ndarray The data to use for the forward pass. Must be of size n x input_n where input_n must come from the first [`DenseLayer`][mlproject.neural_net._dense_layer.DenseLayer] in the network. Returns ------- 2d ndarray An n x output_n array where output_n corresponds to the output_n of the last [`DenseLayer`][mlproject.neural_net._dense_layer.DenseLayer] in the network and n is the number of data points. \"\"\" self . activations . append ( X ) for layer in self . layers : X = layer . forward ( X ) self . activations . append ( X ) self . sums . append ( layer . z ) return X def predict ( self , X ): \"\"\"Predict class labels for the given data. Parameters ---------- X : 2d ndarray The data that we want to use to make predictions. Returns ------- 1d ndarray All predicted class labels with size n, where n is the number of data points. \"\"\" probabilities = self . predict_proba ( X ) return np . array ( [ self . label [ pred ] for pred in np . argmax ( probabilities , axis = 1 ) . astype ( int )] ) def predict_proba ( self , X ): \"\"\"Predict class probabilities for the given data Parameters ---------- X : 2d ndarray The data that we want to use to make predictions Returns ------- 2d ndarray All probabilites with size n x k, where n is the number of data points and k is the number classes \"\"\" return self . forward ( X ) def fit ( self , X , y , batches : int = 1 , epochs : int = 1000 , lr : float = 0.01 ): r \"\"\"The actual training of the network to the given data Parameters ---------- X : 2d ndarray An $N \\times P$ matrix of data points where n is the number of data points and p is the number of features. y : 1d ndarray $N \\times 1$ vector of target class labels batches : int, optional The number of batches to use for training in each epoch, an integer indicating the number of splits to split the data into, by default $1$ which corresponds to training on the entire dataset in every epoch. epochs : int, optional The number of iterations to train for lr : float, optional The learning rate for gradient descent \"\"\" self . X = X self . n , self . p = self . X . shape self . y = y self . learning_rate = lr unique_classes = np . unique ( y ) self . k = len ( unique_classes ) one_hot = OneHotEncoder ( categories = [ unique_classes ]) self . y_hot_encoded = one_hot . fit_transform ( self . y ) . toarray () if self . layers [ - 1 ] . out_neurons () != self . k : raise ValueError ( f \"The number of neurons in the output layer, output_n: ( { self . layers [ - 1 ] . out_neurons () } ) must be equal to the number of classes, k: ( { self . k } )\" ) if self . layers [ 0 ] . in_neurons () != self . X . shape [ 1 ]: raise ValueError ( f \"The number of neurons in the input layer, input_n: ( { self . layers [ 0 ] . in_neurons () } ) must be equal to the number features in the dataset: ( { self . X . shape [ 1 ] } )\" ) # populate label-intcode dictionaries self . label = { k : unique_classes [ k ] for k in range ( self . k )} self . intcode = { unique_classes [ k ]: k for k in range ( self . k )} self . loss_history = [] self . accuracy_history = [] # get indices of every data point idxs = np . arange ( self . n ) with progress as pb : t1 = pb . add_task ( \"[blue]Training\" , total = epochs ) for epoch in range ( epochs ): # randomly shuffle the data --> split it into number of batches # here np.array_split returns an array of arrays of indices # of the different splits np . random . shuffle ( idxs ) batch_idxs = np . array_split ( idxs , batches ) for batch in batch_idxs : X_batch = self . X [ batch ] y_batch = self . y_hot_encoded [ batch ] # compute the initial class probabilities by doing a single forward pass # note: this should come 'automatically' from defining the last layer # in the model as a layer with output_n = k with softmax activation # where k is the number of classes. init_probs = self . forward ( X_batch ) # dividide by the number of data points in this specific batch to get the average loss. loss = self . loss ( y_batch , init_probs ) / len ( y_batch ) self . backward ( y_batch ) # add the latest loss to the history self . loss_history . append ( loss ) # predict with the current weights and biases on the whole data set batch_predict = self . predict ( self . X ) # calculate the accuracy score of the prediction train_accuracy = accuracy_score ( self . y , batch_predict ) # add accuracy to the history self . accuracy_history . append ( train_accuracy ) # update rich progress bar for each epoch pb . update ( t1 , advance = 1 ) if progress . finished : pb . update ( t1 , description = \"[bright_green]Training complete!\" ) def backward ( self , y_batch ): \"\"\"Computes a single backward pass all the way through the network. as well as updating the weights and biases. Parameters ---------- y_batch : 2d ndarray array of one-hot encoded ground_truth labels \"\"\" delta = self . activations [ - 1 ] - y_batch grad_bias = delta . sum ( 0 ) grad_weight = self . activations [ - 2 ] . T @ delta grad_biases , grad_weights = [], [] grad_weights . append ( grad_weight ) grad_biases . append ( grad_bias ) for i in range ( 2 , len ( self . layers ) + 1 ): layer = self . layers [ - i + 1 ] dzda = delta @ layer . weights . T delta = dzda * leaky_relu_der ( self . sums [ - i ]) grad_bias = delta . sum ( 0 ) grad_weight = self . activations [ - i - 1 ] . T @ delta grad_weights . append ( grad_weight ) grad_biases . append ( grad_bias ) # reverse the gradient lists so we can index them normally. grad_biases_rev = list ( reversed ( grad_biases )) grad_weights_rev = list ( reversed ( grad_weights )) for i in range ( 0 , len ( self . layers )): self . layers [ i ] . weights -= self . learning_rate * grad_weights_rev [ i ] self . layers [ i ] . biases -= self . learning_rate * grad_biases_rev [ i ] def __str__ ( self ): s = \" \\n NeuralNetworkClassifier \\n \" s += \"-------------------------------- \\n \" s += f \"Loss function: { self . loss_str } \\n\\n \" layers = [ self . layers [ i ] for i in range ( 0 , len ( self . layers ))] layers_neu = [ f \" \\t Input: { i . input_n } , Output: { i . output_n } , Activation: { i . activation_function () } \" for i in layers ] layer_num = 0 for layer in layers_neu : if layer_num == 0 : s += \"Input layer: \\n \" + layer + \" \\n\\n \" elif layer_num == len ( self . layers ) - 1 : s += f \"Output layer: \\n \" + layer else : s += f \"Layer: { layer_num } \\n \" + layer + \" \\n\\n \" layer_num += 1 return s add ( layer ) \u2693\ufe0e Add a new layer to the network, after the current layer. Parameters: Name Type Description Default layer DenseLayer Fully connected layer. required Example \u2693\ufe0e >>> NN = NeuralNetworkClassifier ( loss = 'cross_entropy' ) >>> NN . add ( DenseLayer ( 784 , 128 , \"leaky_relu\" )) >>> NN . add ( DenseLayer ( 128 , 5 , \"softmax\" )) >>> print ( NN ) NeuralNetworkClassifier -------------------------------- Loss function : cross_entropy_loss Input layer : Input : 784 , Output : 128 , Activation : leaky_relu Output layer : Input : 128 , Output : 5 , Activation : softmax Source code in mlproject/neural_net/_neural_net.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 def add ( self , layer : DenseLayer ): \"\"\"Add a new layer to the network, after the current layer. Parameters ---------- layer : DenseLayer Fully connected layer. Example ------- ``` py >>> NN = NeuralNetworkClassifier(loss='cross_entropy') >>> NN.add(DenseLayer(784,128,\"leaky_relu\")) >>> NN.add(DenseLayer(128,5,\"softmax\")) >>> print(NN) NeuralNetworkClassifier -------------------------------- Loss function: cross_entropy_loss Input layer: Input: 784, Output: 128 , Activation: leaky_relu Output layer: Input: 128, Output: 5 , Activation: softmax ``` \"\"\" self . layers . append ( layer ) backward ( y_batch ) \u2693\ufe0e Computes a single backward pass all the way through the network. as well as updating the weights and biases. Parameters: Name Type Description Default y_batch 2d ndarray array of one-hot encoded ground_truth labels required Source code in mlproject/neural_net/_neural_net.py 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 def backward ( self , y_batch ): \"\"\"Computes a single backward pass all the way through the network. as well as updating the weights and biases. Parameters ---------- y_batch : 2d ndarray array of one-hot encoded ground_truth labels \"\"\" delta = self . activations [ - 1 ] - y_batch grad_bias = delta . sum ( 0 ) grad_weight = self . activations [ - 2 ] . T @ delta grad_biases , grad_weights = [], [] grad_weights . append ( grad_weight ) grad_biases . append ( grad_bias ) for i in range ( 2 , len ( self . layers ) + 1 ): layer = self . layers [ - i + 1 ] dzda = delta @ layer . weights . T delta = dzda * leaky_relu_der ( self . sums [ - i ]) grad_bias = delta . sum ( 0 ) grad_weight = self . activations [ - i - 1 ] . T @ delta grad_weights . append ( grad_weight ) grad_biases . append ( grad_bias ) # reverse the gradient lists so we can index them normally. grad_biases_rev = list ( reversed ( grad_biases )) grad_weights_rev = list ( reversed ( grad_weights )) for i in range ( 0 , len ( self . layers )): self . layers [ i ] . weights -= self . learning_rate * grad_weights_rev [ i ] self . layers [ i ] . biases -= self . learning_rate * grad_biases_rev [ i ] fit ( X , y , batches = 1 , epochs = 1000 , lr = 0.01 ) \u2693\ufe0e The actual training of the network to the given data Parameters: Name Type Description Default X 2d ndarray An \\(N \\times P\\) matrix of data points where n is the number of data points and p is the number of features. required y 1d ndarray \\(N \\times 1\\) vector of target class labels required batches int , optional The number of batches to use for training in each epoch, an integer indicating the number of splits to split the data into, by default \\(1\\) which corresponds to training on the entire dataset in every epoch. 1 epochs int , optional The number of iterations to train for 1000 lr float , optional The learning rate for gradient descent 0.01 Source code in mlproject/neural_net/_neural_net.py 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 def fit ( self , X , y , batches : int = 1 , epochs : int = 1000 , lr : float = 0.01 ): r \"\"\"The actual training of the network to the given data Parameters ---------- X : 2d ndarray An $N \\times P$ matrix of data points where n is the number of data points and p is the number of features. y : 1d ndarray $N \\times 1$ vector of target class labels batches : int, optional The number of batches to use for training in each epoch, an integer indicating the number of splits to split the data into, by default $1$ which corresponds to training on the entire dataset in every epoch. epochs : int, optional The number of iterations to train for lr : float, optional The learning rate for gradient descent \"\"\" self . X = X self . n , self . p = self . X . shape self . y = y self . learning_rate = lr unique_classes = np . unique ( y ) self . k = len ( unique_classes ) one_hot = OneHotEncoder ( categories = [ unique_classes ]) self . y_hot_encoded = one_hot . fit_transform ( self . y ) . toarray () if self . layers [ - 1 ] . out_neurons () != self . k : raise ValueError ( f \"The number of neurons in the output layer, output_n: ( { self . layers [ - 1 ] . out_neurons () } ) must be equal to the number of classes, k: ( { self . k } )\" ) if self . layers [ 0 ] . in_neurons () != self . X . shape [ 1 ]: raise ValueError ( f \"The number of neurons in the input layer, input_n: ( { self . layers [ 0 ] . in_neurons () } ) must be equal to the number features in the dataset: ( { self . X . shape [ 1 ] } )\" ) # populate label-intcode dictionaries self . label = { k : unique_classes [ k ] for k in range ( self . k )} self . intcode = { unique_classes [ k ]: k for k in range ( self . k )} self . loss_history = [] self . accuracy_history = [] # get indices of every data point idxs = np . arange ( self . n ) with progress as pb : t1 = pb . add_task ( \"[blue]Training\" , total = epochs ) for epoch in range ( epochs ): # randomly shuffle the data --> split it into number of batches # here np.array_split returns an array of arrays of indices # of the different splits np . random . shuffle ( idxs ) batch_idxs = np . array_split ( idxs , batches ) for batch in batch_idxs : X_batch = self . X [ batch ] y_batch = self . y_hot_encoded [ batch ] # compute the initial class probabilities by doing a single forward pass # note: this should come 'automatically' from defining the last layer # in the model as a layer with output_n = k with softmax activation # where k is the number of classes. init_probs = self . forward ( X_batch ) # dividide by the number of data points in this specific batch to get the average loss. loss = self . loss ( y_batch , init_probs ) / len ( y_batch ) self . backward ( y_batch ) # add the latest loss to the history self . loss_history . append ( loss ) # predict with the current weights and biases on the whole data set batch_predict = self . predict ( self . X ) # calculate the accuracy score of the prediction train_accuracy = accuracy_score ( self . y , batch_predict ) # add accuracy to the history self . accuracy_history . append ( train_accuracy ) # update rich progress bar for each epoch pb . update ( t1 , advance = 1 ) if progress . finished : pb . update ( t1 , description = \"[bright_green]Training complete!\" ) forward ( X ) \u2693\ufe0e Compute a single forward pass of the network. Parameters: Name Type Description Default X 2d ndarray The data to use for the forward pass. Must be of size n x input_n where input_n must come from the first DenseLayer in the network. required Returns: Type Description 2d ndarray An n x output_n array where output_n corresponds to the output_n of the last DenseLayer in the network and n is the number of data points. Source code in mlproject/neural_net/_neural_net.py 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 def forward ( self , X ): \"\"\"Compute a single forward pass of the network. Parameters ---------- X : 2d ndarray The data to use for the forward pass. Must be of size n x input_n where input_n must come from the first [`DenseLayer`][mlproject.neural_net._dense_layer.DenseLayer] in the network. Returns ------- 2d ndarray An n x output_n array where output_n corresponds to the output_n of the last [`DenseLayer`][mlproject.neural_net._dense_layer.DenseLayer] in the network and n is the number of data points. \"\"\" self . activations . append ( X ) for layer in self . layers : X = layer . forward ( X ) self . activations . append ( X ) self . sums . append ( layer . z ) return X predict ( X ) \u2693\ufe0e Predict class labels for the given data. Parameters: Name Type Description Default X 2d ndarray The data that we want to use to make predictions. required Returns: Type Description 1d ndarray All predicted class labels with size n, where n is the number of data points. Source code in mlproject/neural_net/_neural_net.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def predict ( self , X ): \"\"\"Predict class labels for the given data. Parameters ---------- X : 2d ndarray The data that we want to use to make predictions. Returns ------- 1d ndarray All predicted class labels with size n, where n is the number of data points. \"\"\" probabilities = self . predict_proba ( X ) return np . array ( [ self . label [ pred ] for pred in np . argmax ( probabilities , axis = 1 ) . astype ( int )] ) predict_proba ( X ) \u2693\ufe0e Predict class probabilities for the given data Parameters: Name Type Description Default X 2d ndarray The data that we want to use to make predictions required Returns: Type Description 2d ndarray All probabilites with size n x k, where n is the number of data points and k is the number classes Source code in mlproject/neural_net/_neural_net.py 146 147 148 149 150 151 152 153 154 155 156 157 158 159 def predict_proba ( self , X ): \"\"\"Predict class probabilities for the given data Parameters ---------- X : 2d ndarray The data that we want to use to make predictions Returns ------- 2d ndarray All probabilites with size n x k, where n is the number of data points and k is the number classes \"\"\" return self . forward ( X )","title":"_neural_net"},{"location":"reference/mlproject/neural_net/_neural_net/#mlproject.neural_net._neural_net.NeuralNetworkClassifier","text":"NeuralNetworkClassifier Feed Forward Neural Network Classifier with however many dense layers (fully connected layers) of class DenseLayer each with own activation function and a network wide loss function. The layers of the network can either be added when initilizing the network, as a list or added individually with the add method after initialization. Parameters: Name Type Description Default layers list , optional A list of class DenseLayer [] loss str , optional The loss function to be used, currently only cross_entropy is supported. 'cross_entropy' Attributes: Name Type Description X 2d ndarray Data points to use for training the neural network y 1d ndarray Target classes n int Number of data points (X.shape[0]) p int Number of features (X.shape[1]) Source code in mlproject/neural_net/_neural_net.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 class NeuralNetworkClassifier : \"\"\"NeuralNetworkClassifier Feed Forward Neural Network Classifier with however many dense layers (fully connected layers) of class [`DenseLayer`][mlproject.neural_net._dense_layer.DenseLayer] each with own activation function and a network wide loss function. The layers of the network can either be added when initilizing the network, as a list or added individually with the [`add`][mlproject.neural_net._neural_net.NeuralNetworkClassifier.add] method after initialization. Parameters ---------- layers : list, optional A list of class [`DenseLayer`][mlproject.neural_net._dense_layer.DenseLayer] loss : str, optional The loss function to be used, currently only [`cross_entropy`][mlproject.neural_net._loss.cross_entropy_loss] is supported. Attributes ---------- X : 2d ndarray Data points to use for training the neural network y : 1d ndarray Target classes n : int Number of data points (X.shape[0]) p : int Number of features (X.shape[1]) \"\"\" def __init__ ( self , layers = [], loss = \"cross_entropy\" ): self . X = None self . n , self . p = None , None self . y = None self . k = None self . layers = layers self . activations , self . sums = [], [] if loss == \"cross_entropy\" : self . loss_str = \"cross_entropy_loss\" self . loss = cross_entropy_loss else : raise NotImplementedError ( f \" { loss } not implemented yet. Choose from ['cross_entropy']\" ) def add ( self , layer : DenseLayer ): \"\"\"Add a new layer to the network, after the current layer. Parameters ---------- layer : DenseLayer Fully connected layer. Example ------- ``` py >>> NN = NeuralNetworkClassifier(loss='cross_entropy') >>> NN.add(DenseLayer(784,128,\"leaky_relu\")) >>> NN.add(DenseLayer(128,5,\"softmax\")) >>> print(NN) NeuralNetworkClassifier -------------------------------- Loss function: cross_entropy_loss Input layer: Input: 784, Output: 128 , Activation: leaky_relu Output layer: Input: 128, Output: 5 , Activation: softmax ``` \"\"\" self . layers . append ( layer ) def forward ( self , X ): \"\"\"Compute a single forward pass of the network. Parameters ---------- X : 2d ndarray The data to use for the forward pass. Must be of size n x input_n where input_n must come from the first [`DenseLayer`][mlproject.neural_net._dense_layer.DenseLayer] in the network. Returns ------- 2d ndarray An n x output_n array where output_n corresponds to the output_n of the last [`DenseLayer`][mlproject.neural_net._dense_layer.DenseLayer] in the network and n is the number of data points. \"\"\" self . activations . append ( X ) for layer in self . layers : X = layer . forward ( X ) self . activations . append ( X ) self . sums . append ( layer . z ) return X def predict ( self , X ): \"\"\"Predict class labels for the given data. Parameters ---------- X : 2d ndarray The data that we want to use to make predictions. Returns ------- 1d ndarray All predicted class labels with size n, where n is the number of data points. \"\"\" probabilities = self . predict_proba ( X ) return np . array ( [ self . label [ pred ] for pred in np . argmax ( probabilities , axis = 1 ) . astype ( int )] ) def predict_proba ( self , X ): \"\"\"Predict class probabilities for the given data Parameters ---------- X : 2d ndarray The data that we want to use to make predictions Returns ------- 2d ndarray All probabilites with size n x k, where n is the number of data points and k is the number classes \"\"\" return self . forward ( X ) def fit ( self , X , y , batches : int = 1 , epochs : int = 1000 , lr : float = 0.01 ): r \"\"\"The actual training of the network to the given data Parameters ---------- X : 2d ndarray An $N \\times P$ matrix of data points where n is the number of data points and p is the number of features. y : 1d ndarray $N \\times 1$ vector of target class labels batches : int, optional The number of batches to use for training in each epoch, an integer indicating the number of splits to split the data into, by default $1$ which corresponds to training on the entire dataset in every epoch. epochs : int, optional The number of iterations to train for lr : float, optional The learning rate for gradient descent \"\"\" self . X = X self . n , self . p = self . X . shape self . y = y self . learning_rate = lr unique_classes = np . unique ( y ) self . k = len ( unique_classes ) one_hot = OneHotEncoder ( categories = [ unique_classes ]) self . y_hot_encoded = one_hot . fit_transform ( self . y ) . toarray () if self . layers [ - 1 ] . out_neurons () != self . k : raise ValueError ( f \"The number of neurons in the output layer, output_n: ( { self . layers [ - 1 ] . out_neurons () } ) must be equal to the number of classes, k: ( { self . k } )\" ) if self . layers [ 0 ] . in_neurons () != self . X . shape [ 1 ]: raise ValueError ( f \"The number of neurons in the input layer, input_n: ( { self . layers [ 0 ] . in_neurons () } ) must be equal to the number features in the dataset: ( { self . X . shape [ 1 ] } )\" ) # populate label-intcode dictionaries self . label = { k : unique_classes [ k ] for k in range ( self . k )} self . intcode = { unique_classes [ k ]: k for k in range ( self . k )} self . loss_history = [] self . accuracy_history = [] # get indices of every data point idxs = np . arange ( self . n ) with progress as pb : t1 = pb . add_task ( \"[blue]Training\" , total = epochs ) for epoch in range ( epochs ): # randomly shuffle the data --> split it into number of batches # here np.array_split returns an array of arrays of indices # of the different splits np . random . shuffle ( idxs ) batch_idxs = np . array_split ( idxs , batches ) for batch in batch_idxs : X_batch = self . X [ batch ] y_batch = self . y_hot_encoded [ batch ] # compute the initial class probabilities by doing a single forward pass # note: this should come 'automatically' from defining the last layer # in the model as a layer with output_n = k with softmax activation # where k is the number of classes. init_probs = self . forward ( X_batch ) # dividide by the number of data points in this specific batch to get the average loss. loss = self . loss ( y_batch , init_probs ) / len ( y_batch ) self . backward ( y_batch ) # add the latest loss to the history self . loss_history . append ( loss ) # predict with the current weights and biases on the whole data set batch_predict = self . predict ( self . X ) # calculate the accuracy score of the prediction train_accuracy = accuracy_score ( self . y , batch_predict ) # add accuracy to the history self . accuracy_history . append ( train_accuracy ) # update rich progress bar for each epoch pb . update ( t1 , advance = 1 ) if progress . finished : pb . update ( t1 , description = \"[bright_green]Training complete!\" ) def backward ( self , y_batch ): \"\"\"Computes a single backward pass all the way through the network. as well as updating the weights and biases. Parameters ---------- y_batch : 2d ndarray array of one-hot encoded ground_truth labels \"\"\" delta = self . activations [ - 1 ] - y_batch grad_bias = delta . sum ( 0 ) grad_weight = self . activations [ - 2 ] . T @ delta grad_biases , grad_weights = [], [] grad_weights . append ( grad_weight ) grad_biases . append ( grad_bias ) for i in range ( 2 , len ( self . layers ) + 1 ): layer = self . layers [ - i + 1 ] dzda = delta @ layer . weights . T delta = dzda * leaky_relu_der ( self . sums [ - i ]) grad_bias = delta . sum ( 0 ) grad_weight = self . activations [ - i - 1 ] . T @ delta grad_weights . append ( grad_weight ) grad_biases . append ( grad_bias ) # reverse the gradient lists so we can index them normally. grad_biases_rev = list ( reversed ( grad_biases )) grad_weights_rev = list ( reversed ( grad_weights )) for i in range ( 0 , len ( self . layers )): self . layers [ i ] . weights -= self . learning_rate * grad_weights_rev [ i ] self . layers [ i ] . biases -= self . learning_rate * grad_biases_rev [ i ] def __str__ ( self ): s = \" \\n NeuralNetworkClassifier \\n \" s += \"-------------------------------- \\n \" s += f \"Loss function: { self . loss_str } \\n\\n \" layers = [ self . layers [ i ] for i in range ( 0 , len ( self . layers ))] layers_neu = [ f \" \\t Input: { i . input_n } , Output: { i . output_n } , Activation: { i . activation_function () } \" for i in layers ] layer_num = 0 for layer in layers_neu : if layer_num == 0 : s += \"Input layer: \\n \" + layer + \" \\n\\n \" elif layer_num == len ( self . layers ) - 1 : s += f \"Output layer: \\n \" + layer else : s += f \"Layer: { layer_num } \\n \" + layer + \" \\n\\n \" layer_num += 1 return s","title":"NeuralNetworkClassifier"},{"location":"reference/mlproject/neural_net/_neural_net/#mlproject.neural_net._neural_net.NeuralNetworkClassifier.add","text":"Add a new layer to the network, after the current layer. Parameters: Name Type Description Default layer DenseLayer Fully connected layer. required","title":"add()"},{"location":"reference/mlproject/neural_net/_neural_net/#mlproject.neural_net._neural_net.NeuralNetworkClassifier.add--example","text":">>> NN = NeuralNetworkClassifier ( loss = 'cross_entropy' ) >>> NN . add ( DenseLayer ( 784 , 128 , \"leaky_relu\" )) >>> NN . add ( DenseLayer ( 128 , 5 , \"softmax\" )) >>> print ( NN ) NeuralNetworkClassifier -------------------------------- Loss function : cross_entropy_loss Input layer : Input : 784 , Output : 128 , Activation : leaky_relu Output layer : Input : 128 , Output : 5 , Activation : softmax Source code in mlproject/neural_net/_neural_net.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 def add ( self , layer : DenseLayer ): \"\"\"Add a new layer to the network, after the current layer. Parameters ---------- layer : DenseLayer Fully connected layer. Example ------- ``` py >>> NN = NeuralNetworkClassifier(loss='cross_entropy') >>> NN.add(DenseLayer(784,128,\"leaky_relu\")) >>> NN.add(DenseLayer(128,5,\"softmax\")) >>> print(NN) NeuralNetworkClassifier -------------------------------- Loss function: cross_entropy_loss Input layer: Input: 784, Output: 128 , Activation: leaky_relu Output layer: Input: 128, Output: 5 , Activation: softmax ``` \"\"\" self . layers . append ( layer )","title":"Example"},{"location":"reference/mlproject/neural_net/_neural_net/#mlproject.neural_net._neural_net.NeuralNetworkClassifier.backward","text":"Computes a single backward pass all the way through the network. as well as updating the weights and biases. Parameters: Name Type Description Default y_batch 2d ndarray array of one-hot encoded ground_truth labels required Source code in mlproject/neural_net/_neural_net.py 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 def backward ( self , y_batch ): \"\"\"Computes a single backward pass all the way through the network. as well as updating the weights and biases. Parameters ---------- y_batch : 2d ndarray array of one-hot encoded ground_truth labels \"\"\" delta = self . activations [ - 1 ] - y_batch grad_bias = delta . sum ( 0 ) grad_weight = self . activations [ - 2 ] . T @ delta grad_biases , grad_weights = [], [] grad_weights . append ( grad_weight ) grad_biases . append ( grad_bias ) for i in range ( 2 , len ( self . layers ) + 1 ): layer = self . layers [ - i + 1 ] dzda = delta @ layer . weights . T delta = dzda * leaky_relu_der ( self . sums [ - i ]) grad_bias = delta . sum ( 0 ) grad_weight = self . activations [ - i - 1 ] . T @ delta grad_weights . append ( grad_weight ) grad_biases . append ( grad_bias ) # reverse the gradient lists so we can index them normally. grad_biases_rev = list ( reversed ( grad_biases )) grad_weights_rev = list ( reversed ( grad_weights )) for i in range ( 0 , len ( self . layers )): self . layers [ i ] . weights -= self . learning_rate * grad_weights_rev [ i ] self . layers [ i ] . biases -= self . learning_rate * grad_biases_rev [ i ]","title":"backward()"},{"location":"reference/mlproject/neural_net/_neural_net/#mlproject.neural_net._neural_net.NeuralNetworkClassifier.fit","text":"The actual training of the network to the given data Parameters: Name Type Description Default X 2d ndarray An \\(N \\times P\\) matrix of data points where n is the number of data points and p is the number of features. required y 1d ndarray \\(N \\times 1\\) vector of target class labels required batches int , optional The number of batches to use for training in each epoch, an integer indicating the number of splits to split the data into, by default \\(1\\) which corresponds to training on the entire dataset in every epoch. 1 epochs int , optional The number of iterations to train for 1000 lr float , optional The learning rate for gradient descent 0.01 Source code in mlproject/neural_net/_neural_net.py 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 def fit ( self , X , y , batches : int = 1 , epochs : int = 1000 , lr : float = 0.01 ): r \"\"\"The actual training of the network to the given data Parameters ---------- X : 2d ndarray An $N \\times P$ matrix of data points where n is the number of data points and p is the number of features. y : 1d ndarray $N \\times 1$ vector of target class labels batches : int, optional The number of batches to use for training in each epoch, an integer indicating the number of splits to split the data into, by default $1$ which corresponds to training on the entire dataset in every epoch. epochs : int, optional The number of iterations to train for lr : float, optional The learning rate for gradient descent \"\"\" self . X = X self . n , self . p = self . X . shape self . y = y self . learning_rate = lr unique_classes = np . unique ( y ) self . k = len ( unique_classes ) one_hot = OneHotEncoder ( categories = [ unique_classes ]) self . y_hot_encoded = one_hot . fit_transform ( self . y ) . toarray () if self . layers [ - 1 ] . out_neurons () != self . k : raise ValueError ( f \"The number of neurons in the output layer, output_n: ( { self . layers [ - 1 ] . out_neurons () } ) must be equal to the number of classes, k: ( { self . k } )\" ) if self . layers [ 0 ] . in_neurons () != self . X . shape [ 1 ]: raise ValueError ( f \"The number of neurons in the input layer, input_n: ( { self . layers [ 0 ] . in_neurons () } ) must be equal to the number features in the dataset: ( { self . X . shape [ 1 ] } )\" ) # populate label-intcode dictionaries self . label = { k : unique_classes [ k ] for k in range ( self . k )} self . intcode = { unique_classes [ k ]: k for k in range ( self . k )} self . loss_history = [] self . accuracy_history = [] # get indices of every data point idxs = np . arange ( self . n ) with progress as pb : t1 = pb . add_task ( \"[blue]Training\" , total = epochs ) for epoch in range ( epochs ): # randomly shuffle the data --> split it into number of batches # here np.array_split returns an array of arrays of indices # of the different splits np . random . shuffle ( idxs ) batch_idxs = np . array_split ( idxs , batches ) for batch in batch_idxs : X_batch = self . X [ batch ] y_batch = self . y_hot_encoded [ batch ] # compute the initial class probabilities by doing a single forward pass # note: this should come 'automatically' from defining the last layer # in the model as a layer with output_n = k with softmax activation # where k is the number of classes. init_probs = self . forward ( X_batch ) # dividide by the number of data points in this specific batch to get the average loss. loss = self . loss ( y_batch , init_probs ) / len ( y_batch ) self . backward ( y_batch ) # add the latest loss to the history self . loss_history . append ( loss ) # predict with the current weights and biases on the whole data set batch_predict = self . predict ( self . X ) # calculate the accuracy score of the prediction train_accuracy = accuracy_score ( self . y , batch_predict ) # add accuracy to the history self . accuracy_history . append ( train_accuracy ) # update rich progress bar for each epoch pb . update ( t1 , advance = 1 ) if progress . finished : pb . update ( t1 , description = \"[bright_green]Training complete!\" )","title":"fit()"},{"location":"reference/mlproject/neural_net/_neural_net/#mlproject.neural_net._neural_net.NeuralNetworkClassifier.forward","text":"Compute a single forward pass of the network. Parameters: Name Type Description Default X 2d ndarray The data to use for the forward pass. Must be of size n x input_n where input_n must come from the first DenseLayer in the network. required Returns: Type Description 2d ndarray An n x output_n array where output_n corresponds to the output_n of the last DenseLayer in the network and n is the number of data points. Source code in mlproject/neural_net/_neural_net.py 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 def forward ( self , X ): \"\"\"Compute a single forward pass of the network. Parameters ---------- X : 2d ndarray The data to use for the forward pass. Must be of size n x input_n where input_n must come from the first [`DenseLayer`][mlproject.neural_net._dense_layer.DenseLayer] in the network. Returns ------- 2d ndarray An n x output_n array where output_n corresponds to the output_n of the last [`DenseLayer`][mlproject.neural_net._dense_layer.DenseLayer] in the network and n is the number of data points. \"\"\" self . activations . append ( X ) for layer in self . layers : X = layer . forward ( X ) self . activations . append ( X ) self . sums . append ( layer . z ) return X","title":"forward()"},{"location":"reference/mlproject/neural_net/_neural_net/#mlproject.neural_net._neural_net.NeuralNetworkClassifier.predict","text":"Predict class labels for the given data. Parameters: Name Type Description Default X 2d ndarray The data that we want to use to make predictions. required Returns: Type Description 1d ndarray All predicted class labels with size n, where n is the number of data points. Source code in mlproject/neural_net/_neural_net.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def predict ( self , X ): \"\"\"Predict class labels for the given data. Parameters ---------- X : 2d ndarray The data that we want to use to make predictions. Returns ------- 1d ndarray All predicted class labels with size n, where n is the number of data points. \"\"\" probabilities = self . predict_proba ( X ) return np . array ( [ self . label [ pred ] for pred in np . argmax ( probabilities , axis = 1 ) . astype ( int )] )","title":"predict()"},{"location":"reference/mlproject/neural_net/_neural_net/#mlproject.neural_net._neural_net.NeuralNetworkClassifier.predict_proba","text":"Predict class probabilities for the given data Parameters: Name Type Description Default X 2d ndarray The data that we want to use to make predictions required Returns: Type Description 2d ndarray All probabilites with size n x k, where n is the number of data points and k is the number classes Source code in mlproject/neural_net/_neural_net.py 146 147 148 149 150 151 152 153 154 155 156 157 158 159 def predict_proba ( self , X ): \"\"\"Predict class probabilities for the given data Parameters ---------- X : 2d ndarray The data that we want to use to make predictions Returns ------- 2d ndarray All probabilites with size n x k, where n is the number of data points and k is the number classes \"\"\" return self . forward ( X )","title":"predict_proba()"}]}