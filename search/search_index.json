{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the documentation for our machine learning project!","text":""},{"location":"#implementations","title":"Implementations","text":""},{"location":"#neural-network-classifier","title":"<code>Neural Network Classifier</code>","text":"<p>A neural network for use in classification problems.</p> <ul> <li> <p><code>DenseLayer</code> - A fully connected layer with user defined size and activation function</p> </li> <li> <p><code>Loss Functions</code> - The loss function to use in the neural network</p> <ul> <li>(currently only <code>categorical cross entropy</code> is supported).</li> </ul> </li> <li> <p><code>Activation Functions</code> - The activation functions to use in the fully connected layers</p> <ul> <li>(currently <code>stable_softmax</code> and <code>leaky_relu</code> is supported).</li> </ul> </li> </ul>"},{"location":"#decision-tree-classifier","title":"<code>Decision Tree Classifier</code>","text":"<p>A decision tree for use in classification problems.</p> <ul> <li> <p><code>Node</code> - A tree node used by the decision tree classifier, is either leaf or not.</p> </li> <li> <p><code>Impurtiy Criterion</code> - The impurity function to use when decision whether to split nodes in the decision tree </p> <ul> <li>(currently only  <code>gini_impurity</code> and  <code>entropy</code> are supported).</li> </ul> </li> </ul>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#normal-installation","title":"Normal installation","text":"<p>To start using the library simply run the following command from the root of the github repository directory</p> <pre><code>pip install .\n</code></pre>"},{"location":"installation/#development-installation","title":"Development installation","text":"<p>If you want to edit or add anything to the library you can install the library in editable mode, so you don't have to reinstall the library after each change:</p> <pre><code>pip install -e .\n</code></pre>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>mlproject<ul> <li>helpers<ul> <li>_data_loader</li> <li>_metrics</li> </ul> </li> <li>neural_net<ul> <li>_activations</li> <li>_loss</li> <li>_dense_layer</li> <li>_neural_net</li> </ul> </li> <li>decision_tree<ul> <li>_decision_tree</li> <li>_node</li> <li>_impurity</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/mlproject/decision_tree/","title":"decision_tree","text":""},{"location":"reference/mlproject/decision_tree/#mlproject.decision_tree.DecisionTreeClassifier","title":"<code>DecisionTreeClassifier</code>","text":"<p>Decision Tree Classifier</p> <p>Simple decision tree classifier with user specific impurity, max depth and minimum number of samples in leaf nodes.</p> <p>Parameters:</p>    Name Type Description Default     <code>criterion</code>  <code>str, optional</code>  <p>The impurity criterion to use when splitting nodes, by default 'gini'</p>  <code>'gini'</code>    <code>max_depth</code>  <code>int, optional</code>  <p>The maximum depth of the decision tree, by default 100</p>  <code>100</code>    <code>min_samples_in_leaf</code>  <code>int, optional</code>  <p>The minimum number of samples that need to be in a leaf, by default 2</p>  <code>2</code>      Source code in <code>mlproject/decision_tree/_decision_tree.py</code> <pre><code>class DecisionTreeClassifier:\n    \"\"\"Decision Tree Classifier\n\n    Simple decision tree classifier with user specific impurity, max depth and\n    minimum number of samples in leaf nodes.\n\n    Parameters\n    ----------\n    criterion : str, optional\n        The impurity criterion to use when splitting nodes, by default 'gini'\n    max_depth : int, optional\n        The maximum depth of the decision tree, by default 100\n    min_samples_in_leaf : int, optional\n        The minimum number of samples that need to be in a leaf, by default 2\n    \"\"\"\n\n    def __init__(self, criterion=\"gini\", max_depth=100, min_samples_in_leaf=2):\n\n        self.max_depth = max_depth\n        self.min_samples_in_leaf = min_samples_in_leaf\n        self.root = None\n\n        if criterion.lower() == \"gini\":\n            self.criterion = gini_impurity\n        elif criterion.lower() == \"entropy\":\n            self.criterion = entropy_impurity\n\n    def fit(self, X, y):\n        \"\"\"Fit the decision tree to the given data\n\n        Parameters\n        ----------\n        X : 2d ndarray\n            The data to be used for fitting the decision tree\n        y : 2d ndarray\n            An array of the true labels for the data points\n        \"\"\"\n\n        # for use in calculating the _most_common_label to get counts even if class not present in leaf\n        self.classes = np.unique(y)\n\n        with progress as pb:\n            t1 = pb.add_task(\"[blue]Training\", total=1)\n\n            self.root = self._grow(X, y)\n            pb.update(t1, advance=1)\n            if progress.finished:\n                pb.update(t1, description=\"[bright_green]Training complete!\")\n\n    def predict(self, X):\n        \"\"\"Predict class labels for the given data.\n\n        For all data points in the dataset traverse the decision tree until it reaches a leaf node\n        and return the majority class of that leaf node.\n\n        Parameters\n        ----------\n        X : 2d ndarray\n            The data that we want to use to make predictions.\n\n        Returns\n        -------\n        1d ndarray\n            All predicted class labels with size n, where n is the number of data points.\n        \"\"\"\n        return np.array(\n            [self._traverse(datapoint, self.root, prob=False) for datapoint in X]\n        )\n\n    def predict_proba(self, X):\n        \"\"\"Predict class probabilities for the given data\n\n        For all data points in the dataset traverse the decision tree until it reaches a leaf node\n        and return the class probabilities of that leaf node.\n\n        Parameters\n        ----------\n        X : 2d ndarray\n            The data that we want to use to make predictions\n\n        Returns\n        -------\n        2d ndarray\n            All probabilites with size n x k, where n is the number of data points and k is the number classes\n        \"\"\"\n        return np.array(\n            [self._traverse(datapoint, self.root, prob=True) for datapoint in X]\n        )\n\n    def _grow(self, X, y, cur_depth=0):\n        \"\"\"Grows a decision tree from the given data.\n        This is the part that is doing the actual fitting of the decision tree.\n\n        Parameters\n        ----------\n        X : 2d ndarray\n            The data to use when growing the decision tree\n        y : 2d ndarray\n            array of the true class labels\n        cur_depth : int, optional\n            The current depth of the decision tree, by default 0\n\n        Returns\n        -------\n        Node\n            A new node of class Node with new left and right children.\n        \"\"\"\n\n        self.n, self.p = X.shape\n        node_unique_classes = np.unique(y)\n        self.node_k = len(node_unique_classes)\n\n        if (\n            cur_depth &gt;= self.max_depth\n            or self.n &lt; self.min_samples_in_leaf\n            or self.node_k == 1\n        ):\n\n            most_common = self._most_common_label(y, prob=False)\n            class_probs = self._most_common_label(y, prob=True)\n            return Node(majority_class=most_common, class_probs=class_probs)\n\n        cur_depth += 1\n\n        best_feature, best_threshold = self._best_split(X, y)\n\n        left_idxs = np.argwhere(X[:, best_feature] &lt;= best_threshold).flatten()\n        right_idxs = np.argwhere(X[:, best_feature] &gt; best_threshold).flatten()\n\n        left = self._grow(X[left_idxs, :], y[left_idxs], cur_depth)\n        right = self._grow(X[right_idxs, :], y[right_idxs], cur_depth)\n\n        return Node(left, right, best_feature, best_threshold)\n\n    def _best_split(self, X, y):\n        \"\"\"Calculates the best split of a node with the given data points\n\n        Parameters\n        ----------\n        X : 2d ndarray\n            The data points to consider for splitting this node\n        y : 2d ndarray\n            The true labels to consider for splitting this node\n\n        Returns\n        -------\n        tuple\n            A tuple containing the best index and threshold for the split\n        \"\"\"\n        best_gain = -np.inf\n\n        for feat_idx in range(X.shape[1]):\n            feature_col = X[:, feat_idx]\n            possible_splits = np.unique(feature_col)\n\n            for split in possible_splits:\n                cur_gain = self._information_gain(y, feature_col, split)\n\n                if cur_gain &gt; best_gain:\n                    best_gain = cur_gain\n                    split_idx = feat_idx\n                    split_thresh = split\n\n        return split_idx, split_thresh\n\n    def _information_gain(self, y, feature_col, split_thresh):\n        \"\"\"Calculates the information gain of a node with the given data labels\n\n        Parameters\n        ----------\n        y : 2d ndarray\n            array of true labels for this node\n        feature_col : 2d ndarray\n            Column of dataset containing the data points of the best feature for this split\n        split_thresh : float or int\n            the threshold for the best split of the data\n\n        Returns\n        -------\n        float\n            The information gain from this node compared to it's parent\n        \"\"\"\n\n        parent_impurity = self.criterion(y)\n\n        left_idxs = np.argwhere(feature_col &lt;= split_thresh).flatten()\n        right_idxs = np.argwhere(feature_col &gt; split_thresh).flatten()\n\n        if len(left_idxs) == 0 or len(right_idxs) == 0:\n            return 0\n\n        n = len(y)\n        left_prob = len(left_idxs) / n\n        right_prob = len(right_idxs) / n\n\n        left_impurity = self.criterion(y[left_idxs])\n        right_impurity = self.criterion(y[right_idxs])\n\n        weighted_impurity = left_prob * left_impurity + right_prob * right_impurity\n\n        information_gain = parent_impurity - weighted_impurity\n        return information_gain\n\n    def _traverse(self, X, node, prob=False):\n        \"\"\"Traverses the tree until it reaches a leaf node and returns either the majority\n        class of that node or the class probabilities if prob is True.\n\n        Parameters\n        ----------\n        X : 2d ndarray\n            The data points to use for traversing the tree\n        node : Node\n            The node to start the traversal from.\n        prob : bool, optional\n            used to specify whether or not to return class probabilities, by default False\n\n        Returns\n        -------\n        int or bool\n            Depending on `prob` it either returns the majority class or the class probabilities\n        \"\"\"\n\n        if node.is_leaf():\n            if prob:\n                return node.class_probs\n            return node.majority_class\n\n        if X[node.feature] &lt;= node.threshold:\n            return self._traverse(X, node.left, prob)\n\n        elif X[node.feature] &gt; node.threshold:\n            return self._traverse(X, node.right, prob)\n\n    def _most_common_label(self, y, prob=False):\n        \"\"\"Calculates the most common label of a leaf node or the class probabilities\n\n        Parameters\n        ----------\n        y : 2d ndarray\n            Array of true labels for this particular node\n        prob : bool, optional\n            used to specify whether or not to return class probabilities, by default False\n\n        Returns\n        -------\n        int or bool\n            Depending on `prob` it either returns the majority class or the class probabilities\n        \"\"\"\n        # flatten y because np.bincount() expects a 1d array\n        y = y.flatten()\n        counts = np.bincount(y, minlength=len(self.classes))\n        n = np.sum(counts)\n\n        if prob:\n            return counts / n\n\n        return np.argmax(counts)\n</code></pre>"},{"location":"reference/mlproject/decision_tree/#mlproject.decision_tree._decision_tree.DecisionTreeClassifier.fit","title":"<code>fit(X, y)</code>","text":"<p>Fit the decision tree to the given data</p> <p>Parameters:</p>    Name Type Description Default     <code>X</code>  <code>2d ndarray</code>  <p>The data to be used for fitting the decision tree</p>  required    <code>y</code>  <code>2d ndarray</code>  <p>An array of the true labels for the data points</p>  required      Source code in <code>mlproject/decision_tree/_decision_tree.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the decision tree to the given data\n\n    Parameters\n    ----------\n    X : 2d ndarray\n        The data to be used for fitting the decision tree\n    y : 2d ndarray\n        An array of the true labels for the data points\n    \"\"\"\n\n    # for use in calculating the _most_common_label to get counts even if class not present in leaf\n    self.classes = np.unique(y)\n\n    with progress as pb:\n        t1 = pb.add_task(\"[blue]Training\", total=1)\n\n        self.root = self._grow(X, y)\n        pb.update(t1, advance=1)\n        if progress.finished:\n            pb.update(t1, description=\"[bright_green]Training complete!\")\n</code></pre>"},{"location":"reference/mlproject/decision_tree/#mlproject.decision_tree._decision_tree.DecisionTreeClassifier.predict","title":"<code>predict(X)</code>","text":"<p>Predict class labels for the given data.</p> <p>For all data points in the dataset traverse the decision tree until it reaches a leaf node and return the majority class of that leaf node.</p> <p>Parameters:</p>    Name Type Description Default     <code>X</code>  <code>2d ndarray</code>  <p>The data that we want to use to make predictions.</p>  required     <p>Returns:</p>    Type Description      <code>1d ndarray</code>  <p>All predicted class labels with size n, where n is the number of data points.</p>     Source code in <code>mlproject/decision_tree/_decision_tree.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict class labels for the given data.\n\n    For all data points in the dataset traverse the decision tree until it reaches a leaf node\n    and return the majority class of that leaf node.\n\n    Parameters\n    ----------\n    X : 2d ndarray\n        The data that we want to use to make predictions.\n\n    Returns\n    -------\n    1d ndarray\n        All predicted class labels with size n, where n is the number of data points.\n    \"\"\"\n    return np.array(\n        [self._traverse(datapoint, self.root, prob=False) for datapoint in X]\n    )\n</code></pre>"},{"location":"reference/mlproject/decision_tree/#mlproject.decision_tree._decision_tree.DecisionTreeClassifier.predict_proba","title":"<code>predict_proba(X)</code>","text":"<p>Predict class probabilities for the given data</p> <p>For all data points in the dataset traverse the decision tree until it reaches a leaf node and return the class probabilities of that leaf node.</p> <p>Parameters:</p>    Name Type Description Default     <code>X</code>  <code>2d ndarray</code>  <p>The data that we want to use to make predictions</p>  required     <p>Returns:</p>    Type Description      <code>2d ndarray</code>  <p>All probabilites with size n x k, where n is the number of data points and k is the number classes</p>     Source code in <code>mlproject/decision_tree/_decision_tree.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict class probabilities for the given data\n\n    For all data points in the dataset traverse the decision tree until it reaches a leaf node\n    and return the class probabilities of that leaf node.\n\n    Parameters\n    ----------\n    X : 2d ndarray\n        The data that we want to use to make predictions\n\n    Returns\n    -------\n    2d ndarray\n        All probabilites with size n x k, where n is the number of data points and k is the number classes\n    \"\"\"\n    return np.array(\n        [self._traverse(datapoint, self.root, prob=True) for datapoint in X]\n    )\n</code></pre>"},{"location":"reference/mlproject/decision_tree/#mlproject.decision_tree.Node","title":"<code>Node</code>","text":"<p>Node object for building a decision tree.</p> <p>Parameters:</p>    Name Type Description Default     <code>feature</code>  <code>int index, optional</code>  <p>index of the best feature for splitting this Node, by default None</p>  <code>None</code>    <code>threshold</code>  <code>float, optional</code>  <p>the threshold for the best split of the data, by default None</p>  <code>None</code>    <code>left</code>  <code>Node, optional</code>  <p>the left child of this Node also of class Node, by default None</p>  <code>None</code>    <code>right</code>  <code>Node, optional</code>  <p>the right child of this Node also of class Node, by default None</p>  <code>None</code>    <code>majority_class</code>  <code>int, optional</code>  <p>The majority class in this node, only if this Node is a leaf, by default None</p>  <code>None</code>    <code>class_probs</code>  <code>1d ndarray, optional</code>  <p>An array of class probabilities for this node, only if this Node is a leaf, by default None</p>  <code>None</code>      Source code in <code>mlproject/decision_tree/_node.py</code> <pre><code>class Node:\n    \"\"\"Node object for building a decision tree.\n\n    Parameters\n    ----------\n    feature : int index, optional\n        index of the best feature for splitting this Node, by default None\n    threshold : float, optional\n        the threshold for the best split of the data, by default None\n    left : Node, optional\n        the left child of this Node also of class Node, by default None\n    right : Node, optional\n        the right child of this Node also of class Node, by default None\n    majority_class : int, optional\n        The majority class in this node, only if this Node is a leaf, by default None\n    class_probs : 1d ndarray, optional\n        An array of class probabilities for this node, only if this Node is a leaf, by default None\n    \"\"\"\n\n    def __init__(\n        self,\n        left=None,\n        right=None,\n        feature=None,\n        threshold=None,\n        *,\n        majority_class=None,\n        class_probs=None\n    ):\n        self.feature = feature\n        self.threshold = threshold\n        self.left, self.right = left, right\n        self.majority_class = majority_class\n        self.class_probs = class_probs\n\n    def is_leaf(self):\n        \"\"\"Returns True if this Node is a leaf node, otherwise False\n\n        Returns\n        -------\n        bool\n            True if this Node is a leaf node, otherwise False\n        \"\"\"\n\n        return self.majority_class is not None\n</code></pre>"},{"location":"reference/mlproject/decision_tree/#mlproject.decision_tree._node.Node.is_leaf","title":"<code>is_leaf()</code>","text":"<p>Returns True if this Node is a leaf node, otherwise False</p> <p>Returns:</p>    Type Description      <code>bool</code>  <p>True if this Node is a leaf node, otherwise False</p>     Source code in <code>mlproject/decision_tree/_node.py</code> <pre><code>def is_leaf(self):\n    \"\"\"Returns True if this Node is a leaf node, otherwise False\n\n    Returns\n    -------\n    bool\n        True if this Node is a leaf node, otherwise False\n    \"\"\"\n\n    return self.majority_class is not None\n</code></pre>"},{"location":"reference/mlproject/decision_tree/_decision_tree/","title":"_decision_tree","text":""},{"location":"reference/mlproject/decision_tree/_decision_tree/#mlproject.decision_tree._decision_tree.DecisionTreeClassifier","title":"<code>DecisionTreeClassifier</code>","text":"<p>Decision Tree Classifier</p> <p>Simple decision tree classifier with user specific impurity, max depth and minimum number of samples in leaf nodes.</p> <p>Parameters:</p>    Name Type Description Default     <code>criterion</code>  <code>str, optional</code>  <p>The impurity criterion to use when splitting nodes, by default 'gini'</p>  <code>'gini'</code>    <code>max_depth</code>  <code>int, optional</code>  <p>The maximum depth of the decision tree, by default 100</p>  <code>100</code>    <code>min_samples_in_leaf</code>  <code>int, optional</code>  <p>The minimum number of samples that need to be in a leaf, by default 2</p>  <code>2</code>      Source code in <code>mlproject/decision_tree/_decision_tree.py</code> <pre><code>class DecisionTreeClassifier:\n    \"\"\"Decision Tree Classifier\n\n    Simple decision tree classifier with user specific impurity, max depth and\n    minimum number of samples in leaf nodes.\n\n    Parameters\n    ----------\n    criterion : str, optional\n        The impurity criterion to use when splitting nodes, by default 'gini'\n    max_depth : int, optional\n        The maximum depth of the decision tree, by default 100\n    min_samples_in_leaf : int, optional\n        The minimum number of samples that need to be in a leaf, by default 2\n    \"\"\"\n\n    def __init__(self, criterion=\"gini\", max_depth=100, min_samples_in_leaf=2):\n\n        self.max_depth = max_depth\n        self.min_samples_in_leaf = min_samples_in_leaf\n        self.root = None\n\n        if criterion.lower() == \"gini\":\n            self.criterion = gini_impurity\n        elif criterion.lower() == \"entropy\":\n            self.criterion = entropy_impurity\n\n    def fit(self, X, y):\n        \"\"\"Fit the decision tree to the given data\n\n        Parameters\n        ----------\n        X : 2d ndarray\n            The data to be used for fitting the decision tree\n        y : 2d ndarray\n            An array of the true labels for the data points\n        \"\"\"\n\n        # for use in calculating the _most_common_label to get counts even if class not present in leaf\n        self.classes = np.unique(y)\n\n        with progress as pb:\n            t1 = pb.add_task(\"[blue]Training\", total=1)\n\n            self.root = self._grow(X, y)\n            pb.update(t1, advance=1)\n            if progress.finished:\n                pb.update(t1, description=\"[bright_green]Training complete!\")\n\n    def predict(self, X):\n        \"\"\"Predict class labels for the given data.\n\n        For all data points in the dataset traverse the decision tree until it reaches a leaf node\n        and return the majority class of that leaf node.\n\n        Parameters\n        ----------\n        X : 2d ndarray\n            The data that we want to use to make predictions.\n\n        Returns\n        -------\n        1d ndarray\n            All predicted class labels with size n, where n is the number of data points.\n        \"\"\"\n        return np.array(\n            [self._traverse(datapoint, self.root, prob=False) for datapoint in X]\n        )\n\n    def predict_proba(self, X):\n        \"\"\"Predict class probabilities for the given data\n\n        For all data points in the dataset traverse the decision tree until it reaches a leaf node\n        and return the class probabilities of that leaf node.\n\n        Parameters\n        ----------\n        X : 2d ndarray\n            The data that we want to use to make predictions\n\n        Returns\n        -------\n        2d ndarray\n            All probabilites with size n x k, where n is the number of data points and k is the number classes\n        \"\"\"\n        return np.array(\n            [self._traverse(datapoint, self.root, prob=True) for datapoint in X]\n        )\n\n    def _grow(self, X, y, cur_depth=0):\n        \"\"\"Grows a decision tree from the given data.\n        This is the part that is doing the actual fitting of the decision tree.\n\n        Parameters\n        ----------\n        X : 2d ndarray\n            The data to use when growing the decision tree\n        y : 2d ndarray\n            array of the true class labels\n        cur_depth : int, optional\n            The current depth of the decision tree, by default 0\n\n        Returns\n        -------\n        Node\n            A new node of class Node with new left and right children.\n        \"\"\"\n\n        self.n, self.p = X.shape\n        node_unique_classes = np.unique(y)\n        self.node_k = len(node_unique_classes)\n\n        if (\n            cur_depth &gt;= self.max_depth\n            or self.n &lt; self.min_samples_in_leaf\n            or self.node_k == 1\n        ):\n\n            most_common = self._most_common_label(y, prob=False)\n            class_probs = self._most_common_label(y, prob=True)\n            return Node(majority_class=most_common, class_probs=class_probs)\n\n        cur_depth += 1\n\n        best_feature, best_threshold = self._best_split(X, y)\n\n        left_idxs = np.argwhere(X[:, best_feature] &lt;= best_threshold).flatten()\n        right_idxs = np.argwhere(X[:, best_feature] &gt; best_threshold).flatten()\n\n        left = self._grow(X[left_idxs, :], y[left_idxs], cur_depth)\n        right = self._grow(X[right_idxs, :], y[right_idxs], cur_depth)\n\n        return Node(left, right, best_feature, best_threshold)\n\n    def _best_split(self, X, y):\n        \"\"\"Calculates the best split of a node with the given data points\n\n        Parameters\n        ----------\n        X : 2d ndarray\n            The data points to consider for splitting this node\n        y : 2d ndarray\n            The true labels to consider for splitting this node\n\n        Returns\n        -------\n        tuple\n            A tuple containing the best index and threshold for the split\n        \"\"\"\n        best_gain = -np.inf\n\n        for feat_idx in range(X.shape[1]):\n            feature_col = X[:, feat_idx]\n            possible_splits = np.unique(feature_col)\n\n            for split in possible_splits:\n                cur_gain = self._information_gain(y, feature_col, split)\n\n                if cur_gain &gt; best_gain:\n                    best_gain = cur_gain\n                    split_idx = feat_idx\n                    split_thresh = split\n\n        return split_idx, split_thresh\n\n    def _information_gain(self, y, feature_col, split_thresh):\n        \"\"\"Calculates the information gain of a node with the given data labels\n\n        Parameters\n        ----------\n        y : 2d ndarray\n            array of true labels for this node\n        feature_col : 2d ndarray\n            Column of dataset containing the data points of the best feature for this split\n        split_thresh : float or int\n            the threshold for the best split of the data\n\n        Returns\n        -------\n        float\n            The information gain from this node compared to it's parent\n        \"\"\"\n\n        parent_impurity = self.criterion(y)\n\n        left_idxs = np.argwhere(feature_col &lt;= split_thresh).flatten()\n        right_idxs = np.argwhere(feature_col &gt; split_thresh).flatten()\n\n        if len(left_idxs) == 0 or len(right_idxs) == 0:\n            return 0\n\n        n = len(y)\n        left_prob = len(left_idxs) / n\n        right_prob = len(right_idxs) / n\n\n        left_impurity = self.criterion(y[left_idxs])\n        right_impurity = self.criterion(y[right_idxs])\n\n        weighted_impurity = left_prob * left_impurity + right_prob * right_impurity\n\n        information_gain = parent_impurity - weighted_impurity\n        return information_gain\n\n    def _traverse(self, X, node, prob=False):\n        \"\"\"Traverses the tree until it reaches a leaf node and returns either the majority\n        class of that node or the class probabilities if prob is True.\n\n        Parameters\n        ----------\n        X : 2d ndarray\n            The data points to use for traversing the tree\n        node : Node\n            The node to start the traversal from.\n        prob : bool, optional\n            used to specify whether or not to return class probabilities, by default False\n\n        Returns\n        -------\n        int or bool\n            Depending on `prob` it either returns the majority class or the class probabilities\n        \"\"\"\n\n        if node.is_leaf():\n            if prob:\n                return node.class_probs\n            return node.majority_class\n\n        if X[node.feature] &lt;= node.threshold:\n            return self._traverse(X, node.left, prob)\n\n        elif X[node.feature] &gt; node.threshold:\n            return self._traverse(X, node.right, prob)\n\n    def _most_common_label(self, y, prob=False):\n        \"\"\"Calculates the most common label of a leaf node or the class probabilities\n\n        Parameters\n        ----------\n        y : 2d ndarray\n            Array of true labels for this particular node\n        prob : bool, optional\n            used to specify whether or not to return class probabilities, by default False\n\n        Returns\n        -------\n        int or bool\n            Depending on `prob` it either returns the majority class or the class probabilities\n        \"\"\"\n        # flatten y because np.bincount() expects a 1d array\n        y = y.flatten()\n        counts = np.bincount(y, minlength=len(self.classes))\n        n = np.sum(counts)\n\n        if prob:\n            return counts / n\n\n        return np.argmax(counts)\n</code></pre>"},{"location":"reference/mlproject/decision_tree/_decision_tree/#mlproject.decision_tree._decision_tree.DecisionTreeClassifier.fit","title":"<code>fit(X, y)</code>","text":"<p>Fit the decision tree to the given data</p> <p>Parameters:</p>    Name Type Description Default     <code>X</code>  <code>2d ndarray</code>  <p>The data to be used for fitting the decision tree</p>  required    <code>y</code>  <code>2d ndarray</code>  <p>An array of the true labels for the data points</p>  required      Source code in <code>mlproject/decision_tree/_decision_tree.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the decision tree to the given data\n\n    Parameters\n    ----------\n    X : 2d ndarray\n        The data to be used for fitting the decision tree\n    y : 2d ndarray\n        An array of the true labels for the data points\n    \"\"\"\n\n    # for use in calculating the _most_common_label to get counts even if class not present in leaf\n    self.classes = np.unique(y)\n\n    with progress as pb:\n        t1 = pb.add_task(\"[blue]Training\", total=1)\n\n        self.root = self._grow(X, y)\n        pb.update(t1, advance=1)\n        if progress.finished:\n            pb.update(t1, description=\"[bright_green]Training complete!\")\n</code></pre>"},{"location":"reference/mlproject/decision_tree/_decision_tree/#mlproject.decision_tree._decision_tree.DecisionTreeClassifier.predict","title":"<code>predict(X)</code>","text":"<p>Predict class labels for the given data.</p> <p>For all data points in the dataset traverse the decision tree until it reaches a leaf node and return the majority class of that leaf node.</p> <p>Parameters:</p>    Name Type Description Default     <code>X</code>  <code>2d ndarray</code>  <p>The data that we want to use to make predictions.</p>  required     <p>Returns:</p>    Type Description      <code>1d ndarray</code>  <p>All predicted class labels with size n, where n is the number of data points.</p>     Source code in <code>mlproject/decision_tree/_decision_tree.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict class labels for the given data.\n\n    For all data points in the dataset traverse the decision tree until it reaches a leaf node\n    and return the majority class of that leaf node.\n\n    Parameters\n    ----------\n    X : 2d ndarray\n        The data that we want to use to make predictions.\n\n    Returns\n    -------\n    1d ndarray\n        All predicted class labels with size n, where n is the number of data points.\n    \"\"\"\n    return np.array(\n        [self._traverse(datapoint, self.root, prob=False) for datapoint in X]\n    )\n</code></pre>"},{"location":"reference/mlproject/decision_tree/_decision_tree/#mlproject.decision_tree._decision_tree.DecisionTreeClassifier.predict_proba","title":"<code>predict_proba(X)</code>","text":"<p>Predict class probabilities for the given data</p> <p>For all data points in the dataset traverse the decision tree until it reaches a leaf node and return the class probabilities of that leaf node.</p> <p>Parameters:</p>    Name Type Description Default     <code>X</code>  <code>2d ndarray</code>  <p>The data that we want to use to make predictions</p>  required     <p>Returns:</p>    Type Description      <code>2d ndarray</code>  <p>All probabilites with size n x k, where n is the number of data points and k is the number classes</p>     Source code in <code>mlproject/decision_tree/_decision_tree.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict class probabilities for the given data\n\n    For all data points in the dataset traverse the decision tree until it reaches a leaf node\n    and return the class probabilities of that leaf node.\n\n    Parameters\n    ----------\n    X : 2d ndarray\n        The data that we want to use to make predictions\n\n    Returns\n    -------\n    2d ndarray\n        All probabilites with size n x k, where n is the number of data points and k is the number classes\n    \"\"\"\n    return np.array(\n        [self._traverse(datapoint, self.root, prob=True) for datapoint in X]\n    )\n</code></pre>"},{"location":"reference/mlproject/decision_tree/_impurity/","title":"_impurity","text":""},{"location":"reference/mlproject/decision_tree/_impurity/#mlproject.decision_tree._impurity.entropy_impurity","title":"<code>entropy_impurity(y)</code>","text":"<p>Calculates the entropy of a given node</p> <p>Parameters:</p>    Name Type Description Default     <code>y</code>  <code>2d ndarray</code>  <p>array of y labels</p>  required     <p>Returns:</p>    Type Description      <code>float</code>  <p>entropy impurity of a given node</p>     Source code in <code>mlproject/decision_tree/_impurity.py</code> <pre><code>def entropy_impurity(y):\n    \"\"\"Calculates the entropy of a given node\n\n    Parameters\n    ----------\n    y : 2d ndarray\n        array of y labels\n\n    Returns\n    -------\n    float\n        entropy impurity of a given node\n    \"\"\"\n    epsilon = 1e-07\n    # flatten the array only because np.bincount expects a 1 dimensional array\n    y = y.flatten()\n    counts = np.bincount(y)\n    N = np.sum(counts)\n    p = counts / N\n    return np.sum(-p * np.log2(p + epsilon))\n</code></pre>"},{"location":"reference/mlproject/decision_tree/_impurity/#mlproject.decision_tree._impurity.gini_impurity","title":"<code>gini_impurity(y)</code>","text":"<p>Calculates the gini impurity of a given node</p> <p>Parameters:</p>    Name Type Description Default     <code>y</code>  <code>2d ndarray</code>  <p>array of y labels</p>  required     <p>Returns:</p>    Type Description      <code>float</code>  <p>gini impurity score for the node</p>     Source code in <code>mlproject/decision_tree/_impurity.py</code> <pre><code>def gini_impurity(y):\n    \"\"\"Calculates the gini impurity of a given node\n\n    Parameters\n    ----------\n    y : 2d ndarray\n        array of y labels\n\n    Returns\n    -------\n    float\n        gini impurity score for the node\n    \"\"\"\n    # flatten the array only because np.bincount expects a 1 dimensional array\n    y = y.flatten()\n    counts = np.bincount(y)\n    N = np.sum(counts)\n    p = counts / N\n    return 1 - np.sum(p**2)\n</code></pre>"},{"location":"reference/mlproject/decision_tree/_node/","title":"_node","text":""},{"location":"reference/mlproject/decision_tree/_node/#mlproject.decision_tree._node.Node","title":"<code>Node</code>","text":"<p>Node object for building a decision tree.</p> <p>Parameters:</p>    Name Type Description Default     <code>feature</code>  <code>int index, optional</code>  <p>index of the best feature for splitting this Node, by default None</p>  <code>None</code>    <code>threshold</code>  <code>float, optional</code>  <p>the threshold for the best split of the data, by default None</p>  <code>None</code>    <code>left</code>  <code>Node, optional</code>  <p>the left child of this Node also of class Node, by default None</p>  <code>None</code>    <code>right</code>  <code>Node, optional</code>  <p>the right child of this Node also of class Node, by default None</p>  <code>None</code>    <code>majority_class</code>  <code>int, optional</code>  <p>The majority class in this node, only if this Node is a leaf, by default None</p>  <code>None</code>    <code>class_probs</code>  <code>1d ndarray, optional</code>  <p>An array of class probabilities for this node, only if this Node is a leaf, by default None</p>  <code>None</code>      Source code in <code>mlproject/decision_tree/_node.py</code> <pre><code>class Node:\n    \"\"\"Node object for building a decision tree.\n\n    Parameters\n    ----------\n    feature : int index, optional\n        index of the best feature for splitting this Node, by default None\n    threshold : float, optional\n        the threshold for the best split of the data, by default None\n    left : Node, optional\n        the left child of this Node also of class Node, by default None\n    right : Node, optional\n        the right child of this Node also of class Node, by default None\n    majority_class : int, optional\n        The majority class in this node, only if this Node is a leaf, by default None\n    class_probs : 1d ndarray, optional\n        An array of class probabilities for this node, only if this Node is a leaf, by default None\n    \"\"\"\n\n    def __init__(\n        self,\n        left=None,\n        right=None,\n        feature=None,\n        threshold=None,\n        *,\n        majority_class=None,\n        class_probs=None\n    ):\n        self.feature = feature\n        self.threshold = threshold\n        self.left, self.right = left, right\n        self.majority_class = majority_class\n        self.class_probs = class_probs\n\n    def is_leaf(self):\n        \"\"\"Returns True if this Node is a leaf node, otherwise False\n\n        Returns\n        -------\n        bool\n            True if this Node is a leaf node, otherwise False\n        \"\"\"\n\n        return self.majority_class is not None\n</code></pre>"},{"location":"reference/mlproject/decision_tree/_node/#mlproject.decision_tree._node.Node.is_leaf","title":"<code>is_leaf()</code>","text":"<p>Returns True if this Node is a leaf node, otherwise False</p> <p>Returns:</p>    Type Description      <code>bool</code>  <p>True if this Node is a leaf node, otherwise False</p>     Source code in <code>mlproject/decision_tree/_node.py</code> <pre><code>def is_leaf(self):\n    \"\"\"Returns True if this Node is a leaf node, otherwise False\n\n    Returns\n    -------\n    bool\n        True if this Node is a leaf node, otherwise False\n    \"\"\"\n\n    return self.majority_class is not None\n</code></pre>"},{"location":"reference/mlproject/helpers/","title":"helpers","text":""},{"location":"reference/mlproject/helpers/#mlproject.helpers.accuracy_score","title":"<code>accuracy_score(y_true, y_pred, normalize=True)</code>","text":"<p>Calculate the accuracy score from a given array of true labels and a given array of predicted labels.</p> <p>Inspired by https://stackoverflow.com/a/64680660</p> <p>Parameters:</p>    Name Type Description Default     <code>y_true</code>  <code>2d ndarray</code>  <p>array of shape (n_samples, 1) of true labels</p>  required    <code>y_pred</code>  <code>2d ndarray</code>  <p>array of shape (n_samples, 1) of predicted labels</p>  required     <p>Returns:</p>    Name Type Description     <code>accuracy_scores</code>  <code>float</code>  <p>calculated accuracy score</p>    <p>Raises:</p>    Type Description      <code>ValueError</code>  <p>if y_true and y_pred are not of the same shape</p>     Source code in <code>mlproject/helpers/_metrics.py</code> <pre><code>def accuracy_score(y_true, y_pred, normalize=True):\n    \"\"\"Calculate the accuracy score from a given array of true labels\n    and a given array of predicted labels.\n\n    Inspired by [https://stackoverflow.com/a/64680660](https://stackoverflow.com/a/64680660)\n\n    Parameters\n    ----------\n    y_true : 2d ndarray\n        array of shape (n_samples, 1) of true labels\n    y_pred : 2d ndarray\n        array of shape (n_samples, 1) of predicted labels\n\n    Returns\n    -------\n    accuracy_scores : float\n        calculated accuracy score\n\n    Raises\n    ------\n    ValueError\n        if y_true and y_pred are not of the same shape\n    \"\"\"\n\n    if y_true.shape[0] != y_pred.shape[0] and y_true.shape[1] != y_pred.shape[1]:\n        raise ValueError(\n            f\"Length of y_true: ({len(y_true)}) and y_pred: ({len(y_pred)}) should be the same!\"\n        )\n\n    accuracy = []\n    for i in range(len(y_pred)):\n        if y_pred[i] == y_true[i]:\n            accuracy.append(1)\n        else:\n            accuracy.append(0)\n    if normalize == True:\n        return np.mean(accuracy)\n    if normalize == False:\n        return sum(accuracy)\n</code></pre>"},{"location":"reference/mlproject/helpers/#mlproject.helpers.data_loader","title":"<code>data_loader(raw=True, scaled=False, pca=False)</code>","text":"<p>Loads the fashion_mnist training and test data from the data directory.</p> <p>The function returns four numpy arrays containing the training and test data respectively.</p> <p>If specified it can also return the standard scaled version of the data or the first 10 principal components of the data.</p> <p>The different dimensions of the returned data is below:</p>     Raw Scaled PCA     Training      \\(X\\) \\((10.000 \\times 784)\\) \\((10.000 \\times 784)\\) \\((10.000 \\times 10)\\)   \\(Y\\) \\((10.000 \\times 1)\\) \\((10.000 \\times 1)\\) \\((10.000 \\times 1)\\)   Test      \\(X\\) \\((5.000 \\times 784)\\) \\((5.000 \\times 784)\\) \\((5.000 \\times 10)\\)   \\(Y\\) \\((5.000 \\times 1)\\) \\((5.000 \\times 1)\\) \\((5.000 \\times 1)\\)    <p>Returns:</p>    Type Description      <code>2d ndarrays</code>  <p>numpy data arrays in the order X_train, X_test, y_train, y_test.</p>     Source code in <code>mlproject/helpers/_data_loader.py</code> <pre><code>def data_loader(raw=True, scaled=False, pca=False):\n    r\"\"\"Loads the fashion_mnist training and test data from the data directory.\n\n    The function returns four numpy arrays containing the training and test data\n    respectively.\n\n    If specified it can also return the standard scaled version of the data or\n    the first 10 principal components of the data.\n\n    The different dimensions of the returned data is below:\n\n    |              |          Raw          |         Scaled        |          PCA         |\n    |:------------:|:---------------------:|:---------------------:|:--------------------:|\n    | **Training** |                       |                       |                      |\n    |      $X$     | $(10.000 \\times 784)$ | $(10.000 \\times 784)$ | $(10.000 \\times 10)$ |\n    |      $Y$     |  $(10.000 \\times 1)$  |  $(10.000 \\times 1)$  |  $(10.000 \\times 1)$ |\n    |   **Test**   |                       |                       |                      |\n    |      $X$     |  $(5.000 \\times 784)$ |  $(5.000 \\times 784)$ |  $(5.000 \\times 10)$ |\n    |      $Y$     |   $(5.000 \\times 1)$  |   $(5.000 \\times 1)$  |  $(5.000 \\times 1)$  |\n\n    Returns\n    -------\n    2d ndarrays\n        numpy data arrays in the order X_train, X_test, y_train, y_test.\n    \"\"\"\n    if raw and not scaled and not pca:\n        X_train, y_train = np.hsplit(\n            np.load(f\"{ROOT_DIR}/data/fashion_train.npy\"), [-1]\n        )\n        X_test, y_test = np.hsplit(np.load(f\"{ROOT_DIR}/data/fashion_test.npy\"), [-1])\n\n    elif scaled and not raw and not pca:\n        X_train, y_train = np.hsplit(\n            np.load(f\"{ROOT_DIR}/data/fashion_train_scaled.npy\"), [-1]\n        )\n        X_test, y_test = np.hsplit(\n            np.load(f\"{ROOT_DIR}/data/fashion_test_scaled.npy\"), [-1]\n        )\n        # converting the y_labels back to integers from floats to avoid issues\n        y_train, y_test = y_train.astype(int), y_test.astype(int)\n\n    elif pca and not raw and not scaled:\n        X_train, y_train = np.hsplit(\n            np.load(f\"{ROOT_DIR}/data/fashion_train_pca.npy\"), [-1]\n        )\n        X_test, y_test = np.hsplit(\n            np.load(f\"{ROOT_DIR}/data/fashion_test_pca.npy\"), [-1]\n        )\n        # converting the y_labels back to integers from floats to avoid issues\n        y_train, y_test = y_train.astype(int), y_test.astype(int)\n    else:\n        raise ValueError(\"If raw, scaled or pca is True, then all other arguments must be False.\")\n\n    return X_train, X_test, y_train, y_test\n</code></pre>"},{"location":"reference/mlproject/helpers/_data_loader/","title":"_data_loader","text":""},{"location":"reference/mlproject/helpers/_data_loader/#mlproject.helpers._data_loader.data_loader","title":"<code>data_loader(raw=True, scaled=False, pca=False)</code>","text":"<p>Loads the fashion_mnist training and test data from the data directory.</p> <p>The function returns four numpy arrays containing the training and test data respectively.</p> <p>If specified it can also return the standard scaled version of the data or the first 10 principal components of the data.</p> <p>The different dimensions of the returned data is below:</p>     Raw Scaled PCA     Training      \\(X\\) \\((10.000 \\times 784)\\) \\((10.000 \\times 784)\\) \\((10.000 \\times 10)\\)   \\(Y\\) \\((10.000 \\times 1)\\) \\((10.000 \\times 1)\\) \\((10.000 \\times 1)\\)   Test      \\(X\\) \\((5.000 \\times 784)\\) \\((5.000 \\times 784)\\) \\((5.000 \\times 10)\\)   \\(Y\\) \\((5.000 \\times 1)\\) \\((5.000 \\times 1)\\) \\((5.000 \\times 1)\\)    <p>Returns:</p>    Type Description      <code>2d ndarrays</code>  <p>numpy data arrays in the order X_train, X_test, y_train, y_test.</p>     Source code in <code>mlproject/helpers/_data_loader.py</code> <pre><code>def data_loader(raw=True, scaled=False, pca=False):\n    r\"\"\"Loads the fashion_mnist training and test data from the data directory.\n\n    The function returns four numpy arrays containing the training and test data\n    respectively.\n\n    If specified it can also return the standard scaled version of the data or\n    the first 10 principal components of the data.\n\n    The different dimensions of the returned data is below:\n\n    |              |          Raw          |         Scaled        |          PCA         |\n    |:------------:|:---------------------:|:---------------------:|:--------------------:|\n    | **Training** |                       |                       |                      |\n    |      $X$     | $(10.000 \\times 784)$ | $(10.000 \\times 784)$ | $(10.000 \\times 10)$ |\n    |      $Y$     |  $(10.000 \\times 1)$  |  $(10.000 \\times 1)$  |  $(10.000 \\times 1)$ |\n    |   **Test**   |                       |                       |                      |\n    |      $X$     |  $(5.000 \\times 784)$ |  $(5.000 \\times 784)$ |  $(5.000 \\times 10)$ |\n    |      $Y$     |   $(5.000 \\times 1)$  |   $(5.000 \\times 1)$  |  $(5.000 \\times 1)$  |\n\n    Returns\n    -------\n    2d ndarrays\n        numpy data arrays in the order X_train, X_test, y_train, y_test.\n    \"\"\"\n    if raw and not scaled and not pca:\n        X_train, y_train = np.hsplit(\n            np.load(f\"{ROOT_DIR}/data/fashion_train.npy\"), [-1]\n        )\n        X_test, y_test = np.hsplit(np.load(f\"{ROOT_DIR}/data/fashion_test.npy\"), [-1])\n\n    elif scaled and not raw and not pca:\n        X_train, y_train = np.hsplit(\n            np.load(f\"{ROOT_DIR}/data/fashion_train_scaled.npy\"), [-1]\n        )\n        X_test, y_test = np.hsplit(\n            np.load(f\"{ROOT_DIR}/data/fashion_test_scaled.npy\"), [-1]\n        )\n        # converting the y_labels back to integers from floats to avoid issues\n        y_train, y_test = y_train.astype(int), y_test.astype(int)\n\n    elif pca and not raw and not scaled:\n        X_train, y_train = np.hsplit(\n            np.load(f\"{ROOT_DIR}/data/fashion_train_pca.npy\"), [-1]\n        )\n        X_test, y_test = np.hsplit(\n            np.load(f\"{ROOT_DIR}/data/fashion_test_pca.npy\"), [-1]\n        )\n        # converting the y_labels back to integers from floats to avoid issues\n        y_train, y_test = y_train.astype(int), y_test.astype(int)\n    else:\n        raise ValueError(\"If raw, scaled or pca is True, then all other arguments must be False.\")\n\n    return X_train, X_test, y_train, y_test\n</code></pre>"},{"location":"reference/mlproject/helpers/_metrics/","title":"_metrics","text":""},{"location":"reference/mlproject/helpers/_metrics/#mlproject.helpers._metrics.accuracy_score","title":"<code>accuracy_score(y_true, y_pred, normalize=True)</code>","text":"<p>Calculate the accuracy score from a given array of true labels and a given array of predicted labels.</p> <p>Inspired by https://stackoverflow.com/a/64680660</p> <p>Parameters:</p>    Name Type Description Default     <code>y_true</code>  <code>2d ndarray</code>  <p>array of shape (n_samples, 1) of true labels</p>  required    <code>y_pred</code>  <code>2d ndarray</code>  <p>array of shape (n_samples, 1) of predicted labels</p>  required     <p>Returns:</p>    Name Type Description     <code>accuracy_scores</code>  <code>float</code>  <p>calculated accuracy score</p>    <p>Raises:</p>    Type Description      <code>ValueError</code>  <p>if y_true and y_pred are not of the same shape</p>     Source code in <code>mlproject/helpers/_metrics.py</code> <pre><code>def accuracy_score(y_true, y_pred, normalize=True):\n    \"\"\"Calculate the accuracy score from a given array of true labels\n    and a given array of predicted labels.\n\n    Inspired by [https://stackoverflow.com/a/64680660](https://stackoverflow.com/a/64680660)\n\n    Parameters\n    ----------\n    y_true : 2d ndarray\n        array of shape (n_samples, 1) of true labels\n    y_pred : 2d ndarray\n        array of shape (n_samples, 1) of predicted labels\n\n    Returns\n    -------\n    accuracy_scores : float\n        calculated accuracy score\n\n    Raises\n    ------\n    ValueError\n        if y_true and y_pred are not of the same shape\n    \"\"\"\n\n    if y_true.shape[0] != y_pred.shape[0] and y_true.shape[1] != y_pred.shape[1]:\n        raise ValueError(\n            f\"Length of y_true: ({len(y_true)}) and y_pred: ({len(y_pred)}) should be the same!\"\n        )\n\n    accuracy = []\n    for i in range(len(y_pred)):\n        if y_pred[i] == y_true[i]:\n            accuracy.append(1)\n        else:\n            accuracy.append(0)\n    if normalize == True:\n        return np.mean(accuracy)\n    if normalize == False:\n        return sum(accuracy)\n</code></pre>"},{"location":"reference/mlproject/neural_net/","title":"neural_net","text":""},{"location":"reference/mlproject/neural_net/#mlproject.neural_net.DenseLayer","title":"<code>DenseLayer</code>","text":"<p>Fully connected layer of a neural network</p> <p>The weight initialisations are dependant on the activation function.</p> <ul> <li> <p>If the activation function is LeakyReLU, then the weights are initialised from a He-Normal distribution - see: http://proceedings.mlr.press/v9/glorot10a.html</p> </li> <li> <p>If the activation function is softmax, then the weights are initialised from a Glorot/Xavier normal distribution - see: https://arxiv.org/abs/1502.01852</p> </li> </ul> <p>Parameters:</p>    Name Type Description Default     <code>input_n</code>  <code>int</code>  <p>The amount of inputs to the DenseLayer</p>  required    <code>output_n</code>  <code>int</code>  <p>The amount of outputs to the DenseLayer</p>  required    <code>activation</code>  <code>str</code>  <p>The activation function to use for all of the neurons in the DenseLayer, either 'leaky_relu' or 'softmax'</p>  required      Source code in <code>mlproject/neural_net/_dense_layer.py</code> <pre><code>class DenseLayer:\n    \"\"\"Fully connected layer of a neural network\n\n    The weight initialisations are dependant on the activation function.\n\n    * If the activation function is LeakyReLU, then the weights are initialised from a He-Normal distribution - see: http://proceedings.mlr.press/v9/glorot10a.html\n\n    * If the activation function is softmax, then the weights are initialised from a Glorot/Xavier normal distribution - see: https://arxiv.org/abs/1502.01852\n\n    Parameters\n    ----------\n    input_n : int\n        The amount of inputs to the DenseLayer\n    output_n : int\n        The amount of outputs to the DenseLayer\n    activation : str\n        The activation function to use for all of the neurons in the DenseLayer, either 'leaky_relu' or 'softmax'\n    \"\"\"\n\n    def __init__(self, input_n: int, output_n: int, activation: str):\n\n        self.output_n, self.input_n = output_n, input_n\n\n        self.z = None\n\n        self.biases = np.zeros(shape=(output_n))\n\n        if activation == \"leaky_relu\":\n            self.activation = leaky_relu\n            # He initiliasation of weights\n            # see https://keras.io/api/layers/initializers/#henormal-class\n            stddev = np.sqrt(2 / input_n)\n            self.weights = np.random.normal(\n                loc=0, scale=stddev, size=(input_n, output_n)\n            )\n\n        elif activation == \"softmax\" or activation == \"stable_softmax\":\n            # Glorot/Xavier initiliasation of weights\n            # see https://keras.io/api/layers/initializers/#glorotnormal-class\n            stddev = np.sqrt(2 / (input_n + output_n))\n            self.weights = np.random.normal(\n                loc=0, scale=stddev, size=(input_n, output_n)\n            )\n            self.activation = stable_softmax\n\n        else:\n            raise NotImplementedError(\n                f\"{activation} not implemented yet. Choose from ['leaky_relu', 'stable_softmax']\"\n            )\n\n    def forward(self, X):\n        \"\"\"Computes a single forward pass of the DenseLayer\n\n        Parameters\n        ----------\n        X : 2d ndarray\n            An n x p matrix of data points\n            where n is the number of data points and p is the number of features.\n\n        Returns\n        -------\n        2d ndarray\n            An n x output_n numpy array where n is the number of samples\n            and output_n is the number of neurons in the DenseLayer\n        \"\"\"\n        self.z = X.dot(self.weights) + self.biases\n        return self.activation(self.z)\n\n    def _out_neurons(self):\n        \"\"\"Return the number of output neurons in the DenseLayer\n\n        Returns\n        -------\n        int\n            The total number of output neurons in the DenseLayer\n        \"\"\"\n        return self.output_n\n\n    def _in_neurons(self):\n        \"\"\"Return the number of input neurons in the DenseLayer\n\n        Returns\n        -------\n        int\n            The total number of input neurons in the DenseLayer\n        \"\"\"\n        return self.input_n\n\n    def activation_function(self):\n        \"\"\"Return a string representing the activation function of the given DenseLayer\n\n        Returns\n        -------\n        string\n            string representing the activation function of the given DenseLayer\n        \"\"\"\n        if self.activation == stable_softmax:\n            return \"softmax\"\n        elif self.activation == leaky_relu:\n            return \"leaky_relu\"\n</code></pre>"},{"location":"reference/mlproject/neural_net/#mlproject.neural_net._dense_layer.DenseLayer.activation_function","title":"<code>activation_function()</code>","text":"<p>Return a string representing the activation function of the given DenseLayer</p> <p>Returns:</p>    Type Description      <code>string</code>  <p>string representing the activation function of the given DenseLayer</p>     Source code in <code>mlproject/neural_net/_dense_layer.py</code> <pre><code>def activation_function(self):\n    \"\"\"Return a string representing the activation function of the given DenseLayer\n\n    Returns\n    -------\n    string\n        string representing the activation function of the given DenseLayer\n    \"\"\"\n    if self.activation == stable_softmax:\n        return \"softmax\"\n    elif self.activation == leaky_relu:\n        return \"leaky_relu\"\n</code></pre>"},{"location":"reference/mlproject/neural_net/#mlproject.neural_net._dense_layer.DenseLayer.forward","title":"<code>forward(X)</code>","text":"<p>Computes a single forward pass of the DenseLayer</p> <p>Parameters:</p>    Name Type Description Default     <code>X</code>  <code>2d ndarray</code>  <p>An n x p matrix of data points where n is the number of data points and p is the number of features.</p>  required     <p>Returns:</p>    Type Description      <code>2d ndarray</code>  <p>An n x output_n numpy array where n is the number of samples and output_n is the number of neurons in the DenseLayer</p>     Source code in <code>mlproject/neural_net/_dense_layer.py</code> <pre><code>def forward(self, X):\n    \"\"\"Computes a single forward pass of the DenseLayer\n\n    Parameters\n    ----------\n    X : 2d ndarray\n        An n x p matrix of data points\n        where n is the number of data points and p is the number of features.\n\n    Returns\n    -------\n    2d ndarray\n        An n x output_n numpy array where n is the number of samples\n        and output_n is the number of neurons in the DenseLayer\n    \"\"\"\n    self.z = X.dot(self.weights) + self.biases\n    return self.activation(self.z)\n</code></pre>"},{"location":"reference/mlproject/neural_net/#mlproject.neural_net.NeuralNetworkClassifier","title":"<code>NeuralNetworkClassifier</code>","text":"<p>NeuralNetworkClassifier</p> <p>Feed Forward Neural Network Classifier with however many dense layers (fully connected layers) of class <code>DenseLayer</code> each with own activation function and a network wide loss function. The layers of the network can either be added when initilizing the network, as a list or added individually with the <code>add</code> method after initialization.</p> <p>Parameters:</p>    Name Type Description Default     <code>layers</code>  <code>list, optional</code>  <p>A list of class <code>DenseLayer</code></p>  <code>[]</code>    <code>loss</code>  <code>str, optional</code>  <p>The loss function to be used, currently only <code>cross_entropy</code> is supported.</p>  <code>'cross_entropy'</code>     <p>Attributes:</p>    Name Type Description     <code>X</code>  <code>2d ndarray</code>  <p>Data points to use for training the neural network</p>   <code>y</code>  <code>1d ndarray</code>  <p>Target classes</p>   <code>n</code>  <code>int</code>  <p>Number of data points (X.shape[0])</p>   <code>p</code>  <code>int</code>  <p>Number of features (X.shape[1])</p>     Source code in <code>mlproject/neural_net/_neural_net.py</code> <pre><code>class NeuralNetworkClassifier:\n    \"\"\"NeuralNetworkClassifier\n\n    Feed Forward Neural Network Classifier with however many\n    dense layers (fully connected layers) of class [`DenseLayer`][mlproject.neural_net._dense_layer.DenseLayer] each with own\n    activation function and a network wide loss function.\n    The layers of the network can either be added when initilizing the network, as a list\n    or added individually with the [`add`][mlproject.neural_net._neural_net.NeuralNetworkClassifier.add] method after initialization.\n\n    Parameters\n    ----------\n    layers : list, optional\n        A list of class [`DenseLayer`][mlproject.neural_net._dense_layer.DenseLayer]\n    loss : str, optional\n        The loss function to be used, currently only [`cross_entropy`][mlproject.neural_net._loss.cross_entropy_loss] is supported.\n\n    Attributes\n    ----------\n    X : 2d ndarray\n        Data points to use for training the neural network\n    y : 1d ndarray\n        Target classes\n    n : int\n        Number of data points (X.shape[0])\n    p : int\n        Number of features (X.shape[1])\n    \"\"\"\n\n    def __init__(self, layers=[], loss=\"cross_entropy\"):\n\n        self.X = None\n        self.n, self.p = None, None\n        self.y = None\n        self.k = None\n        self.layers = layers\n        self.activations, self.sums = [], []\n\n        if loss == \"cross_entropy\":\n            self.loss_str = \"cross_entropy_loss\"\n            self.loss = cross_entropy_loss\n        else:\n            raise NotImplementedError(\n                f\"{loss} not implemented yet. Choose from ['cross_entropy']\"\n            )\n\n    def add(self, layer: DenseLayer):\n        \"\"\"Add a new layer to the network, after the current layer.\n\n        Parameters\n        ----------\n        layer : DenseLayer\n            Fully connected layer.\n\n        Example\n        -------\n        ``` py\n        &gt;&gt;&gt; NN = NeuralNetworkClassifier(loss='cross_entropy')\n        &gt;&gt;&gt; NN.add(DenseLayer(784,128,\"leaky_relu\"))\n        &gt;&gt;&gt; NN.add(DenseLayer(128,5,\"softmax\"))\n        &gt;&gt;&gt; print(NN)\n\n        NeuralNetworkClassifier\n        --------------------------------\n        Loss function: cross_entropy_loss\n\n        Input layer:\n                Neurons: 128 , Activation: leaky_relu\n\n        Output layer:\n                Neurons: 5 , Activation: softmax\n        ```\n        \"\"\"\n        self.layers.append(layer)\n\n    def forward(self, X):\n        \"\"\"Compute a single forward pass of the network.\n\n        Parameters\n        ----------\n        X : 2d ndarray\n            The data to use for the forward pass.\n            Must be of size n x input_n\n            where input_n must come from the first [`DenseLayer`][mlproject.neural_net._dense_layer.DenseLayer] in the network.\n\n        Returns\n        -------\n        2d ndarray\n            An n x output_n array\n            where output_n corresponds to the output_n of the last [`DenseLayer`][mlproject.neural_net._dense_layer.DenseLayer] in the network\n            and n is the number of data points.\n        \"\"\"\n        self.activations.append(X)\n        for layer in self.layers:\n            X = layer.forward(X)\n            self.activations.append(X)\n            self.sums.append(layer.z)\n\n        return X\n\n    def predict(self, X):\n        \"\"\"Predict class labels for the given data.\n\n        Parameters\n        ----------\n        X : 2d ndarray\n            The data that we want to use to make predictions.\n\n        Returns\n        -------\n        1d ndarray\n            All predicted class labels with size n, where n is the number of data points.\n        \"\"\"\n        probabilities = self.predict_proba(X)\n\n        return np.array(\n            [self.label[pred] for pred in np.argmax(probabilities, axis=1).astype(int)]\n        )\n\n    def predict_proba(self, X):\n        \"\"\"Predict class probabilities for the given data\n\n        Parameters\n        ----------\n        X : 2d ndarray\n            The data that we want to use to make predictions\n\n        Returns\n        -------\n        2d ndarray\n            All probabilites with size n x k, where n is the number of data points and k is the number classes\n        \"\"\"\n        return self.forward(X)\n\n    def fit(self, X, y, batches: int = 1, epochs: int = 1000, lr: float = 0.01):\n        r\"\"\"The actual training of the network to the given data\n\n        Parameters\n        ----------\n        X : 2d ndarray\n            An $N \\times P$ matrix of data points\n            where n is the number of data points and p is the number of features.\n        y : 1d ndarray\n            $N \\times 1$ vector of target class labels\n        batches : int, optional\n            The number of batches to use for training in each epoch,\n            an integer indicating the number of splits to split the data into,\n            by default $1$ which corresponds to training on the entire dataset\n            in every epoch.\n        epochs : int, optional\n            The number of iterations to train for\n        lr : float, optional\n            The learning rate for gradient descent\n        \"\"\"\n\n        self.X = X\n        self.n, self.p = self.X.shape\n        self.y = y\n        self.learning_rate = lr\n\n        unique_classes = np.unique(y)\n        self.k = len(unique_classes)\n\n        one_hot = OneHotEncoder(categories=[unique_classes])\n        self.y_hot_encoded = one_hot.fit_transform(self.y).toarray()\n\n        if self.layers[-1]._out_neurons() != self.k:\n            raise ValueError(\n                f\"The number of neurons in the output layer, output_n: ({self.layers[-1].out_neurons()}) must be equal to the number of classes, k: ({self.k})\"\n            )\n        if self.layers[0]._in_neurons() != self.X.shape[1]:\n            raise ValueError(\n                f\"The number of neurons in the input layer, input_n: ({self.layers[0].in_neurons()}) must be equal to the number features in the dataset: ({self.X.shape[1]})\"\n            )\n\n        # populate label-intcode dictionaries\n        self.label = {k: unique_classes[k] for k in range(self.k)}\n        self.intcode = {unique_classes[k]: k for k in range(self.k)}\n\n        self.loss_history = []\n        self.accuracy_history = []\n\n        # get indices of every data point\n        idxs = np.arange(self.n)\n\n        with progress as pb:\n            t1 = pb.add_task(\"[blue]Training\", total=epochs)\n\n            for epoch in range(epochs):\n\n                # randomly shuffle the data --&gt; split it into number of batches\n                # here np.array_split returns an array of arrays of indices\n                # of the different splits\n                np.random.shuffle(idxs)\n                batch_idxs = np.array_split(idxs, batches)\n\n                for batch in batch_idxs:\n\n                    X_batch = self.X[batch]\n                    y_batch = self.y_hot_encoded[batch]\n\n                    # compute the initial class probabilities by doing a single forward pass\n                    # note: this should come 'automatically' from defining the last layer\n                    # in the model as a layer with output_n = k with softmax activation\n                    # where k is the number of classes.\n                    init_probs = self.forward(X_batch)\n                    if np.isnan(init_probs).any() or np.isinf(init_probs).any():\n                        raise ValueError(\n                            f\"Unexpected value for init_probs, please try different parameters for either `batches`, `epocs` or `lr`\"\n                        )\n\n                    # dividide by the number of data points in this specific batch to get the average loss.\n                    loss = self.loss(y_batch, init_probs) / len(y_batch)\n                    if np.isnan(loss) or np.isinf(loss):\n                        raise ValueError(\n                            f\"Unexpected value for loss, please try different parameters for either `batches`, `epocs` or `lr`\"\n                        )\n\n                    self._backward(y_batch)\n\n                # add the latest loss to the history\n                self.loss_history.append(loss)\n\n                # predict with the current weights and biases on the whole data set\n                batch_predict = self.predict(self.X)\n\n                # calculate the accuracy score of the prediction\n                train_accuracy = accuracy_score(self.y, batch_predict)\n\n                # add accuracy to the history\n                self.accuracy_history.append(train_accuracy)\n\n                # update rich progress bar for each epoch\n                pb.update(t1, advance=1)\n\n                if progress.finished:\n                    pb.update(t1, description=\"[bright_green]Training complete!\")\n\n    def _backward(self, y_batch):\n        \"\"\"Computes a single backward pass all the way through the network.\n        Also updates the weights and biases.\n\n        Parameters\n        ----------\n        y_batch : 2d ndarray\n            array of one-hot encoded ground_truth labels\n        \"\"\"\n\n        # This is the derivative of loss function w.r.t Z. Explanation here https://www.mldawn.com/back-propagation-with-cross-entropy-and-softmax/\n        delta = self.activations[-1] - y_batch\n\n        grad_bias = delta.sum(0)\n\n        grad_weight = self.activations[-2].T @ delta\n\n        grad_biases, grad_weights = [], []\n        grad_weights.append(grad_weight)\n        grad_biases.append(grad_bias)\n\n        for i in range(2, len(self.layers) + 1):\n            layer = self.layers[-i + 1]\n            dzda = delta @ layer.weights.T\n            delta = dzda * leaky_relu_der(self.sums[-i])\n\n            grad_bias = delta.sum(0)\n            grad_weight = self.activations[-i - 1].T @ delta\n            grad_weights.append(grad_weight)\n            grad_biases.append(grad_bias)\n\n        # reverse the gradient lists so we can index them normally.\n        grad_biases_rev = list(reversed(grad_biases))\n        grad_weights_rev = list(reversed(grad_weights))\n\n        for i in range(0, len(self.layers)):\n            self.layers[i].weights -= self.learning_rate * grad_weights_rev[i]\n            self.layers[i].biases -= self.learning_rate * grad_biases_rev[i]\n\n    def __str__(self):\n        s = \"\\nNeuralNetworkClassifier \\n\"\n        s += \"--------------------------------\\n\"\n        s += f\"Loss function: {self.loss_str}\\n\\n\"\n        layers = [self.layers[i] for i in range(0, len(self.layers))]\n        layers_neu = [\n            f\"\\tInput: {i.input_n}, Output: {i.output_n} , Activation: {i.activation_function()}\"\n            for i in layers\n        ]\n        layer_num = 0\n        for layer in layers_neu:\n            if layer_num == 0:\n                s += \"Input layer: \\n\" + layer + \"\\n\\n\"\n            elif layer_num == len(self.layers) - 1:\n                s += f\"Output layer: \\n\" + layer\n            else:\n                s += f\"Layer: {layer_num}\\n\" + layer + \"\\n\\n\"\n            layer_num += 1\n\n        return s\n</code></pre>"},{"location":"reference/mlproject/neural_net/#mlproject.neural_net._neural_net.NeuralNetworkClassifier.add","title":"<code>add(layer)</code>","text":"<p>Add a new layer to the network, after the current layer.</p> <p>Parameters:</p>    Name Type Description Default     <code>layer</code>  <code>DenseLayer</code>  <p>Fully connected layer.</p>  required"},{"location":"reference/mlproject/neural_net/#mlproject.neural_net._neural_net.NeuralNetworkClassifier.add--example","title":"Example","text":"<pre><code>&gt;&gt;&gt; NN = NeuralNetworkClassifier(loss='cross_entropy')\n&gt;&gt;&gt; NN.add(DenseLayer(784,128,\"leaky_relu\"))\n&gt;&gt;&gt; NN.add(DenseLayer(128,5,\"softmax\"))\n&gt;&gt;&gt; print(NN)\n\nNeuralNetworkClassifier\n--------------------------------\nLoss function: cross_entropy_loss\n\nInput layer:\n        Neurons: 128 , Activation: leaky_relu\n\nOutput layer:\n        Neurons: 5 , Activation: softmax\n</code></pre>  Source code in <code>mlproject/neural_net/_neural_net.py</code> <pre><code>def add(self, layer: DenseLayer):\n    \"\"\"Add a new layer to the network, after the current layer.\n\n    Parameters\n    ----------\n    layer : DenseLayer\n        Fully connected layer.\n\n    Example\n    -------\n    ``` py\n    &gt;&gt;&gt; NN = NeuralNetworkClassifier(loss='cross_entropy')\n    &gt;&gt;&gt; NN.add(DenseLayer(784,128,\"leaky_relu\"))\n    &gt;&gt;&gt; NN.add(DenseLayer(128,5,\"softmax\"))\n    &gt;&gt;&gt; print(NN)\n\n    NeuralNetworkClassifier\n    --------------------------------\n    Loss function: cross_entropy_loss\n\n    Input layer:\n            Neurons: 128 , Activation: leaky_relu\n\n    Output layer:\n            Neurons: 5 , Activation: softmax\n    ```\n    \"\"\"\n    self.layers.append(layer)\n</code></pre>"},{"location":"reference/mlproject/neural_net/#mlproject.neural_net._neural_net.NeuralNetworkClassifier.fit","title":"<code>fit(X, y, batches=1, epochs=1000, lr=0.01)</code>","text":"<p>The actual training of the network to the given data</p> <p>Parameters:</p>    Name Type Description Default     <code>X</code>  <code>2d ndarray</code>  <p>An \\(N \\times P\\) matrix of data points where n is the number of data points and p is the number of features.</p>  required    <code>y</code>  <code>1d ndarray</code>  <p>\\(N \\times 1\\) vector of target class labels</p>  required    <code>batches</code>  <code>int, optional</code>  <p>The number of batches to use for training in each epoch, an integer indicating the number of splits to split the data into, by default \\(1\\) which corresponds to training on the entire dataset in every epoch.</p>  <code>1</code>    <code>epochs</code>  <code>int, optional</code>  <p>The number of iterations to train for</p>  <code>1000</code>    <code>lr</code>  <code>float, optional</code>  <p>The learning rate for gradient descent</p>  <code>0.01</code>      Source code in <code>mlproject/neural_net/_neural_net.py</code> <pre><code>def fit(self, X, y, batches: int = 1, epochs: int = 1000, lr: float = 0.01):\n    r\"\"\"The actual training of the network to the given data\n\n    Parameters\n    ----------\n    X : 2d ndarray\n        An $N \\times P$ matrix of data points\n        where n is the number of data points and p is the number of features.\n    y : 1d ndarray\n        $N \\times 1$ vector of target class labels\n    batches : int, optional\n        The number of batches to use for training in each epoch,\n        an integer indicating the number of splits to split the data into,\n        by default $1$ which corresponds to training on the entire dataset\n        in every epoch.\n    epochs : int, optional\n        The number of iterations to train for\n    lr : float, optional\n        The learning rate for gradient descent\n    \"\"\"\n\n    self.X = X\n    self.n, self.p = self.X.shape\n    self.y = y\n    self.learning_rate = lr\n\n    unique_classes = np.unique(y)\n    self.k = len(unique_classes)\n\n    one_hot = OneHotEncoder(categories=[unique_classes])\n    self.y_hot_encoded = one_hot.fit_transform(self.y).toarray()\n\n    if self.layers[-1]._out_neurons() != self.k:\n        raise ValueError(\n            f\"The number of neurons in the output layer, output_n: ({self.layers[-1].out_neurons()}) must be equal to the number of classes, k: ({self.k})\"\n        )\n    if self.layers[0]._in_neurons() != self.X.shape[1]:\n        raise ValueError(\n            f\"The number of neurons in the input layer, input_n: ({self.layers[0].in_neurons()}) must be equal to the number features in the dataset: ({self.X.shape[1]})\"\n        )\n\n    # populate label-intcode dictionaries\n    self.label = {k: unique_classes[k] for k in range(self.k)}\n    self.intcode = {unique_classes[k]: k for k in range(self.k)}\n\n    self.loss_history = []\n    self.accuracy_history = []\n\n    # get indices of every data point\n    idxs = np.arange(self.n)\n\n    with progress as pb:\n        t1 = pb.add_task(\"[blue]Training\", total=epochs)\n\n        for epoch in range(epochs):\n\n            # randomly shuffle the data --&gt; split it into number of batches\n            # here np.array_split returns an array of arrays of indices\n            # of the different splits\n            np.random.shuffle(idxs)\n            batch_idxs = np.array_split(idxs, batches)\n\n            for batch in batch_idxs:\n\n                X_batch = self.X[batch]\n                y_batch = self.y_hot_encoded[batch]\n\n                # compute the initial class probabilities by doing a single forward pass\n                # note: this should come 'automatically' from defining the last layer\n                # in the model as a layer with output_n = k with softmax activation\n                # where k is the number of classes.\n                init_probs = self.forward(X_batch)\n                if np.isnan(init_probs).any() or np.isinf(init_probs).any():\n                    raise ValueError(\n                        f\"Unexpected value for init_probs, please try different parameters for either `batches`, `epocs` or `lr`\"\n                    )\n\n                # dividide by the number of data points in this specific batch to get the average loss.\n                loss = self.loss(y_batch, init_probs) / len(y_batch)\n                if np.isnan(loss) or np.isinf(loss):\n                    raise ValueError(\n                        f\"Unexpected value for loss, please try different parameters for either `batches`, `epocs` or `lr`\"\n                    )\n\n                self._backward(y_batch)\n\n            # add the latest loss to the history\n            self.loss_history.append(loss)\n\n            # predict with the current weights and biases on the whole data set\n            batch_predict = self.predict(self.X)\n\n            # calculate the accuracy score of the prediction\n            train_accuracy = accuracy_score(self.y, batch_predict)\n\n            # add accuracy to the history\n            self.accuracy_history.append(train_accuracy)\n\n            # update rich progress bar for each epoch\n            pb.update(t1, advance=1)\n\n            if progress.finished:\n                pb.update(t1, description=\"[bright_green]Training complete!\")\n</code></pre>"},{"location":"reference/mlproject/neural_net/#mlproject.neural_net._neural_net.NeuralNetworkClassifier.forward","title":"<code>forward(X)</code>","text":"<p>Compute a single forward pass of the network.</p> <p>Parameters:</p>    Name Type Description Default     <code>X</code>  <code>2d ndarray</code>  <p>The data to use for the forward pass. Must be of size n x input_n where input_n must come from the first <code>DenseLayer</code> in the network.</p>  required     <p>Returns:</p>    Type Description      <code>2d ndarray</code>  <p>An n x output_n array where output_n corresponds to the output_n of the last <code>DenseLayer</code> in the network and n is the number of data points.</p>     Source code in <code>mlproject/neural_net/_neural_net.py</code> <pre><code>def forward(self, X):\n    \"\"\"Compute a single forward pass of the network.\n\n    Parameters\n    ----------\n    X : 2d ndarray\n        The data to use for the forward pass.\n        Must be of size n x input_n\n        where input_n must come from the first [`DenseLayer`][mlproject.neural_net._dense_layer.DenseLayer] in the network.\n\n    Returns\n    -------\n    2d ndarray\n        An n x output_n array\n        where output_n corresponds to the output_n of the last [`DenseLayer`][mlproject.neural_net._dense_layer.DenseLayer] in the network\n        and n is the number of data points.\n    \"\"\"\n    self.activations.append(X)\n    for layer in self.layers:\n        X = layer.forward(X)\n        self.activations.append(X)\n        self.sums.append(layer.z)\n\n    return X\n</code></pre>"},{"location":"reference/mlproject/neural_net/#mlproject.neural_net._neural_net.NeuralNetworkClassifier.predict","title":"<code>predict(X)</code>","text":"<p>Predict class labels for the given data.</p> <p>Parameters:</p>    Name Type Description Default     <code>X</code>  <code>2d ndarray</code>  <p>The data that we want to use to make predictions.</p>  required     <p>Returns:</p>    Type Description      <code>1d ndarray</code>  <p>All predicted class labels with size n, where n is the number of data points.</p>     Source code in <code>mlproject/neural_net/_neural_net.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict class labels for the given data.\n\n    Parameters\n    ----------\n    X : 2d ndarray\n        The data that we want to use to make predictions.\n\n    Returns\n    -------\n    1d ndarray\n        All predicted class labels with size n, where n is the number of data points.\n    \"\"\"\n    probabilities = self.predict_proba(X)\n\n    return np.array(\n        [self.label[pred] for pred in np.argmax(probabilities, axis=1).astype(int)]\n    )\n</code></pre>"},{"location":"reference/mlproject/neural_net/#mlproject.neural_net._neural_net.NeuralNetworkClassifier.predict_proba","title":"<code>predict_proba(X)</code>","text":"<p>Predict class probabilities for the given data</p> <p>Parameters:</p>    Name Type Description Default     <code>X</code>  <code>2d ndarray</code>  <p>The data that we want to use to make predictions</p>  required     <p>Returns:</p>    Type Description      <code>2d ndarray</code>  <p>All probabilites with size n x k, where n is the number of data points and k is the number classes</p>     Source code in <code>mlproject/neural_net/_neural_net.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict class probabilities for the given data\n\n    Parameters\n    ----------\n    X : 2d ndarray\n        The data that we want to use to make predictions\n\n    Returns\n    -------\n    2d ndarray\n        All probabilites with size n x k, where n is the number of data points and k is the number classes\n    \"\"\"\n    return self.forward(X)\n</code></pre>"},{"location":"reference/mlproject/neural_net/_activations/","title":"_activations","text":""},{"location":"reference/mlproject/neural_net/_activations/#mlproject.neural_net._activations.leaky_relu","title":"<code>leaky_relu(z)</code>","text":"<p>Leaky relu activation function</p> <p>Parameters:</p>    Name Type Description Default     <code>z</code>  <code>2d ndarray</code>  <p>input to the leaky relu activation function</p>  required     <p>Returns:</p>    Type Description      <code>2d ndarray</code>  <p>leaky relu 'activated' version of the input <code>z</code></p>     Source code in <code>mlproject/neural_net/_activations.py</code> <pre><code>def leaky_relu(z):\n    \"\"\"Leaky relu activation function\n\n    Parameters\n    ----------\n    z : 2d ndarray\n        input to the leaky relu activation function\n\n    Returns\n    -------\n    2d ndarray\n        leaky relu 'activated' version of the input `z`\n    \"\"\"\n    return np.where(z &gt; 0, z, z * 0.01)\n</code></pre>"},{"location":"reference/mlproject/neural_net/_activations/#mlproject.neural_net._activations.leaky_relu_der","title":"<code>leaky_relu_der(z)</code>","text":"<p>Derivative of the leaky relu activation function</p> <p>Parameters:</p>    Name Type Description Default     <code>z</code>  <code>2d ndarray</code>  <p>input to calculate the derivative of</p>  required     <p>Returns:</p>    Type Description      <code>2d ndarray</code>  <p>derivative of the specific neuron with a leaky relu activation function</p>     Source code in <code>mlproject/neural_net/_activations.py</code> <pre><code>def leaky_relu_der(z):\n    \"\"\"Derivative of the leaky relu activation function\n\n    Parameters\n    ----------\n    z : 2d ndarray\n        input to calculate the derivative of\n\n    Returns\n    -------\n    2d ndarray\n        derivative of the specific neuron with a leaky relu activation function\n    \"\"\"\n    return np.where(z &gt; 0, 1, 0.01)\n</code></pre>"},{"location":"reference/mlproject/neural_net/_activations/#mlproject.neural_net._activations.stable_softmax","title":"<code>stable_softmax(z)</code>","text":"<p>Numerically stable softmax activation function</p> <p>Inspired by https://stackoverflow.com/a/50425683 &amp; https://github.com/scipy/scipy/blob/v1.9.3/scipy/special/_logsumexp.py#L130-L223</p> <p>Parameters:</p>    Name Type Description Default     <code>z</code>  <code>2d ndarray</code>  <p>input to the softmax activation function</p>  required     <p>Returns:</p>    Type Description      <code>2d ndarray</code>  <p>softmax 'activated' version of the input <code>z</code></p>     Source code in <code>mlproject/neural_net/_activations.py</code> <pre><code>def stable_softmax(z):\n    \"\"\"Numerically stable softmax activation function\n\n    Inspired by https://stackoverflow.com/a/50425683 &amp;\n    https://github.com/scipy/scipy/blob/v1.9.3/scipy/special/_logsumexp.py#L130-L223\n\n    Parameters\n    ----------\n    z : 2d ndarray\n        input to the softmax activation function\n\n    Returns\n    -------\n    2d ndarray\n        softmax 'activated' version of the input `z`\n    \"\"\"\n    # When keepdims is set to True we keep the original dimensions/shape of the input.\n    # axis = 1 means that we find the maximum value along the first axis i.e. the rows.\n    e_max = np.amax(z, axis=1, keepdims=True)\n    e = np.subtract(z, e_max)\n    e = np.exp(e)\n    return e / np.sum(e, axis=1, keepdims=True)\n</code></pre>"},{"location":"reference/mlproject/neural_net/_dense_layer/","title":"_dense_layer","text":""},{"location":"reference/mlproject/neural_net/_dense_layer/#mlproject.neural_net._dense_layer.DenseLayer","title":"<code>DenseLayer</code>","text":"<p>Fully connected layer of a neural network</p> <p>The weight initialisations are dependant on the activation function.</p> <ul> <li> <p>If the activation function is LeakyReLU, then the weights are initialised from a He-Normal distribution - see: http://proceedings.mlr.press/v9/glorot10a.html</p> </li> <li> <p>If the activation function is softmax, then the weights are initialised from a Glorot/Xavier normal distribution - see: https://arxiv.org/abs/1502.01852</p> </li> </ul> <p>Parameters:</p>    Name Type Description Default     <code>input_n</code>  <code>int</code>  <p>The amount of inputs to the DenseLayer</p>  required    <code>output_n</code>  <code>int</code>  <p>The amount of outputs to the DenseLayer</p>  required    <code>activation</code>  <code>str</code>  <p>The activation function to use for all of the neurons in the DenseLayer, either 'leaky_relu' or 'softmax'</p>  required      Source code in <code>mlproject/neural_net/_dense_layer.py</code> <pre><code>class DenseLayer:\n    \"\"\"Fully connected layer of a neural network\n\n    The weight initialisations are dependant on the activation function.\n\n    * If the activation function is LeakyReLU, then the weights are initialised from a He-Normal distribution - see: http://proceedings.mlr.press/v9/glorot10a.html\n\n    * If the activation function is softmax, then the weights are initialised from a Glorot/Xavier normal distribution - see: https://arxiv.org/abs/1502.01852\n\n    Parameters\n    ----------\n    input_n : int\n        The amount of inputs to the DenseLayer\n    output_n : int\n        The amount of outputs to the DenseLayer\n    activation : str\n        The activation function to use for all of the neurons in the DenseLayer, either 'leaky_relu' or 'softmax'\n    \"\"\"\n\n    def __init__(self, input_n: int, output_n: int, activation: str):\n\n        self.output_n, self.input_n = output_n, input_n\n\n        self.z = None\n\n        self.biases = np.zeros(shape=(output_n))\n\n        if activation == \"leaky_relu\":\n            self.activation = leaky_relu\n            # He initiliasation of weights\n            # see https://keras.io/api/layers/initializers/#henormal-class\n            stddev = np.sqrt(2 / input_n)\n            self.weights = np.random.normal(\n                loc=0, scale=stddev, size=(input_n, output_n)\n            )\n\n        elif activation == \"softmax\" or activation == \"stable_softmax\":\n            # Glorot/Xavier initiliasation of weights\n            # see https://keras.io/api/layers/initializers/#glorotnormal-class\n            stddev = np.sqrt(2 / (input_n + output_n))\n            self.weights = np.random.normal(\n                loc=0, scale=stddev, size=(input_n, output_n)\n            )\n            self.activation = stable_softmax\n\n        else:\n            raise NotImplementedError(\n                f\"{activation} not implemented yet. Choose from ['leaky_relu', 'stable_softmax']\"\n            )\n\n    def forward(self, X):\n        \"\"\"Computes a single forward pass of the DenseLayer\n\n        Parameters\n        ----------\n        X : 2d ndarray\n            An n x p matrix of data points\n            where n is the number of data points and p is the number of features.\n\n        Returns\n        -------\n        2d ndarray\n            An n x output_n numpy array where n is the number of samples\n            and output_n is the number of neurons in the DenseLayer\n        \"\"\"\n        self.z = X.dot(self.weights) + self.biases\n        return self.activation(self.z)\n\n    def _out_neurons(self):\n        \"\"\"Return the number of output neurons in the DenseLayer\n\n        Returns\n        -------\n        int\n            The total number of output neurons in the DenseLayer\n        \"\"\"\n        return self.output_n\n\n    def _in_neurons(self):\n        \"\"\"Return the number of input neurons in the DenseLayer\n\n        Returns\n        -------\n        int\n            The total number of input neurons in the DenseLayer\n        \"\"\"\n        return self.input_n\n\n    def activation_function(self):\n        \"\"\"Return a string representing the activation function of the given DenseLayer\n\n        Returns\n        -------\n        string\n            string representing the activation function of the given DenseLayer\n        \"\"\"\n        if self.activation == stable_softmax:\n            return \"softmax\"\n        elif self.activation == leaky_relu:\n            return \"leaky_relu\"\n</code></pre>"},{"location":"reference/mlproject/neural_net/_dense_layer/#mlproject.neural_net._dense_layer.DenseLayer.activation_function","title":"<code>activation_function()</code>","text":"<p>Return a string representing the activation function of the given DenseLayer</p> <p>Returns:</p>    Type Description      <code>string</code>  <p>string representing the activation function of the given DenseLayer</p>     Source code in <code>mlproject/neural_net/_dense_layer.py</code> <pre><code>def activation_function(self):\n    \"\"\"Return a string representing the activation function of the given DenseLayer\n\n    Returns\n    -------\n    string\n        string representing the activation function of the given DenseLayer\n    \"\"\"\n    if self.activation == stable_softmax:\n        return \"softmax\"\n    elif self.activation == leaky_relu:\n        return \"leaky_relu\"\n</code></pre>"},{"location":"reference/mlproject/neural_net/_dense_layer/#mlproject.neural_net._dense_layer.DenseLayer.forward","title":"<code>forward(X)</code>","text":"<p>Computes a single forward pass of the DenseLayer</p> <p>Parameters:</p>    Name Type Description Default     <code>X</code>  <code>2d ndarray</code>  <p>An n x p matrix of data points where n is the number of data points and p is the number of features.</p>  required     <p>Returns:</p>    Type Description      <code>2d ndarray</code>  <p>An n x output_n numpy array where n is the number of samples and output_n is the number of neurons in the DenseLayer</p>     Source code in <code>mlproject/neural_net/_dense_layer.py</code> <pre><code>def forward(self, X):\n    \"\"\"Computes a single forward pass of the DenseLayer\n\n    Parameters\n    ----------\n    X : 2d ndarray\n        An n x p matrix of data points\n        where n is the number of data points and p is the number of features.\n\n    Returns\n    -------\n    2d ndarray\n        An n x output_n numpy array where n is the number of samples\n        and output_n is the number of neurons in the DenseLayer\n    \"\"\"\n    self.z = X.dot(self.weights) + self.biases\n    return self.activation(self.z)\n</code></pre>"},{"location":"reference/mlproject/neural_net/_loss/","title":"_loss","text":""},{"location":"reference/mlproject/neural_net/_loss/#mlproject.neural_net._loss.cross_entropy_loss","title":"<code>cross_entropy_loss(y_true, y_pred)</code>","text":"<p>Compute the categorical cross entropy loss from the given true labels and predicted labels.</p> <p>We add \\(1\\mathrm{e}{-7}\\) (epsilon) to the prediction to avoid taking the log of \\(0\\)</p> <ul> <li>Inspired by keras implemenation: Keras implementation where epsilon is defined here</li> </ul> <p>Parameters:</p>    Name Type Description Default     <code>y_true</code>  <code>1d ndarray</code>  <p>true class labels of size 1 x n where n is the number of data points.</p>  required    <code>y_pred</code>  <code>1d ndarray</code>  <p>predicted class labels of size 1 x n where n is the number of data points.</p>  required     <p>Returns:</p>    Type Description      <code>float</code>  <p>Cross entropy score for the given prediction</p>     Source code in <code>mlproject/neural_net/_loss.py</code> <pre><code>def cross_entropy_loss(y_true, y_pred):\n    r\"\"\"Compute the categorical cross entropy loss\n    from the given true labels and predicted labels.\n\n    We add $1\\mathrm{e}{-7}$ (epsilon) to the prediction to avoid taking the log of $0$\n\n    - Inspired by keras implemenation:\n    [Keras implementation](https://github.com/keras-team/keras/blob/master/keras/backend.py#L5487-L5547) where\n    epsilon is defined [here](https://github.com/keras-team/keras/blob/master/keras/backend_config.py#L34-L44)\n\n\n    Parameters\n    ----------\n    y_true : 1d ndarray\n        true class labels of size 1 x n where n is the number of data points.\n    y_pred : 1d ndarray\n        predicted class labels of size 1 x n where n is the number of data points.\n\n    Returns\n    -------\n    float\n        Cross entropy score for the given prediction\n    \"\"\"\n    epsilon = 1e-07\n    return -np.sum(np.log(y_pred + epsilon) * y_true)\n</code></pre>"},{"location":"reference/mlproject/neural_net/_neural_net/","title":"_neural_net","text":""},{"location":"reference/mlproject/neural_net/_neural_net/#mlproject.neural_net._neural_net.NeuralNetworkClassifier","title":"<code>NeuralNetworkClassifier</code>","text":"<p>NeuralNetworkClassifier</p> <p>Feed Forward Neural Network Classifier with however many dense layers (fully connected layers) of class <code>DenseLayer</code> each with own activation function and a network wide loss function. The layers of the network can either be added when initilizing the network, as a list or added individually with the <code>add</code> method after initialization.</p> <p>Parameters:</p>    Name Type Description Default     <code>layers</code>  <code>list, optional</code>  <p>A list of class <code>DenseLayer</code></p>  <code>[]</code>    <code>loss</code>  <code>str, optional</code>  <p>The loss function to be used, currently only <code>cross_entropy</code> is supported.</p>  <code>'cross_entropy'</code>     <p>Attributes:</p>    Name Type Description     <code>X</code>  <code>2d ndarray</code>  <p>Data points to use for training the neural network</p>   <code>y</code>  <code>1d ndarray</code>  <p>Target classes</p>   <code>n</code>  <code>int</code>  <p>Number of data points (X.shape[0])</p>   <code>p</code>  <code>int</code>  <p>Number of features (X.shape[1])</p>     Source code in <code>mlproject/neural_net/_neural_net.py</code> <pre><code>class NeuralNetworkClassifier:\n    \"\"\"NeuralNetworkClassifier\n\n    Feed Forward Neural Network Classifier with however many\n    dense layers (fully connected layers) of class [`DenseLayer`][mlproject.neural_net._dense_layer.DenseLayer] each with own\n    activation function and a network wide loss function.\n    The layers of the network can either be added when initilizing the network, as a list\n    or added individually with the [`add`][mlproject.neural_net._neural_net.NeuralNetworkClassifier.add] method after initialization.\n\n    Parameters\n    ----------\n    layers : list, optional\n        A list of class [`DenseLayer`][mlproject.neural_net._dense_layer.DenseLayer]\n    loss : str, optional\n        The loss function to be used, currently only [`cross_entropy`][mlproject.neural_net._loss.cross_entropy_loss] is supported.\n\n    Attributes\n    ----------\n    X : 2d ndarray\n        Data points to use for training the neural network\n    y : 1d ndarray\n        Target classes\n    n : int\n        Number of data points (X.shape[0])\n    p : int\n        Number of features (X.shape[1])\n    \"\"\"\n\n    def __init__(self, layers=[], loss=\"cross_entropy\"):\n\n        self.X = None\n        self.n, self.p = None, None\n        self.y = None\n        self.k = None\n        self.layers = layers\n        self.activations, self.sums = [], []\n\n        if loss == \"cross_entropy\":\n            self.loss_str = \"cross_entropy_loss\"\n            self.loss = cross_entropy_loss\n        else:\n            raise NotImplementedError(\n                f\"{loss} not implemented yet. Choose from ['cross_entropy']\"\n            )\n\n    def add(self, layer: DenseLayer):\n        \"\"\"Add a new layer to the network, after the current layer.\n\n        Parameters\n        ----------\n        layer : DenseLayer\n            Fully connected layer.\n\n        Example\n        -------\n        ``` py\n        &gt;&gt;&gt; NN = NeuralNetworkClassifier(loss='cross_entropy')\n        &gt;&gt;&gt; NN.add(DenseLayer(784,128,\"leaky_relu\"))\n        &gt;&gt;&gt; NN.add(DenseLayer(128,5,\"softmax\"))\n        &gt;&gt;&gt; print(NN)\n\n        NeuralNetworkClassifier\n        --------------------------------\n        Loss function: cross_entropy_loss\n\n        Input layer:\n                Neurons: 128 , Activation: leaky_relu\n\n        Output layer:\n                Neurons: 5 , Activation: softmax\n        ```\n        \"\"\"\n        self.layers.append(layer)\n\n    def forward(self, X):\n        \"\"\"Compute a single forward pass of the network.\n\n        Parameters\n        ----------\n        X : 2d ndarray\n            The data to use for the forward pass.\n            Must be of size n x input_n\n            where input_n must come from the first [`DenseLayer`][mlproject.neural_net._dense_layer.DenseLayer] in the network.\n\n        Returns\n        -------\n        2d ndarray\n            An n x output_n array\n            where output_n corresponds to the output_n of the last [`DenseLayer`][mlproject.neural_net._dense_layer.DenseLayer] in the network\n            and n is the number of data points.\n        \"\"\"\n        self.activations.append(X)\n        for layer in self.layers:\n            X = layer.forward(X)\n            self.activations.append(X)\n            self.sums.append(layer.z)\n\n        return X\n\n    def predict(self, X):\n        \"\"\"Predict class labels for the given data.\n\n        Parameters\n        ----------\n        X : 2d ndarray\n            The data that we want to use to make predictions.\n\n        Returns\n        -------\n        1d ndarray\n            All predicted class labels with size n, where n is the number of data points.\n        \"\"\"\n        probabilities = self.predict_proba(X)\n\n        return np.array(\n            [self.label[pred] for pred in np.argmax(probabilities, axis=1).astype(int)]\n        )\n\n    def predict_proba(self, X):\n        \"\"\"Predict class probabilities for the given data\n\n        Parameters\n        ----------\n        X : 2d ndarray\n            The data that we want to use to make predictions\n\n        Returns\n        -------\n        2d ndarray\n            All probabilites with size n x k, where n is the number of data points and k is the number classes\n        \"\"\"\n        return self.forward(X)\n\n    def fit(self, X, y, batches: int = 1, epochs: int = 1000, lr: float = 0.01):\n        r\"\"\"The actual training of the network to the given data\n\n        Parameters\n        ----------\n        X : 2d ndarray\n            An $N \\times P$ matrix of data points\n            where n is the number of data points and p is the number of features.\n        y : 1d ndarray\n            $N \\times 1$ vector of target class labels\n        batches : int, optional\n            The number of batches to use for training in each epoch,\n            an integer indicating the number of splits to split the data into,\n            by default $1$ which corresponds to training on the entire dataset\n            in every epoch.\n        epochs : int, optional\n            The number of iterations to train for\n        lr : float, optional\n            The learning rate for gradient descent\n        \"\"\"\n\n        self.X = X\n        self.n, self.p = self.X.shape\n        self.y = y\n        self.learning_rate = lr\n\n        unique_classes = np.unique(y)\n        self.k = len(unique_classes)\n\n        one_hot = OneHotEncoder(categories=[unique_classes])\n        self.y_hot_encoded = one_hot.fit_transform(self.y).toarray()\n\n        if self.layers[-1]._out_neurons() != self.k:\n            raise ValueError(\n                f\"The number of neurons in the output layer, output_n: ({self.layers[-1].out_neurons()}) must be equal to the number of classes, k: ({self.k})\"\n            )\n        if self.layers[0]._in_neurons() != self.X.shape[1]:\n            raise ValueError(\n                f\"The number of neurons in the input layer, input_n: ({self.layers[0].in_neurons()}) must be equal to the number features in the dataset: ({self.X.shape[1]})\"\n            )\n\n        # populate label-intcode dictionaries\n        self.label = {k: unique_classes[k] for k in range(self.k)}\n        self.intcode = {unique_classes[k]: k for k in range(self.k)}\n\n        self.loss_history = []\n        self.accuracy_history = []\n\n        # get indices of every data point\n        idxs = np.arange(self.n)\n\n        with progress as pb:\n            t1 = pb.add_task(\"[blue]Training\", total=epochs)\n\n            for epoch in range(epochs):\n\n                # randomly shuffle the data --&gt; split it into number of batches\n                # here np.array_split returns an array of arrays of indices\n                # of the different splits\n                np.random.shuffle(idxs)\n                batch_idxs = np.array_split(idxs, batches)\n\n                for batch in batch_idxs:\n\n                    X_batch = self.X[batch]\n                    y_batch = self.y_hot_encoded[batch]\n\n                    # compute the initial class probabilities by doing a single forward pass\n                    # note: this should come 'automatically' from defining the last layer\n                    # in the model as a layer with output_n = k with softmax activation\n                    # where k is the number of classes.\n                    init_probs = self.forward(X_batch)\n                    if np.isnan(init_probs).any() or np.isinf(init_probs).any():\n                        raise ValueError(\n                            f\"Unexpected value for init_probs, please try different parameters for either `batches`, `epocs` or `lr`\"\n                        )\n\n                    # dividide by the number of data points in this specific batch to get the average loss.\n                    loss = self.loss(y_batch, init_probs) / len(y_batch)\n                    if np.isnan(loss) or np.isinf(loss):\n                        raise ValueError(\n                            f\"Unexpected value for loss, please try different parameters for either `batches`, `epocs` or `lr`\"\n                        )\n\n                    self._backward(y_batch)\n\n                # add the latest loss to the history\n                self.loss_history.append(loss)\n\n                # predict with the current weights and biases on the whole data set\n                batch_predict = self.predict(self.X)\n\n                # calculate the accuracy score of the prediction\n                train_accuracy = accuracy_score(self.y, batch_predict)\n\n                # add accuracy to the history\n                self.accuracy_history.append(train_accuracy)\n\n                # update rich progress bar for each epoch\n                pb.update(t1, advance=1)\n\n                if progress.finished:\n                    pb.update(t1, description=\"[bright_green]Training complete!\")\n\n    def _backward(self, y_batch):\n        \"\"\"Computes a single backward pass all the way through the network.\n        Also updates the weights and biases.\n\n        Parameters\n        ----------\n        y_batch : 2d ndarray\n            array of one-hot encoded ground_truth labels\n        \"\"\"\n\n        # This is the derivative of loss function w.r.t Z. Explanation here https://www.mldawn.com/back-propagation-with-cross-entropy-and-softmax/\n        delta = self.activations[-1] - y_batch\n\n        grad_bias = delta.sum(0)\n\n        grad_weight = self.activations[-2].T @ delta\n\n        grad_biases, grad_weights = [], []\n        grad_weights.append(grad_weight)\n        grad_biases.append(grad_bias)\n\n        for i in range(2, len(self.layers) + 1):\n            layer = self.layers[-i + 1]\n            dzda = delta @ layer.weights.T\n            delta = dzda * leaky_relu_der(self.sums[-i])\n\n            grad_bias = delta.sum(0)\n            grad_weight = self.activations[-i - 1].T @ delta\n            grad_weights.append(grad_weight)\n            grad_biases.append(grad_bias)\n\n        # reverse the gradient lists so we can index them normally.\n        grad_biases_rev = list(reversed(grad_biases))\n        grad_weights_rev = list(reversed(grad_weights))\n\n        for i in range(0, len(self.layers)):\n            self.layers[i].weights -= self.learning_rate * grad_weights_rev[i]\n            self.layers[i].biases -= self.learning_rate * grad_biases_rev[i]\n\n    def __str__(self):\n        s = \"\\nNeuralNetworkClassifier \\n\"\n        s += \"--------------------------------\\n\"\n        s += f\"Loss function: {self.loss_str}\\n\\n\"\n        layers = [self.layers[i] for i in range(0, len(self.layers))]\n        layers_neu = [\n            f\"\\tInput: {i.input_n}, Output: {i.output_n} , Activation: {i.activation_function()}\"\n            for i in layers\n        ]\n        layer_num = 0\n        for layer in layers_neu:\n            if layer_num == 0:\n                s += \"Input layer: \\n\" + layer + \"\\n\\n\"\n            elif layer_num == len(self.layers) - 1:\n                s += f\"Output layer: \\n\" + layer\n            else:\n                s += f\"Layer: {layer_num}\\n\" + layer + \"\\n\\n\"\n            layer_num += 1\n\n        return s\n</code></pre>"},{"location":"reference/mlproject/neural_net/_neural_net/#mlproject.neural_net._neural_net.NeuralNetworkClassifier.add","title":"<code>add(layer)</code>","text":"<p>Add a new layer to the network, after the current layer.</p> <p>Parameters:</p>    Name Type Description Default     <code>layer</code>  <code>DenseLayer</code>  <p>Fully connected layer.</p>  required"},{"location":"reference/mlproject/neural_net/_neural_net/#mlproject.neural_net._neural_net.NeuralNetworkClassifier.add--example","title":"Example","text":"<pre><code>&gt;&gt;&gt; NN = NeuralNetworkClassifier(loss='cross_entropy')\n&gt;&gt;&gt; NN.add(DenseLayer(784,128,\"leaky_relu\"))\n&gt;&gt;&gt; NN.add(DenseLayer(128,5,\"softmax\"))\n&gt;&gt;&gt; print(NN)\n\nNeuralNetworkClassifier\n--------------------------------\nLoss function: cross_entropy_loss\n\nInput layer:\n        Neurons: 128 , Activation: leaky_relu\n\nOutput layer:\n        Neurons: 5 , Activation: softmax\n</code></pre>  Source code in <code>mlproject/neural_net/_neural_net.py</code> <pre><code>def add(self, layer: DenseLayer):\n    \"\"\"Add a new layer to the network, after the current layer.\n\n    Parameters\n    ----------\n    layer : DenseLayer\n        Fully connected layer.\n\n    Example\n    -------\n    ``` py\n    &gt;&gt;&gt; NN = NeuralNetworkClassifier(loss='cross_entropy')\n    &gt;&gt;&gt; NN.add(DenseLayer(784,128,\"leaky_relu\"))\n    &gt;&gt;&gt; NN.add(DenseLayer(128,5,\"softmax\"))\n    &gt;&gt;&gt; print(NN)\n\n    NeuralNetworkClassifier\n    --------------------------------\n    Loss function: cross_entropy_loss\n\n    Input layer:\n            Neurons: 128 , Activation: leaky_relu\n\n    Output layer:\n            Neurons: 5 , Activation: softmax\n    ```\n    \"\"\"\n    self.layers.append(layer)\n</code></pre>"},{"location":"reference/mlproject/neural_net/_neural_net/#mlproject.neural_net._neural_net.NeuralNetworkClassifier.fit","title":"<code>fit(X, y, batches=1, epochs=1000, lr=0.01)</code>","text":"<p>The actual training of the network to the given data</p> <p>Parameters:</p>    Name Type Description Default     <code>X</code>  <code>2d ndarray</code>  <p>An \\(N \\times P\\) matrix of data points where n is the number of data points and p is the number of features.</p>  required    <code>y</code>  <code>1d ndarray</code>  <p>\\(N \\times 1\\) vector of target class labels</p>  required    <code>batches</code>  <code>int, optional</code>  <p>The number of batches to use for training in each epoch, an integer indicating the number of splits to split the data into, by default \\(1\\) which corresponds to training on the entire dataset in every epoch.</p>  <code>1</code>    <code>epochs</code>  <code>int, optional</code>  <p>The number of iterations to train for</p>  <code>1000</code>    <code>lr</code>  <code>float, optional</code>  <p>The learning rate for gradient descent</p>  <code>0.01</code>      Source code in <code>mlproject/neural_net/_neural_net.py</code> <pre><code>def fit(self, X, y, batches: int = 1, epochs: int = 1000, lr: float = 0.01):\n    r\"\"\"The actual training of the network to the given data\n\n    Parameters\n    ----------\n    X : 2d ndarray\n        An $N \\times P$ matrix of data points\n        where n is the number of data points and p is the number of features.\n    y : 1d ndarray\n        $N \\times 1$ vector of target class labels\n    batches : int, optional\n        The number of batches to use for training in each epoch,\n        an integer indicating the number of splits to split the data into,\n        by default $1$ which corresponds to training on the entire dataset\n        in every epoch.\n    epochs : int, optional\n        The number of iterations to train for\n    lr : float, optional\n        The learning rate for gradient descent\n    \"\"\"\n\n    self.X = X\n    self.n, self.p = self.X.shape\n    self.y = y\n    self.learning_rate = lr\n\n    unique_classes = np.unique(y)\n    self.k = len(unique_classes)\n\n    one_hot = OneHotEncoder(categories=[unique_classes])\n    self.y_hot_encoded = one_hot.fit_transform(self.y).toarray()\n\n    if self.layers[-1]._out_neurons() != self.k:\n        raise ValueError(\n            f\"The number of neurons in the output layer, output_n: ({self.layers[-1].out_neurons()}) must be equal to the number of classes, k: ({self.k})\"\n        )\n    if self.layers[0]._in_neurons() != self.X.shape[1]:\n        raise ValueError(\n            f\"The number of neurons in the input layer, input_n: ({self.layers[0].in_neurons()}) must be equal to the number features in the dataset: ({self.X.shape[1]})\"\n        )\n\n    # populate label-intcode dictionaries\n    self.label = {k: unique_classes[k] for k in range(self.k)}\n    self.intcode = {unique_classes[k]: k for k in range(self.k)}\n\n    self.loss_history = []\n    self.accuracy_history = []\n\n    # get indices of every data point\n    idxs = np.arange(self.n)\n\n    with progress as pb:\n        t1 = pb.add_task(\"[blue]Training\", total=epochs)\n\n        for epoch in range(epochs):\n\n            # randomly shuffle the data --&gt; split it into number of batches\n            # here np.array_split returns an array of arrays of indices\n            # of the different splits\n            np.random.shuffle(idxs)\n            batch_idxs = np.array_split(idxs, batches)\n\n            for batch in batch_idxs:\n\n                X_batch = self.X[batch]\n                y_batch = self.y_hot_encoded[batch]\n\n                # compute the initial class probabilities by doing a single forward pass\n                # note: this should come 'automatically' from defining the last layer\n                # in the model as a layer with output_n = k with softmax activation\n                # where k is the number of classes.\n                init_probs = self.forward(X_batch)\n                if np.isnan(init_probs).any() or np.isinf(init_probs).any():\n                    raise ValueError(\n                        f\"Unexpected value for init_probs, please try different parameters for either `batches`, `epocs` or `lr`\"\n                    )\n\n                # dividide by the number of data points in this specific batch to get the average loss.\n                loss = self.loss(y_batch, init_probs) / len(y_batch)\n                if np.isnan(loss) or np.isinf(loss):\n                    raise ValueError(\n                        f\"Unexpected value for loss, please try different parameters for either `batches`, `epocs` or `lr`\"\n                    )\n\n                self._backward(y_batch)\n\n            # add the latest loss to the history\n            self.loss_history.append(loss)\n\n            # predict with the current weights and biases on the whole data set\n            batch_predict = self.predict(self.X)\n\n            # calculate the accuracy score of the prediction\n            train_accuracy = accuracy_score(self.y, batch_predict)\n\n            # add accuracy to the history\n            self.accuracy_history.append(train_accuracy)\n\n            # update rich progress bar for each epoch\n            pb.update(t1, advance=1)\n\n            if progress.finished:\n                pb.update(t1, description=\"[bright_green]Training complete!\")\n</code></pre>"},{"location":"reference/mlproject/neural_net/_neural_net/#mlproject.neural_net._neural_net.NeuralNetworkClassifier.forward","title":"<code>forward(X)</code>","text":"<p>Compute a single forward pass of the network.</p> <p>Parameters:</p>    Name Type Description Default     <code>X</code>  <code>2d ndarray</code>  <p>The data to use for the forward pass. Must be of size n x input_n where input_n must come from the first <code>DenseLayer</code> in the network.</p>  required     <p>Returns:</p>    Type Description      <code>2d ndarray</code>  <p>An n x output_n array where output_n corresponds to the output_n of the last <code>DenseLayer</code> in the network and n is the number of data points.</p>     Source code in <code>mlproject/neural_net/_neural_net.py</code> <pre><code>def forward(self, X):\n    \"\"\"Compute a single forward pass of the network.\n\n    Parameters\n    ----------\n    X : 2d ndarray\n        The data to use for the forward pass.\n        Must be of size n x input_n\n        where input_n must come from the first [`DenseLayer`][mlproject.neural_net._dense_layer.DenseLayer] in the network.\n\n    Returns\n    -------\n    2d ndarray\n        An n x output_n array\n        where output_n corresponds to the output_n of the last [`DenseLayer`][mlproject.neural_net._dense_layer.DenseLayer] in the network\n        and n is the number of data points.\n    \"\"\"\n    self.activations.append(X)\n    for layer in self.layers:\n        X = layer.forward(X)\n        self.activations.append(X)\n        self.sums.append(layer.z)\n\n    return X\n</code></pre>"},{"location":"reference/mlproject/neural_net/_neural_net/#mlproject.neural_net._neural_net.NeuralNetworkClassifier.predict","title":"<code>predict(X)</code>","text":"<p>Predict class labels for the given data.</p> <p>Parameters:</p>    Name Type Description Default     <code>X</code>  <code>2d ndarray</code>  <p>The data that we want to use to make predictions.</p>  required     <p>Returns:</p>    Type Description      <code>1d ndarray</code>  <p>All predicted class labels with size n, where n is the number of data points.</p>     Source code in <code>mlproject/neural_net/_neural_net.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict class labels for the given data.\n\n    Parameters\n    ----------\n    X : 2d ndarray\n        The data that we want to use to make predictions.\n\n    Returns\n    -------\n    1d ndarray\n        All predicted class labels with size n, where n is the number of data points.\n    \"\"\"\n    probabilities = self.predict_proba(X)\n\n    return np.array(\n        [self.label[pred] for pred in np.argmax(probabilities, axis=1).astype(int)]\n    )\n</code></pre>"},{"location":"reference/mlproject/neural_net/_neural_net/#mlproject.neural_net._neural_net.NeuralNetworkClassifier.predict_proba","title":"<code>predict_proba(X)</code>","text":"<p>Predict class probabilities for the given data</p> <p>Parameters:</p>    Name Type Description Default     <code>X</code>  <code>2d ndarray</code>  <p>The data that we want to use to make predictions</p>  required     <p>Returns:</p>    Type Description      <code>2d ndarray</code>  <p>All probabilites with size n x k, where n is the number of data points and k is the number classes</p>     Source code in <code>mlproject/neural_net/_neural_net.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict class probabilities for the given data\n\n    Parameters\n    ----------\n    X : 2d ndarray\n        The data that we want to use to make predictions\n\n    Returns\n    -------\n    2d ndarray\n        All probabilites with size n x k, where n is the number of data points and k is the number classes\n    \"\"\"\n    return self.forward(X)\n</code></pre>"}]}